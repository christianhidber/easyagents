{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf_agents_orso_talk.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zpzHtN3-kQ26",
        "w3OdHyWEEEwy",
        "bzoq0VM85p46"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/talk/tf_agents_orso_talk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44SKfTDHsayI"
      },
      "source": [
        "# Orso's live running on raw TF Agents\n",
        "\n",
        "Make our bear Orso find all the honey in his home turf choosing the most convenient path.  \n",
        "\n",
        "<img src='https://github.com/christianhidber/easyagents/blob/master/jupyter_notebooks/images/Orso.png?raw=1'>\n",
        "<img src='https://github.com/christianhidber/easyagents/blob/master/jupyter_notebooks/images/turf.png?raw=1'>\n",
        "\n",
        "https://opendatascience.com/bears-need-to-learn-as-well-practical-reinforcement-learning-with-tensorflow-2-0-tf-agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d8CBZOeEteb4",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plt.xkcd()\n",
        "# plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 9)\n",
        "# mpl.rcParams['figure.figsize'] = (20, 12)\n",
        "# mpl.rcParams[\"figure.dpi\"] = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D1au63NZtez2",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install -q easyagents 2>/dev/null\n",
        "!pip install -q networkx==2.3.0 2>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_stl7FMotiB_",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "if 'google.colab' in sys.modules:\n",
		"    !apt-get update >/dev/null\n",
        "    !apt-get install xvfb >/dev/null\n",
        "    !pip install pyvirtualdisplay >/dev/null    \n",
        "    \n",
        "    from pyvirtualdisplay import Display\n",
        "    Display(visible=0, size=(960, 720)).start() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LNjWlw6sayL",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "\n",
        "from tf_agents.agents.reinforce import reinforce_agent\n",
        "from tf_agents.agents.ppo import ppo_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.networks import value_network\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjylke35xJTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tf_agents\n",
        "tf_agents.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQGDnu7GpJME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4THMPripN7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_probability as tfp\n",
        "tfp.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShPALskHrmQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import easyagents\n",
        "easyagents.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x-olom0nwiSX"
      },
      "source": [
        "### Orso's Environment (OpenAI Gym)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3plH2u3Swotj",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.image as mpi\n",
        "from matplotlib.offsetbox import (OffsetImage, AnnotationBbox)\n",
        "from IPython.display import display, clear_output\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "graph = {\n",
        "            'S': [('A', 300), ('B', 100), ('C', 200)],\n",
        "            'A': [('S', 300), ('B', 100), ('E', 100), ('D', 100)],\n",
        "            'B': [('S', 100), ('A', 100), ('C', 50), ('K', 200)],\n",
        "            'C': [('S', 200), ('B', 50), ('M', 100), ('L', 200)],\n",
        "            'D': [('A', 100), ('F', 50)],\n",
        "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
        "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
        "            'G': [('F', 200), ('O', 300)],\n",
        "            'H': [('E', 100), ('K', 300)],\n",
        "            'K': [('B', 200), ('H', 300)],\n",
        "            'L': [('C', 200), ('M', 50)],\n",
        "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
        "            'N': [('M', 100), ('O', 100)],\n",
        "            'O': [('N', 100), ('G', 300)]\n",
        "        }\n",
        "\n",
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "        'D': 4,\n",
        "        'E': 5,\n",
        "        'F': 6,\n",
        "        'G': 7,\n",
        "        'H': 8,\n",
        "        'K': 9,\n",
        "        'L': 10,\n",
        "        'M': 11,\n",
        "        'N': 12,\n",
        "        'O': 13\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C',\n",
        "        4: 'D',\n",
        "        5: 'E',\n",
        "        6: 'F',\n",
        "        7: 'G',\n",
        "        8: 'H',\n",
        "        9: 'K',\n",
        "        10: 'L',\n",
        "        11: 'M',\n",
        "        12: 'N',\n",
        "        13: 'O'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "\n",
        "class OrsoEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    showStep = False\n",
        "\n",
        "    def __init__(self):\n",
        "        self.map = graph\n",
        "        max_paths = 4\n",
        "        self.action_space = spaces.Discrete(max_paths)\n",
        "\n",
        "        positions = len(self.map)\n",
        "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
        "        # non existing path is -1000 and no position change\n",
        "        # look at what #getObservation returns if you are confused\n",
        "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
        "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
        "        self.observation_space = spaces.Box(low=low,\n",
        "                                            high=high,\n",
        "                                            dtype=np.float32)\n",
        "        self.reward_range = (-1, 1)\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "        self._figure = None\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "        base = \"https://raw.githubusercontent.com/christianhidber/easyagents/master/jupyter_notebooks/images/\"\n",
        "        self.image_orso = mpi.imread(base + \"Orso.png\")\n",
        "        self.image_cave = mpi.imread(base + \"Cave.png\")\n",
        "        self.image_honey = mpi.imread(base + \"Honey.png\")\n",
        "        self.image_empty_pot = mpi.imread(base + \"EmptyPot.png\")\n",
        "        self.nx_graph, self.nx_pos = self._create_nx_graph()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def iterate_path(self, state, action):\n",
        "        paths = self.map[state]\n",
        "        if action < len(paths):\n",
        "            return paths[action]\n",
        "        else:\n",
        "            # sorry, no such action, stay where you are and pay a high penalty\n",
        "            return (state, 1000)\n",
        "\n",
        "    def step(self, action):\n",
        "        destination, cost = self.iterate_path(self.state, action)\n",
        "        source = self.state\n",
        "\n",
        "        self.cost = cost\n",
        "        self.action = action\n",
        "        self.lastStep_state = self.state\n",
        "        self.state = destination\n",
        "        self.customerReward = self.customer_reward[destination]\n",
        "        self.reward = 0\n",
        "        self.reward = (self.customerReward - self.cost) / self.optimum\n",
        "\n",
        "        self.customer_visited(destination)\n",
        "        done = (destination == 'S' and self.all_customers_visited())\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += self.reward\n",
        "        self.stepCount += 1\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "        info = {\"from\": source, \"to\": destination}\n",
        "        return observation, self.reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = np.array([position,\n",
        "                           self.getPathObservation(position, 0),\n",
        "                           self.getPathObservation(position, 1),\n",
        "                           self.getPathObservation(position, 2),\n",
        "                           self.getPathObservation(position, 3)\n",
        "                           ],\n",
        "                          dtype=np.float32)\n",
        "        all_rest_rewards = list(self.customer_reward.values())\n",
        "        result = np.append(result, all_rest_rewards)\n",
        "        return result\n",
        "\n",
        "    def getPathObservation(self, position, path):\n",
        "        paths = self.map[self.state]\n",
        "        if path < len(paths):\n",
        "            target, cost = paths[path]\n",
        "            reward = self.customer_reward[target]\n",
        "            result = reward - cost\n",
        "        else:\n",
        "            result = -1000\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "    # each node gets a reward, either 0 or 10000\n",
        "    def modulate_reward(self):\n",
        "        self.customer_reward = {}\n",
        "\n",
        "        node_names = list(self.map.keys())\n",
        "        # initialize all nodes with 0\n",
        "        for node_name in node_names:\n",
        "            self.customer_reward[node_name] = 0\n",
        "\n",
        "        # 10000 rewards are only at a few random places\n",
        "        number_of_customers = len(self.map) - 1\n",
        "        number_per_consultant = int(number_of_customers / 2)\n",
        "        self._honeypot_places = []\n",
        "\n",
        "        # starting from 1, not 0, so that 'S' (position of the cave) never gets a reward \n",
        "        samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
        "        for sample in samples:\n",
        "            self.customer_reward[node_names[sample]] = 1000\n",
        "            self._honeypot_places = self._honeypot_places + [node_names[sample]]\n",
        "\n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "        self.state = 'S'\n",
        "        self.cost = 0\n",
        "        self.action = 0\n",
        "        self.lastStep_state = ''\n",
        "        self.customerReward = None\n",
        "        self._honeypot_places = None\n",
        "        self.reward = 0\n",
        "        self.envEpisodeCount += 1\n",
        "        self.modulate_reward()\n",
        "        self._figure = None\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "\n",
        "    def _create_nx_graph(self):\n",
        "        \"\"\" generates the networkx graph representing orso's world with all its paths.\n",
        "\n",
        "        :return: graph, positions\n",
        "        \"\"\"\n",
        "        nx_graph = nx.Graph()\n",
        "        for node_id in self.map.keys():\n",
        "            zoom = 0.6\n",
        "            image = self.image_empty_pot\n",
        "            nx_graph.add_node(node_id, image=image, zoom=zoom)\n",
        "        for source, connections in self.map.items():\n",
        "            for action, (target, cost) in enumerate(connections):\n",
        "                if cost >= 300:\n",
        "                    color = 'dodgerblue'\n",
        "                elif cost >= 200:\n",
        "                    color = 'darkgoldenrod'\n",
        "                elif cost >= 100:\n",
        "                    color = 'forestgreen'\n",
        "                else:\n",
        "                    color = 'greenyellow'\n",
        "                directed_label = str(source) + \":\" + str(action)\n",
        "                existing_edge = nx_graph.get_edge_data(source, target)\n",
        "                if existing_edge is not None: \n",
        "                    directed_label = str(existing_edge['label']) + ' - ' + directed_label\n",
        "                nx_graph.add_edge(source, target, color=color, weight=6, cost=cost, label=directed_label, image=self.image_cave)\n",
        "        nx_pos = nx.kamada_kawai_layout(nx_graph)\n",
        "        return nx_graph, nx_pos\n",
        "\n",
        "    def _render_to_figure(self, render_graph_labels=False, render_costs=False):\n",
        "        \"\"\" Renders the current state as a graph with matplotlib\n",
        "        \"\"\"\n",
        "        # draw graph using matplotlib\n",
        "        if (self._figure is not None):\n",
        "            plt.close(self._figure)\n",
        "        self._figure = plt.figure()\n",
        "        if len(self._figure.axes) == 0:\n",
        "            self._figure.add_subplot(1, 1, 1)\n",
        "        self._figure.axes[0].cla()\n",
        "        ax = self._figure.axes[0]\n",
        "\n",
        "        edges = self.nx_graph.edges()\n",
        "        edge_colors = [self.nx_graph[u][v]['color'] for u, v in edges]\n",
        "        edge_weights = [self.nx_graph[u][v]['weight'] for u, v in edges]\n",
        "\n",
        "        nx.draw(self.nx_graph, pos=self.nx_pos, ax=ax, node_color='lightgrey',\n",
        "                edges=edges, edge_color=edge_colors, width=edge_weights, with_labels=render_graph_labels)\n",
        "\n",
        "        if render_graph_labels:\n",
        "          edge_labels = [self.nx_graph[u][v]['label'] for u, v in edges]\n",
        "          zipped_edge_labels = dict(zip(edges, edge_labels))\n",
        "          nx.draw_networkx_edge_labels(self.nx_graph, pos=self.nx_pos, ax=ax, edge_labels=zipped_edge_labels)\n",
        "\n",
        "        if render_costs:\n",
        "          edge_costs = [self.nx_graph[u][v]['cost'] for u, v in edges]\n",
        "          zipped_edge_labels = dict(zip(edges, edge_costs))\n",
        "          nx.draw_networkx_edge_labels(self.nx_graph, pos=self.nx_pos, ax=ax, edge_labels=zipped_edge_labels)\n",
        "\n",
        "        # draw images on graph nodes\n",
        "        # set image (according to the current state) and sizes (make orso's current position larger)\n",
        "        for node_id in self.nx_graph.nodes():\n",
        "            node = self.nx_graph.node[node_id]\n",
        "            node['zoom'] = 0.4\n",
        "            if node_id == self.state:\n",
        "                node['zoom'] = 0.6\n",
        "            if node_id in self._honeypot_places:\n",
        "                node['image'] = self.image_empty_pot\n",
        "                if self.customer_reward[node_id] > 0:\n",
        "                    node['image'] = self.image_honey\n",
        "            else:\n",
        "                node['image'] = None\n",
        "            if node_id == 'S':\n",
        "                node['image'] = self.image_cave\n",
        "            if self.state == node_id:\n",
        "                node['image'] = self.image_orso\n",
        "\n",
        "        # position images\n",
        "        for n in self.nx_pos:\n",
        "            node = self.nx_graph.node[n]\n",
        "            image = node['image']\n",
        "            if image is not None: \n",
        "                xp, yp = self.nx_pos[n]\n",
        "                offset_image = OffsetImage(image, node['zoom'])\n",
        "                offset_image.image.axes = ax\n",
        "                ab = AnnotationBbox(offset_image, (xp, yp),\n",
        "                                    xybox=(0, 0),\n",
        "                                    xycoords='data',\n",
        "                                    boxcoords=\"offset points\",\n",
        "                                    pad=0.0,\n",
        "                                    frameon=False\n",
        "                                    )\n",
        "                ax.add_artist(ab)\n",
        "\n",
        "        self._figure.canvas.draw()\n",
        "\n",
        "    def _render_ansi(self):\n",
        "        result = (\"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) +\n",
        "                  \" Step: \" + (\"%4.0f  \" % self.stepCount) +\n",
        "                  self.lastStep_state + ' --' + str(self.action) + '-> ' + self.state +\n",
        "                  ' R=' + (\"% 2.2f\" % self.reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) +\n",
        "                  ' cost=' + (\"%4.0f\" % self.cost) + ' customerR=' + (\"%4.0f\" % self.customerReward) + ' optimum=' + (\n",
        "                          \"%4.0f\" % self.optimum)\n",
        "                  )\n",
        "        return result\n",
        "\n",
        "    def _render_rgb(self):\n",
        "        self._render_to_figure()\n",
        "        self._figure.canvas.draw()\n",
        "        buf = self._figure.canvas.tostring_rgb()\n",
        "        num_cols, num_rows = self._figure.canvas.get_width_height()\n",
        "        plt.close(self._figure)\n",
        "        self._figure = None\n",
        "        result = np.fromstring(buf, dtype=np.uint8).reshape(num_rows, num_cols, 3)\n",
        "        return result\n",
        "\n",
        "    def render(self, mode='human', render_graph_labels=True, render_costs=False):\n",
        "        if mode == 'ansi':\n",
        "            return self._render_ansi()\n",
        "        elif mode == 'human':\n",
        "            clear_output(wait=True)\n",
        "            self._render_to_figure(render_graph_labels=render_graph_labels, render_costs=render_costs)\n",
        "            plt.pause(0.01)\n",
        "            return\n",
        "        elif mode == 'rgb_array':\n",
        "            return self._render_rgb()\n",
        "        else:\n",
        "            super().render(mode=mode)\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fHrBrxxI5Rt",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "env = OrsoEnv()\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s2RYX2_2I5Ry",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "observation, reward, done, info = env.step(0)\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMtVpLo9UGM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation, reward, done, info = env.step(0)\n",
        "env.render()\n",
        "observation, reward, done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CK2tTbjMsayV"
      },
      "source": [
        "### Register with OpenAI Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OS3pnnbAI5R9",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "env_name=\"Orso-v1\"\n",
        "\n",
        "env = gym.envs.registration.register(id=env_name,entry_point=OrsoEnv, max_episode_steps=25)\n",
        "\n",
        "# two envs for train and eval\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "# converted to tf-agents tensorflow envs\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p8fxHl11sayY",
        "colab": {}
      },
      "source": [
        "time_step = train_env.reset()\n",
        "observation = time_step.observation\n",
        "observation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1-ZNIgb_f-I6"
      },
      "source": [
        "### What you need using tf-agents while training\n",
        "\n",
        "![TF-Agents overview](https://github.com/christianhidber/easyagents/blob/master/jupyter_notebooks/images/tf-agents-overview.png?raw=1)\n",
        "\n",
        "https://www.youtube.com/watch?v=tAOApRQAgpc\n",
        "<br>\n",
        "https://www.youtube.com/watch?v=-TTziY7EmUA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RwLuXyAesayZ"
      },
      "source": [
        "## PPO uses two networks\n",
        "\n",
        "_given an observation_\n",
        "* Actor Network: which action to take?\n",
        "* Value Network: what reward to expect?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ZOrCWj8sayf"
      },
      "source": [
        "### Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lyhesiZksayf",
        "colab": {}
      },
      "source": [
        "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=(500, 500, 500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R05Fjxh6sayh",
        "colab": {}
      },
      "source": [
        "train_env.action_spec()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gIxzIGAusayj",
        "colab": {}
      },
      "source": [
        "# https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Categorical\n",
        "distribution, _ = actor_net(observation, time_step.step_type, None)\n",
        "num_categories = len(distribution.logits.numpy()[0])\n",
        "num_categories, distribution.logits.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6QQ5i1Tvsayl",
        "colab": {}
      },
      "source": [
        "# greedy, highest category\n",
        "distribution.mode().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jd0X4B8tsaya"
      },
      "source": [
        "### Value Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htwga3NQ5Ufb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "value_net = value_network.ValueNetwork(train_env.observation_spec(), \n",
        "                                       fc_layer_params=(500, 500, 500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Qb5MccTsayb",
        "colab": {}
      },
      "source": [
        "value_net(observation)[0].numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "34LlkBsEsayd",
        "colab": {}
      },
      "source": [
        "train_env.observation_spec()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vT_srWKysayn"
      },
      "source": [
        "### Custom Network (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRFUpAw4sayn"
      },
      "source": [
        "#### plain network using Keras layer API\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_4qUMpfesayo",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.4),\n",
        "    \n",
        "    tf.keras.layers.Dense(250, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.4)\n",
        "])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lbKJH413sayq"
      },
      "source": [
        "#### glue code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ol4K0USZsayq",
        "colab": {}
      },
      "source": [
        "from tf_agents.utils import nest_utils\n",
        "\n",
        "class CustomActorDistributionNetwork(actor_distribution_network.ActorDistributionNetwork):\n",
        "    def __init__(self,\n",
        "               input_tensor_spec,\n",
        "               output_tensor_spec,\n",
        "               custom_hidden_model):\n",
        "\n",
        "        super(CustomActorDistributionNetwork, self).__init__(input_tensor_spec, output_tensor_spec)\n",
        "        self._custom_hidden_model = custom_hidden_model\n",
        "\n",
        "    def call(self, observations, step_type, network_state):\n",
        "        state = self._custom_hidden_model(observations)\n",
        "        outer_rank = nest_utils.get_outer_rank(observations, self.input_tensor_spec)\n",
        "        output_actions = tf.nest.map_structure(\n",
        "            lambda proj_net: proj_net(state, outer_rank), self._projection_networks)\n",
        "        return output_actions, network_state\n",
        "\n",
        "\n",
        "custom_actor_net = CustomActorDistributionNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    custom_hidden_model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DajwUCIXsays"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UGpyj8wAsays",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4)\n",
        "\n",
        "tf_ppo_agent = ppo_agent.PPOAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    actor_net=actor_net,\n",
        "    # actor_net=custom_actor_net,\n",
        "    value_net=value_net,\n",
        "    optimizer=optimizer,\n",
        "#     importance_ratio_clipping=.2,\n",
        "#     initial_adaptive_kl_beta=0,\n",
        "#     entropy_regularization=0.1,\n",
        "#     value_pred_loss_coef=0.01,\n",
        "    num_epochs=10)\n",
        "tf_ppo_agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OY8fAQHZsayw"
      },
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GIiR9QXHsayw",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=tf_ppo_agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=10001)\n",
        "\n",
        "avg_return = tf_metrics.AverageReturnMetric()\n",
        "avg_length = tf_metrics.AverageEpisodeLengthMetric()\n",
        "\n",
        "collect_driver = DynamicEpisodeDriver(train_env,\n",
        "                                      tf_ppo_agent.collect_policy,\n",
        "                                      observers=[replay_buffer.add_batch, avg_return, avg_length],\n",
        "                                      num_episodes=10)\n",
        "\n",
        "# just for optimized execution\n",
        "tf_ppo_agent.train = common.function(tf_ppo_agent.train)\n",
        "collect_driver.run = common.function(collect_driver.run)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ojzPyOIosayz"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "1. collect data by playing\n",
        "1. train with that data\n",
        "1. erase all data \n",
        "1. repeat for a number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq_cBo2V4Mf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collect_driver.run()\n",
        "trajectories = replay_buffer.gather_all()\n",
        "\n",
        "loss_info = tf_ppo_agent.train(trajectories)\n",
        "loss_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0PuAqMfisay0",
        "colab": {}
      },
      "source": [
        "num_iterations = 100  # @param {type:\"integer\"}\n",
        "num_iterations_between_log = 5 # @param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U3TW7bzpsay2",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "losses = []\n",
        "policy_gradient_losses = []\n",
        "value_estimation_losses = []\n",
        "kl_losses = []\n",
        "entropy_losses = []\n",
        "\n",
        "returns = []\n",
        "lengths = []\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    \n",
        "    # play to collect\n",
        "    collect_driver.run()\n",
        "    trajectories = replay_buffer.gather_all()\n",
        "    \n",
        "    # train with a new set of trajectories\n",
        "    loss_info = tf_ppo_agent.train(trajectories)\n",
        "    # and clear afterwards\n",
        "    replay_buffer.clear()\n",
        "\n",
        "    # just tracking of losses and other metrics\n",
        "    total_loss = loss_info.loss.numpy()\n",
        "    losses.append(total_loss)\n",
        "\n",
        "    policy_gradient_loss = loss_info.extra.policy_gradient_loss.numpy()\n",
        "    policy_gradient_losses.append(policy_gradient_loss)\n",
        "    \n",
        "    value_estimation_loss = loss_info.extra.value_estimation_loss.numpy()\n",
        "    value_estimation_losses.append(value_estimation_loss)\n",
        "\n",
        "    kl_loss = loss_info.extra.kl_penalty_loss.numpy()\n",
        "    kl_losses.append(kl_loss)\n",
        "\n",
        "    entropy_losses.append(loss_info.extra.entropy_regularization_loss.numpy())\n",
        "\n",
        "    returns.append(avg_return.result().numpy())\n",
        "    lengths.append(avg_length.result().numpy())    \n",
        "\n",
        "    # logging\n",
        "    step = tf_ppo_agent.train_step_counter.numpy()\n",
        "    if iteration % num_iterations_between_log == 0:\n",
        "        print('iteration = {4}: loss = {1}, avg return = {2}, avg length = {3}'.format(\n",
        "            step, total_loss, avg_return.result().numpy(), avg_length.result().numpy(), iteration))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kWQg0sLbsay4"
      },
      "source": [
        "### How did the training go?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H_WMFpUssay6",
        "colab": {}
      },
      "source": [
        "steps = range(0, num_iterations)\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.plot(steps, losses)\n",
        "plt.title('Total loss (log)')\n",
        "plt.ylabel('Total loss')\n",
        "plt.xlabel('Iteration');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD_g08biYOQb",
        "colab_type": "text"
      },
      "source": [
        "### Loss Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upzc0ZKi4MgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XewNuuS4MgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min(losses), max(losses), statistics.mean(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XxCqsIDYiUnL",
        "colab": {}
      },
      "source": [
        "steps = range(0, num_iterations)\n",
        "\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.plot(steps, policy_gradient_losses)\n",
        "plt.title('Actor loss (log)')\n",
        "plt.ylabel('Actor loss')\n",
        "plt.xlabel('Iteration');\n",
        "# plt.savefig('actor-loss-high-lr.png', transparent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEbzmzSW4MgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min(policy_gradient_losses), max(policy_gradient_losses), statistics.mean(policy_gradient_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JXo1kOlRiX8a",
        "colab": {}
      },
      "source": [
        "steps = range(0, num_iterations)\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.plot(steps, value_estimation_losses)\n",
        "plt.title('Critic loss (log)')\n",
        "plt.ylabel('Critic loss')\n",
        "plt.xlabel('Iteration');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVv1bW104MgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min(value_estimation_losses), max(value_estimation_losses), statistics.mean(value_estimation_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0SWW1nn4MgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps = range(0, num_iterations)\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.plot(steps, kl_losses)\n",
        "plt.title('KL loss (log)')\n",
        "plt.ylabel('KL loss')\n",
        "plt.xlabel('Iteration');\n",
        "# plt.savefig('kl-loss-high-lr.png', transparent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GotKtPN84MgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min(kl_losses), max(kl_losses), statistics.mean(kl_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqgJEojH4MgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps = range(0, num_iterations)\n",
        "\n",
        "plt.plot(steps, entropy_losses)\n",
        "plt.title('Entropy loss')\n",
        "plt.ylabel('Entropy loss')\n",
        "plt.xlabel('Iteration');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jUhnxvC4MgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min(entropy_losses), max(entropy_losses), statistics.mean(entropy_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UqLCc8_YaV0",
        "colab_type": "text"
      },
      "source": [
        "### Returns & Lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3RyISuOCsay8",
        "colab": {}
      },
      "source": [
        "steps = range(0, num_iterations)\n",
        "plt.plot(steps, returns)\n",
        "plt.title('Average Return')\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iteration');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgToQfE54MgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min(returns), max(returns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u9AnUC4Ssay-",
        "colab": {}
      },
      "source": [
        "steps = range(0, num_iterations)\n",
        "\n",
        "plt.plot(steps, lengths)\n",
        "plt.title('Length of episode')\n",
        "plt.ylabel('Length of episode')\n",
        "plt.xlabel('Iteration');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e14YxJhi4MgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min(lengths), max(lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wbsoVSpxsazA"
      },
      "source": [
        "### Trying out the policy network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLFQ582k-pYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_step = eval_env.reset()\n",
        "observation = time_step.observation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhlFJovy_aci",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "_for play we just need the policy_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U3xjpWkvsazA",
        "colab": {}
      },
      "source": [
        "# not a collect policy, we are trying our best here\n",
        "policy = tf_ppo_agent.policy\n",
        "policy.name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki133L9m_hYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(time_step.observation)\n",
        "action_step = policy.action(time_step)\n",
        "print(action_step.action.numpy()[0])  \n",
        "time_step = eval_env.step(action_step.action)\n",
        "print(time_step.observation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6dd7boGasazE",
        "colab": {}
      },
      "source": [
        "while not time_step.is_last():\n",
        "#   print(time_step.observation.numpy())\n",
        "  action_step = policy.action(time_step)\n",
        "  print(action_step.action.numpy()[0])  \n",
        "  time_step = eval_env.step(action_step.action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aJVr0AqWsazF"
      },
      "source": [
        "### Let's finally watch our bear (most likely) fail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0u4jLAvBsazG",
        "colab": {}
      },
      "source": [
        "num_episodes = 1\n",
        "\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "video_filename = 'imageio.mp4'\n",
        "with imageio.get_writer(video_filename, fps=5) as video:\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = eval_env.reset()\n",
        "    video.append_data(eval_py_env.render())\n",
        "    while not time_step.is_last():\n",
        "      action_step = tf_ppo_agent.policy.action(time_step)\n",
        "#       print(action_step.action.numpy()[0])  \n",
        "      time_step = eval_env.step(action_step.action)\n",
        "      video.append_data(eval_py_env.render())\n",
        "\n",
        "embed_mp4(video_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULEgiCHmyElx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}