{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "easyagents_line_solution.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/workshop/easyagents_line_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Line World\n",
    "\n",
    "* an agent lives in a finite linear world of uneven elements\n",
    "* at each moment it is in a certain position\n",
    "* initial position is the middle\n",
    "* some positions gain rewards, some don't\n",
    "* rewards are between 0 and 15\n",
    "* agent can either move left or right\n",
    "* Objective: maximize total reward = sum(rewards) + sum(steps)\n",
    "* Cost per step: -1\n",
    "* Done Condition: agent is at pos 0 or total reward <= -20\n",
    "\n",
    "<img src='https://github.com/christianhidber/easyagents/blob/master/jupyter_notebooks/workshop/line-world.png?raw=1'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Install gym, tensorflow, tf-agents,..., setup display"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "QFQXRNBFI5Re",
    "pycharm": {
     "is_executing": false
    },
    "outputId": "08c26324-22ba-448e-d132-d82a493d7e60",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install easyagents >/dev/null\n",
    "    !pip install -q --ignore-installed tensorflow-probability==0.7.0 2>/dev/null\n",
    "    !pip install -q networkx==2.3.0 "
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**=> Restart notebook to activate the downgraded tensorflow-probability dependency.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LYBf4wft6RYG",
    "colab_type": "code",
    "outputId": "a38a278d-7756-42a0-b570-387842f3c156",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    }
   },
   "source": [
    "import easyagents\n",
    "easyagents.__version__"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aS8yqznR8UL7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### suppress package warnings, in colab: load additional packages for rendering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "DcilvDdeI5Ri",
    "outputId": "d3ab7548-1dad-49c3-d0b9-ad1283bd769e",
    "pycharm": {
     "is_executing": false
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install xvfb >/dev/null\n",
    "    !pip install pyvirtualdisplay >/dev/null    \n",
    "    \n",
    "    from pyvirtualdisplay import Display\n",
    "    Display(visible=0, size=(960, 720)).start() "
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OdHyWEEEwy"
   },
   "source": [
    "# Define Gym Environment\n",
    "\n",
    "## What you need to define\n",
    "1. Actions\n",
    "1. Observation\n",
    "1. Reward\n",
    "1. Game done?\n",
    "\n",
    "## Remember how an environment looks like\n",
    "\n",
    "```python\n",
    "class MyEnv(gym.Env):\n",
    "  def __init__(self):\n",
    "      # set up the environment\n",
    "\n",
    "    def step(self, action):\n",
    "        # make the action have an impact on the environment\n",
    "        # and return the information the algorithms need\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # reset this environment to the initial state and return the according observation\n",
    "        return observation\n",
    "\n",
    "    def render(self, mode='ansi'):\n",
    "        # spits out a human readable rendering of the environment\n",
    "        if mode == 'ansi':\n",
    "            return self._render_ansi()\n",
    "        if mode == 'human':\n",
    "            return self._render_human()            \n",
    "        elif mode == 'rgb_array':\n",
    "            return self._render_rgb() \n",
    "```\n",
    "\n",
    "https://gym.openai.com/\n",
    "\n",
    "https://github.com/openai/gym/blob/master/docs/creating-environments.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcsWIabUT19a",
    "colab_type": "text"
   },
   "source": [
    "## Good sanity check\n",
    "\n",
    "_Would you as a human be able to play the game based on the observation and reward you get?_\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "6yEvuCliwJiD",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "class LineWorldEnv(gym.Env):\n",
    "\n",
    "    # constructor sets up the properties of the environment\n",
    "    # most important action space, observation space and reward range\n",
    "    def __init__(self, world=[10, 0, 0, 5, 0, 2, 15]):\n",
    "        self.world = np.array(world)\n",
    "        # the agent can perform  different actions\n",
    "        number_of_actions = 2\n",
    "        self.action_space = spaces.Discrete(number_of_actions)\n",
    "\n",
    "        self.size_of_world = len(world)\n",
    "        \n",
    "        # STEP II\n",
    "        # this needs to match size and range of what you return in\n",
    "        # get_observation \n",
    "        # definition of world is a list in self.world\n",
    "        # the length of the world is in self.size_of_world\n",
    "\n",
    "        # CHANGE HERE START\n",
    "\n",
    "        self.max_reward = 15\n",
    "        self.min_reward = 0\n",
    "        # CHANGE TWO: machting size of observation  \n",
    "        self.observation_size = self.observation_size = 1 + self.size_of_world\n",
    "\n",
    "        # CHANGE HERE END\n",
    "        \n",
    "        # the environment's current state is described by the position of the agent and the remaining rewards\n",
    "        low = np.full(self.observation_size, self.min_reward)\n",
    "        high = np.full(self.observation_size, self.max_reward)\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                            high=high,\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self.reward_range = (-1, 1)\n",
    "        # 32 is only theoretical, because we need to travel at least 9 steps, leaving us with 23 practically\n",
    "        self.optimum = self.world.sum() - 9\n",
    "\n",
    "        self._figure = None\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    # STEP I\n",
    "    # position: [self.pos]\n",
    "    # remaining rewards: self.remaining_rewards\n",
    "    # if you want to append two lists/arrays use np.append(list1, list2)\n",
    "\n",
    "    # CHANGE HERE START\n",
    "\n",
    "    def get_observation(self):\n",
    "        # CHANGE ONE: observation passed to agent is now meaningful\n",
    "        # contains current position and all remaining rewards\n",
    "        return np.append([self.pos], self.remaining_rewards)\n",
    "\n",
    "    # CHANGE HERE END\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.done = False\n",
    "        self.pos = math.floor(len(self.world) / 2)\n",
    "        self.steps = 0\n",
    "\n",
    "        self.remaining_rewards = np.array(self.world, copy=True)\n",
    "        return self.get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == LEFT and self.pos != 0:\n",
    "          self.pos -= 1\n",
    "        elif self.pos < self.size_of_world -1:\n",
    "          self.pos += 1\n",
    "\n",
    "        reward = self.remaining_rewards[self.pos] - 1\n",
    "        normalized_reward = reward / self.optimum\n",
    "        self.total_reward += normalized_reward\n",
    "        self.remaining_rewards[self.pos] = 0\n",
    "\n",
    "        if self.pos == 0 or self.total_reward * self.optimum <= -20:\n",
    "          self.done = True\n",
    "        self.steps += 1\n",
    "\n",
    "        observation = self.get_observation()\n",
    "        info = None\n",
    "        return observation, normalized_reward, self.done, info\n",
    "\n",
    "    def _render_to_ansi(self):\n",
    "        return 'position: {position}, remaining rewards: {rewards}, total reward so far: {total}, normalized total reward: {normalized_total}, steps so far: {steps}, game done: {done}'.format(\n",
    "            position=self.pos, \n",
    "            rewards=self.remaining_rewards, \n",
    "            total=self.total_reward * self.optimum, \n",
    "            normalized_total = self.total_reward,\n",
    "            done=self.done,\n",
    "            steps=self.steps)              \n",
    "\n",
    "    def _render_to_figure(self):\n",
    "        \"\"\" Renders the current state as a graph with matplotlib \"\"\"\n",
    "        if self._figure is not None:\n",
    "            plt.close(self._figure)\n",
    "        self._figure, ax = plt.subplots(1, figsize=(8, 4))\n",
    "        ax.set_ylim(bottom=-1, top=self.max_reward + 1)\n",
    "        x = np.arange(0, self.size_of_world, 1, dtype=np.uint8)\n",
    "        y = self.remaining_rewards\n",
    "        plt.plot([self.pos, self.pos], [0, 2], 'r^-')\n",
    "        ax.scatter(x, y, s=75)\n",
    "        self._figure.canvas.draw()\n",
    "        \n",
    "    def _render_to_human(self):\n",
    "        \"\"\" show render_to_figure in a jupyter cell. \n",
    "            the result of each call is rendered in the same cell\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        self._render_to_figure()\n",
    "        plt.pause(0.01)\n",
    "        \n",
    "    def _render_to_rgb(self):\n",
    "        \"\"\" convert the output of render_to_figure to a rgb_array \"\"\"\n",
    "        self._render_to_figure()\n",
    "        self._figure.canvas.draw()\n",
    "        buf = self._figure.canvas.tostring_rgb()\n",
    "        num_cols, num_rows = self._figure.canvas.get_width_height()\n",
    "        plt.close(self._figure)\n",
    "        self._figure = None\n",
    "        result = np.fromstring(buf, dtype=np.uint8).reshape(num_rows, num_cols, 3)\n",
    "        return result        \n",
    "\n",
    "    def render(self, mode='ansi'):\n",
    "        if mode == 'ansi':\n",
    "            return self._render_to_ansi()\n",
    "        elif mode == 'human':\n",
    "            return self._render_to_human()\n",
    "        elif mode == 'rgb_array':\n",
    "            return self._render_to_rgb()\n",
    "        else:\n",
    "            super().render(mode=mode)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2aXwjnTc9LV"
   },
   "source": [
    "# Hands-On 1: Try out our environment\n",
    "\n",
    "_use yourself to simulate the agent_\n",
    "\n",
    "* Create an environment and play the game to the end manually using the cells below\n",
    "* Use a sequence of left and right commands to find the best solution possible\n",
    "* Show yourself only local observations and reward for each step and choose an action\n",
    "* Would this be sufficent for a good reward?\n",
    "* What reward do you get? What is the optimum?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "_fHrBrxxI5Rt",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "env = LineWorldEnv()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "SJARCo5U9PfR",
    "outputId": "0b01cd1d-8eff-4128-aea6-7b5b849a845e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "# shows the raw observation, does look meaningful now \n",
    "env.reset()"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "stTFpBh_9R6X",
    "outputId": "96dc5283-1995-4446-844e-15fee96d6576",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "# more complete and readable information\n",
    "env.render()"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "s2RYX2_2I5Ry",
    "outputId": "b02114f5-9d61-4e9b-9a60-d9bb122c7b44",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "JdHyE42vBhbC",
    "outputId": "637855a9-04e5-405c-9c34-3414db55874d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "RvfFHbSfckCy",
    "outputId": "81dbaac8-596e-4dd5-9606-1372251e459f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    }
   },
   "source": [
    "env.render(mode='human')"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uEfpOeeI5R8"
   },
   "source": [
    "# Train policy with tfagents PpoAgent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "gFISrkwhI5SZ",
    "outputId": "87b46a67-5052-418d-df33-0392b8f3357d",
    "pycharm": {
     "is_executing": false
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    }
   },
   "source": [
    "%%time\n",
    "\n",
    "from easyagents.env import register_with_gym\n",
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import log, plot\n",
    "\n",
    "env_name = \"LineWorld-v0\"\n",
    "register_with_gym(gym_env_name=env_name, entry_point=LineWorldEnv, max_episode_steps=100)\n",
    "\n",
    "ppoAgent = PpoAgent(gym_env_name = env_name, fc_layers=(500,500,500))\n",
    "\n",
    "plots = [plot.State(), plot.Actions(), plot.StepRewards(), plot.Rewards(), plot.Steps(), plot.ToMovie()]\n",
    "ppoAgent.train(plots, \n",
    "               num_iterations = 25, \n",
    "               num_episodes_per_iteration = 10,\n",
    "               max_steps_per_episode = 20)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRoljypzdBML"
   },
   "source": [
    "# Hands-On 2: Define the observation in a way that allows the agent to train\n",
    "\n",
    "* You see the agent does not learn with the observation given. Why is that?\n",
    "\n",
    "* Find an observation that works and impement it in `get_observation`\n",
    "\n",
    "*  You need to update the `observation_space` to the same shape of your observation at the same time.\n",
    "\n",
    "* Use env.reset() to check the observation\n",
    "\n",
    "* Make sure to re-register your environment before restarting the training\n",
    "\n",
    "* Why does it work? Is this the best observation? How would you even know?\n",
    "\n",
    "* Do you think this would be sufficient for a real world example? If not, what is missing?\n",
    "\n",
    "Â # STOP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0VqefK12wWG",
    "colab_type": "text"
   },
   "source": [
    "## Play (turn ouput into a movie)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6SSXO5H_2wWH",
    "colab_type": "code",
    "outputId": "83fbcccc-6c7e-41d7-8f59-1149cb0848dd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    }
   },
   "source": [
    "ppoAgent.play([plot.State(), plot.StepRewards(num_steps_between_plot=1), plot.Actions(), plot.ToMovie(fps=3)], \n",
    "              num_episodes=5, max_steps_per_episode=20)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLXlllMd2wWL",
    "colab_type": "text"
   },
   "source": [
    "Play uses the previously trained policy. We play for 5 episodes, each with max 20 steps. All steps are rendered into a movie with a frame rate of 3 steps per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDOWfWAmc8mx"
   },
   "source": [
    "# How does Reinforcement Learning really work?\n",
    "\n",
    "Presentation on slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uotjTcRgE2P"
   },
   "source": [
    "# Hands-On 3: Make your notebook work with Reinforce\n",
    "\n",
    "* What parts do you need to change?\n",
    "\n",
    "* How do the results compare to what you have seen with PPO?\n",
    "\n",
    "* Can you tweak the parameters (like the size of the neural network) to improve the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "IpNcF2PCaGUB",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "outputId": "f9e44a7c-df86-4cee-8577-854dc465f633"
   },
   "source": [
    "from easyagents.agents import ReinforceAgent\n",
    "\n",
    "reinforceAgent = ReinforceAgent(gym_env_name = env_name, fc_layers=(500,500,500))\n",
    "\n",
    "plots = [plot.State(), plot.Actions(), plot.StepRewards(), plot.Rewards(), plot.Steps(), plot.ToMovie()]\n",
    "reinforceAgent.train(plots, \n",
    "               num_iterations = 50, \n",
    "               num_episodes_per_iteration = 10,\n",
    "               max_steps_per_episode = 20)"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqx3EtlHzyEp",
    "colab_type": "text"
   },
   "source": [
    "# Next Steps:\n",
    "\n",
    "## Generalizing our Environment\n",
    "\n",
    "* solving a single set up of our line world is not very realistic\n",
    "* there should be variations to rewards, size, and starting position\n",
    "* while training with with each episode we could randomly set rewards and starting postition\n",
    "* different sizes could be simulated by using a fixed max size with a random number of padding entires (e.g -1) at the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM_NdaQfTowB",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "## Applying Reinforcement Learning to a problem\n",
    "\n",
    "* Is your problem approachable by Reinforcement Learning (more on that later)?\n",
    "  * Can you defined what would be the agent and what the environment?\n",
    "* You can simulate your environment or are able to safely perform a large number of experiments in the real world\n",
    "* You need to be able to define your problem as a Markov Decision Process (MDP)\n",
    "* Good sanity check: Would you as a human be able to play the game based on the observation and reward you get?\n",
    "* there are different kinds of observations\n",
    "  * fully observed environment (e.g. Jump'n'Run Game)\n",
    "  * partially observed environment (e.g. Ego Shooter)\n",
    "* Choose a proper Reinforcement Learning Algorithm (more on that later)\n",
    "\n",
    "## MDP: Markov Decision Process in Reinforcement Learning\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Markov_decision_process\n",
    "* Model your observation in a way that which actions the agent took to get to a certain state are irrelevant\n",
    "* That means all information necessary to derive a proper action need to be in a single observation\n",
    "* The agents must not be forced to also learn the sequence of observations\n",
    "* E.g. the position of the PacMan is not enough. Which path taken results in where the ghosts are and what pills have been eaten"
   ]
  }
 ]
}