{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reinforcement Learning for Practitioners (v1.2, 19Q4) Status: under active development, breaking changes may occur. Release notes . EasyAgents is a high level reinforcement learning api focusing on ease of use and simplicity. Written in Python and running on top of established reinforcement learning libraries like tf-Agents , tensorforce or keras-rl . Environments are implemented in OpenAI gym . For an example of an industrial application of reinforcement learning see here . In collaboration with Oliver Zeigermann . Features provides the same, simple api across all libraries . Thus you can easily switch between different implementations and you don't have to learn for each of them a new api. to create and run any algorithm you only need 2 lines of code , all the parameters are named consistently across all algorithms. supports a broad set of different algorithms runs inside jupyter notebooks as well as stand-alone, easy to install requiring only a single 'pip install easyagents'. easy to understand, ready-made plots and logs to investigate the algorithms and environments behaviour Examples from easyagents.agents import PpoAgent from easyagents.callbacks import plot ppoAgent = PpoAgent ( 'CartPole-v0' ) ppoAgent . train ([ plot . State (), plot . Loss (), plot . Rewards ()]) More Detailed from easyagents.agents import PpoAgent from easyagents.callbacks import plot ppoAgent = PpoAgent ( 'Orso-v1' , fc_layers = ( 500 , 500 , 500 )) ppoAgent . train ([ plot . State (), plot . Loss (), plot . Rewards (), plot . Actions (), plot . StepRewards (), plot . Steps (), plot . ToMovie ()], learning_rate = 0.0001 , num_iterations = 500 , max_steps_per_episode = 50 ) Try it on colab 1. Introduction (CartPole on colab) : training, plotting, switching algorithms & backends. based on the classic reinforcement learning example balancing a stick on a cart. 2. Next steps & backend switching (Orso on colab) : custom training, creating a movie & switching backends. gym environment based on a routing problem. 3. Creating your own environment (LineWorld on colab) : implement a gym environment from scratch, workshop example. 4. Logging, seeding & plot clearing (on colab) : Investigate how an agents api and how it interacts with the gym environment; how to set seeds; controlling jupyter output cell clearing Available Algorithms and Backends algorithm tf-Agents tensorforce keras-rl easyagents class name CEM not available not available yes CemAgent Dqn yes yes yes DqnAgent Double Dqn open not available yes DoubleDqnAgent Dueling Dqn not available not yet available yes DuelingDqnAgent Ppo yes yes not available PpoAgent Random yes yes not available RandomAgent REINFORCE yes yes not available ReinforceAgent SAC preview not available not available SacAgent [191001] if you are interested in other algorithms, backends or hyperparameters let us know by creating an issue . We'll try our best to support you. for a documentation of the agents api see here . keras-rl is not compatible with tensorflow eager execution mode. Thus keras-rl based agents should run in a different python / jupyter notebook instance than tf-agents or tensorforce based agents. Industrial Application Geberit - a sanitary technology company with > 12'000 employees - produces in particular pipes and other parts to get rain-water of flat roofs - so called syphonic roof drainage systems . They warrant that large buildings like stadiums, airports or shopping malls do not collapse during heavy rainfalls. However it is surprisingly difficult to find the right dimensions for the pipes. It is actually so difficult, that as of today no feasable, deterministic algorithm is known. Thus traditional heuristics and classic machine learning were used to support the users in finding a suitable solution. Using reinforcement learning the failrate of the previous solution was reduced by 70%, resulting in an end-to-end success-rate of > 98%. For more details take a look at this talk . Installation Install from pypi using pip: pip install easyagents More Documentation for release notes & class diagram , for agents & api . Guiding Principles easily train, evaluate & debug policies for (you own) gym environment over \"designing new algorithms\" simple & consistent over \"flexible & powerful\" inspired by keras: same api across all algorithms support different implementations of the same algorithm extensible (pluggable backends, plots & training schemes) EasyAgents may not be ideal if you would like to leverage implementation specific advantages of an algorithm you want to do distributed or in parallel reinforcement learning Note If you have any difficulties in installing or using easyagents please let us know by creating an issue . We'll try our best to help you. Any ideas, help, suggestions, comments etc in python / open source development / reinforcement learning / whatever are more than welcome.","title":"Home"},{"location":"#reinforcement-learning-for-practitioners-v12-19q4","text":"Status: under active development, breaking changes may occur. Release notes . EasyAgents is a high level reinforcement learning api focusing on ease of use and simplicity. Written in Python and running on top of established reinforcement learning libraries like tf-Agents , tensorforce or keras-rl . Environments are implemented in OpenAI gym . For an example of an industrial application of reinforcement learning see here . In collaboration with Oliver Zeigermann .","title":"Reinforcement Learning for Practitioners (v1.2, 19Q4)"},{"location":"#features","text":"provides the same, simple api across all libraries . Thus you can easily switch between different implementations and you don't have to learn for each of them a new api. to create and run any algorithm you only need 2 lines of code , all the parameters are named consistently across all algorithms. supports a broad set of different algorithms runs inside jupyter notebooks as well as stand-alone, easy to install requiring only a single 'pip install easyagents'. easy to understand, ready-made plots and logs to investigate the algorithms and environments behaviour","title":"Features"},{"location":"#examples","text":"from easyagents.agents import PpoAgent from easyagents.callbacks import plot ppoAgent = PpoAgent ( 'CartPole-v0' ) ppoAgent . train ([ plot . State (), plot . Loss (), plot . Rewards ()])","title":"Examples"},{"location":"#more-detailed","text":"from easyagents.agents import PpoAgent from easyagents.callbacks import plot ppoAgent = PpoAgent ( 'Orso-v1' , fc_layers = ( 500 , 500 , 500 )) ppoAgent . train ([ plot . State (), plot . Loss (), plot . Rewards (), plot . Actions (), plot . StepRewards (), plot . Steps (), plot . ToMovie ()], learning_rate = 0.0001 , num_iterations = 500 , max_steps_per_episode = 50 )","title":"More Detailed"},{"location":"#try-it-on-colab","text":"1. Introduction (CartPole on colab) : training, plotting, switching algorithms & backends. based on the classic reinforcement learning example balancing a stick on a cart. 2. Next steps & backend switching (Orso on colab) : custom training, creating a movie & switching backends. gym environment based on a routing problem. 3. Creating your own environment (LineWorld on colab) : implement a gym environment from scratch, workshop example. 4. Logging, seeding & plot clearing (on colab) : Investigate how an agents api and how it interacts with the gym environment; how to set seeds; controlling jupyter output cell clearing","title":"Try it on colab"},{"location":"#available-algorithms-and-backends","text":"algorithm tf-Agents tensorforce keras-rl easyagents class name CEM not available not available yes CemAgent Dqn yes yes yes DqnAgent Double Dqn open not available yes DoubleDqnAgent Dueling Dqn not available not yet available yes DuelingDqnAgent Ppo yes yes not available PpoAgent Random yes yes not available RandomAgent REINFORCE yes yes not available ReinforceAgent SAC preview not available not available SacAgent [191001] if you are interested in other algorithms, backends or hyperparameters let us know by creating an issue . We'll try our best to support you. for a documentation of the agents api see here . keras-rl is not compatible with tensorflow eager execution mode. Thus keras-rl based agents should run in a different python / jupyter notebook instance than tf-agents or tensorforce based agents.","title":"Available Algorithms and Backends"},{"location":"#industrial-application","text":"Geberit - a sanitary technology company with > 12'000 employees - produces in particular pipes and other parts to get rain-water of flat roofs - so called syphonic roof drainage systems . They warrant that large buildings like stadiums, airports or shopping malls do not collapse during heavy rainfalls. However it is surprisingly difficult to find the right dimensions for the pipes. It is actually so difficult, that as of today no feasable, deterministic algorithm is known. Thus traditional heuristics and classic machine learning were used to support the users in finding a suitable solution. Using reinforcement learning the failrate of the previous solution was reduced by 70%, resulting in an end-to-end success-rate of > 98%. For more details take a look at this talk .","title":"Industrial Application"},{"location":"#installation","text":"Install from pypi using pip: pip install easyagents","title":"Installation"},{"location":"#more","text":"","title":"More"},{"location":"#documentation","text":"for release notes & class diagram , for agents & api .","title":"Documentation"},{"location":"#guiding-principles","text":"easily train, evaluate & debug policies for (you own) gym environment over \"designing new algorithms\" simple & consistent over \"flexible & powerful\" inspired by keras: same api across all algorithms support different implementations of the same algorithm extensible (pluggable backends, plots & training schemes)","title":"Guiding Principles"},{"location":"#easyagents-may-not-be-ideal-if","text":"you would like to leverage implementation specific advantages of an algorithm you want to do distributed or in parallel reinforcement learning","title":"EasyAgents may not be ideal if"},{"location":"#note","text":"If you have any difficulties in installing or using easyagents please let us know by creating an issue . We'll try our best to help you. Any ideas, help, suggestions, comments etc in python / open source development / reinforcement learning / whatever are more than welcome.","title":"Note"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing Interested in contributing to EasyAgents ? We appreciate all kinds of help. Pull Requests We gladly welcome pull requests . Before making any changes, we recommend opening an issue (if it doesn't already exist) and discussing your proposed changes. This will let us give you advice on the proposed changes. If the changes are minor, then feel free to make them without discussion. General Guidelines While not being dogmatic, we try to adhere to the following * Google Docstring for code documentation * PEP 8 as style guide for code * PyTest for unit testing * Typing to support type hints Supported Platforms Jupyter notebooks on colab using the latest Chrome browser Python 3.7 on Windows and Ubuntu (Xenial)","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Interested in contributing to EasyAgents ? We appreciate all kinds of help.","title":"Contributing"},{"location":"CONTRIBUTING/#pull-requests","text":"We gladly welcome pull requests . Before making any changes, we recommend opening an issue (if it doesn't already exist) and discussing your proposed changes. This will let us give you advice on the proposed changes. If the changes are minor, then feel free to make them without discussion.","title":"Pull Requests"},{"location":"CONTRIBUTING/#general-guidelines","text":"While not being dogmatic, we try to adhere to the following * Google Docstring for code documentation * PEP 8 as style guide for code * PyTest for unit testing * Typing to support type hints","title":"General Guidelines"},{"location":"CONTRIBUTING/#supported-platforms","text":"Jupyter notebooks on colab using the latest Chrome browser Python 3.7 on Windows and Ubuntu (Xenial)","title":"Supported Platforms"},{"location":"documentation/markdown/Release_Notes/","text":"Release notes v1.2 [19Q4] 1.2.2: fix for CemAgent and SacAgent default backend registration 1.2.1: SacAgent for tfagents preview; notebook on 'Agent logging, seeding and jupyter output cells' 1.2.0: Agent.score v1.1 [19Q3] 1.1.23: CemAgent for keras-rl backend; DqnAgent, RandomAgent for tensorforce 1.1.22: DuelingDqnAgent, DoubleDqnAgent with keras-rl backend 1.1.21: keras-rl backend (dqn) 1.1.20: #54 logging in jupyter notebook solved, doc updates 1.1.19: jupyter plotting performance improved plot.ToMovie with support for animated gifs 1.1.18: tensorforce backend (ppo, reinforce) 1.1.11: plot.StepRewards, plot.Actions default_plots parameter (instead of default_callbacks) v1.0.1 [19Q3] api based on pluggable backends and callbacks (for plotting, logging, training durations) backend: tf-agents, default algorithms: dqn, ppo, random plots: State, Loss (including actor-/critic loss), Steps, Rewards support for creating a mp4 movie (plot.ToMovie) v0.1 [19Q2] prototype implementation / proof of concept hard-wired support for Ppo, Reinforce, Dqn on tf-agents hard-wired plots for loss, sum-of-rewards, steps and state rendering hard-wired mp4 rendering Design guidelines separate \"public api\" from concrete implementation using a frontend / backend architecture (inspired by scikit learn, matplotlib, keras) pluggable backends extensible through callbacks (inspired by keras). separate callback types for training, evaluation and monitoring pre-configurable, algorithm specific train & play loops Class diagram","title":"Release Notes"},{"location":"documentation/markdown/Release_Notes/#release-notes","text":"v1.2 [19Q4] 1.2.2: fix for CemAgent and SacAgent default backend registration 1.2.1: SacAgent for tfagents preview; notebook on 'Agent logging, seeding and jupyter output cells' 1.2.0: Agent.score v1.1 [19Q3] 1.1.23: CemAgent for keras-rl backend; DqnAgent, RandomAgent for tensorforce 1.1.22: DuelingDqnAgent, DoubleDqnAgent with keras-rl backend 1.1.21: keras-rl backend (dqn) 1.1.20: #54 logging in jupyter notebook solved, doc updates 1.1.19: jupyter plotting performance improved plot.ToMovie with support for animated gifs 1.1.18: tensorforce backend (ppo, reinforce) 1.1.11: plot.StepRewards, plot.Actions default_plots parameter (instead of default_callbacks) v1.0.1 [19Q3] api based on pluggable backends and callbacks (for plotting, logging, training durations) backend: tf-agents, default algorithms: dqn, ppo, random plots: State, Loss (including actor-/critic loss), Steps, Rewards support for creating a mp4 movie (plot.ToMovie) v0.1 [19Q2] prototype implementation / proof of concept hard-wired support for Ppo, Reinforce, Dqn on tf-agents hard-wired plots for loss, sum-of-rewards, steps and state rendering hard-wired mp4 rendering","title":"Release notes"},{"location":"documentation/markdown/Release_Notes/#design-guidelines","text":"separate \"public api\" from concrete implementation using a frontend / backend architecture (inspired by scikit learn, matplotlib, keras) pluggable backends extensible through callbacks (inspired by keras). separate callback types for training, evaluation and monitoring pre-configurable, algorithm specific train & play loops","title":"Design guidelines"},{"location":"documentation/markdown/Release_Notes/#class-diagram","text":"","title":"Class diagram"},{"location":"reference/easyagents/agents/","text":"Module easyagents.agents This module contains the public api of the EasyAgents reinforcement learning library. It consist mainly of the class hierarchy of the available agents (algorithms), registrations and the management of the available backends. In their implementation the agents forward their calls to the chosen backend. View Source \"\"\"This module contains the public api of the EasyAgents reinforcement learning library. It consist mainly of the class hierarchy of the available agents (algorithms), registrations and the management of the available backends. In their implementation the agents forward their calls to the chosen backend. \"\"\" from abc import ABC from typing import List , Tuple , Optional , Union , Type from easyagents import core from easyagents.callbacks import plot from easyagents.backends import core as bcore import easyagents.backends.default import easyagents.backends.kerasrl import easyagents.backends.tfagents import easyagents.backends.tforce import statistics _backends : [ bcore . BackendAgentFactory ] = [] \"\"\"The seed used for all agents and gym environments. If None no seed is set (default).\"\"\" seed : Optional [ int ] = None def register_backend ( backend : bcore . BackendAgentFactory ): \"\"\"registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. \"\"\" assert backend old_backends = [ b for b in _backends if b . backend_name == backend . backend_name ] for old_backend in old_backends : _backends . remove ( old_backend ) _backends . append ( backend ) # register all backends deployed with easyagents register_backend ( easyagents . backends . default . BackendAgentFactory ()) register_backend ( easyagents . backends . tfagents . TfAgentAgentFactory ()) register_backend ( easyagents . backends . tforce . TensorforceAgentFactory ()) register_backend ( easyagents . backends . kerasrl . KerasRlAgentFactory ()) class EasyAgent ( ABC ): \"\"\"Abstract base class for all easy reinforcment learning agents.\"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ... ]] = None , backend : str = None ): \"\"\" Args: gym_env_name: name of an OpenAI gym environment to be used for training and evaluation fc_layers: defines the neural network to be used, a sequence of fully connected layers of the given size. Eg (75,40) yields a neural network consisting out of 2 hidden layers, the first one containing 75 and the second layer containing 40 neurons. backend=the backend to be used (eg 'tfagents'), if None a default implementation is used. call get_backends() to get a list of the available backends. \"\"\" self . _initialize ( gym_env_name = gym_env_name , fc_layers = fc_layers , backend_name = backend ) return def _initialize ( self , gym_env_name : str = None , fc_layers : Tuple [ int , ... ] = None , model_config : core . ModelConfig = None , backend_name : str = None ): if model_config is None : model_config = core . ModelConfig ( gym_env_name = gym_env_name , fc_layers = fc_layers ) if backend_name is None : backend_name = easyagents . backends . default . BackendAgentFactory . backend_name backend : bcore . BackendAgentFactory = _get_backend ( backend_name ) assert model_config is not None , \"model_config not set.\" assert backend , f 'Backend \"{backend_name}\" not found. The registered backends are {get_backends()}.' self . _model_config : core . ModelConfig = model_config backend_agent = backend . create_agent ( easyagent_type = type ( self ), model_config = model_config ) assert backend_agent , f 'Backend \"{backend_name}\" does not implement \"{type(self).__name__}\". ' + \\ f 'Choose one of the following backend {get_backends(type(self))}.' self . _backend_agent : Optional [ bcore . _BackendAgent ] = backend_agent return def _prepare_callbacks ( self , callbacks : List [ core . AgentCallback ], default_plots : Optional [ bool ], default_plot_callbacks : List [ plot . _PlotCallback ], ) -> List [ core . AgentCallback ]: \"\"\"Adds the default callbacks and sorts all callbacks in the order _PreProcessCallbacks, AgentCallbacks, _PostProcessCallbacks. Args: callbacks: existing callbacks to prepare default_plots: if set or if None and callbacks does not contain plots then the default plots are added default_plot_callbacks: plot callbacks to add. \"\"\" pre_process : List [ core . AgentCallback ] = [ plot . _PreProcess ()] agent : List [ core . AgentCallback ] = [] post_process : List [ core . AgentCallback ] = [ plot . _PostProcess ()] if default_plots is None : default_plots = True for c in callbacks : default_plots = default_plots and ( not isinstance ( c , plot . _PlotCallback )) if default_plots : agent = default_plot_callbacks for c in callbacks : if isinstance ( c , core . _PreProcessCallback ): pre_process . append ( c ) else : if isinstance ( c , core . _PostProcessCallback ): post_process . append ( c ) else : agent . append ( c ) result : List [ core . AgentCallback ] = pre_process + agent + post_process return result def _play ( self , play_context : core . PlayContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ]): \"\"\"Plays episodes with the current policy according to play_context. Hints: o updates rewards in play_context Args: play_context: specifies the num of episodes to play callbacks: list of callbacks called during the play of the episodes default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: play_context containing the actions taken and the rewards received during training \"\"\" assert play_context , \"play_context not set.\" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ): assert isinstance ( callbacks , core . AgentCallback ), \"callback not an AgentCallback or a list thereof.\" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ): \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ), statistics . stdev ( all ), min ( all ), max ( all ), all def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ): \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ), statistics . stdev ( all ), min ( all ), max ( all ), all def train ( self , train_context : core . TrainContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ]): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \"train_context not set.\" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ): assert isinstance ( callbacks , core . AgentCallback ), \"callback not a AgentCallback or a list thereof.\" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Loss (), plot . Steps (), plot . Rewards ()]) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks ) def get_backends ( agent : Optional [ Type [ EasyAgent ]] = None , skip_v1 : bool = False ): \"\"\"returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. skip_v1: if set only backends compatible with tensorflow v2 compatibility mode and eager execution are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. \"\"\" backends = [ b for b in _backends if ( not skip_v1 ) or b . tensorflow_v2_eager_compatible ] result = [ b . backend_name for b in backends ] if agent : result = [ b . backend_name for b in backends if agent in b . get_algorithms ()] return result def _get_backend ( backend_name : str ): \"\"\"Yields the backend with the given name. Returns: the backend instance or None if no backend is found.\"\"\" assert backend_name backends = [ b for b in _backends if b . backend_name == backend_name ] assert len ( backends ) <= 1 , f 'no backend found with name \"{backend_name}\". Available backends = {get_backends()}' result = None if backends : result = backends [ 0 ] return result class CemAgent ( EasyAgent ): \"\"\"creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0.1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class DqnAgent ( EasyAgent ): \"\"\"creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class DoubleDqnAgent ( DqnAgent ): \"\"\"Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)\"\"\" class DuelingDqnAgent ( DqnAgent ): \"\"\"Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).\"\"\" class PpoAgent ( EasyAgent ): \"\"\"creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class RandomAgent ( EasyAgent ): \"\"\"Agent which always chooses uniform random actions.\"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ): \"\"\"Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class ReinforceAgent ( EasyAgent ): \"\"\"creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class SacAgent ( DqnAgent ): \"\"\"Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).\"\"\" Variables seed Functions get_backends def get_backends ( agent : Union [ Type [ easyagents . agents . EasyAgent ], NoneType ] = None , skip_v1 : bool = False ) returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. skip_v1: if set only backends compatible with tensorflow v2 compatibility mode and eager execution are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. View Source def get_backends ( agent : Optional [ Type [ EasyAgent ]] = None , skip_v1 : bool = False ) : \"\"\" returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args : agent : type deriving from EasyAgent for which the backend identifiers are returned . skip_v1 : if set only backends compatible with tensorflow v2 compatibility mode and eager execution are returned . Returns : a list of admissible values for the ' backend ' argument of EazyAgents constructors or a list of all available backends if agent is None . \"\"\" backends = [ b for b in _backends if ( not skip_v1 ) or b . tensorflow_v2_eager_compatible ] result = [ b . backend_name for b in backends ] if agent : result = [ b . backend_name for b in backends if agent in b . get_algorithms () ] return result register_backend def register_backend ( backend : easyagents . backends . core . BackendAgentFactory ) registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. View Source def register_backend ( backend : bcore . BackendAgentFactory ) : \"\"\" registers a backend as a factory for agent implementations. If another backend with the same name is already registered , the old backend is replaced by backend . \"\"\" assert backend old_backends = [ b for b in _backends if b . backend_name == backend . backend_name ] for old_backend in old_backends : _backends . remove ( old_backend ) _backends . append ( backend ) Classes CemAgent class CemAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf View Source class CemAgent ( EasyAgent ) : \"\"\" creates a new agent based on the cross-entropy-method algorithm. From https : // learning . mpi - sws . org / mlss2016 / slides / 2016 - MLSS - RL . pdf : Initialize \u00b5 \u2208 Rd ,\u03c3 \u2208 Rd for iteration = 1 , 2 ,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8 i \u223c N ( \u00b5, diag ( \u03c3 )) Perform a noisy evaluation Ri \u223c \u03b8 i Select the top elite_set_fraction of samples ( e . g . p = 0 . 2 ) , which we \u2019 ll call the elite set Fit a Gaussian distribution , with diagonal covariance , to the elite set , obtaining a new \u00b5,\u03c3. end for Return the \ufb01 nal \u00b5. see https : // citeseerx . ist . psu . edu / viewdoc / download ? doi = 10 . 1 . 1 . 81 . 6579 & rep = rep1 & type = pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0 . 1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played in each iteration . for each episode a new policy is sampled from the current weight distribution . max_steps_per_episode : maximum number of steps per episode elite_set_fraction : the fraction of policies which are members of the elite set . These policies are used to fit a new weight distribution in each iteration . num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0.1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : easyagents . core . CemTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0 . 1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played in each iteration . for each episode a new policy is sampled from the current weight distribution . max_steps_per_episode : maximum number of steps per episode elite_set_fraction : the fraction of policies which are members of the elite set . These policies are used to fit a new weight distribution in each iteration . num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context DoubleDqnAgent class DoubleDqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461) View Source class DoubleDqnAgent ( DqnAgent ): \"\"\"Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)\"\"\" Ancestors (in MRO) easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context DqnAgent class DqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning View Source class DqnAgent ( EasyAgent ) : \"\"\" creates a new agent based on the Dqn algorithm. From wikipedia : The DeepMind system used a deep convolutional neural network , with layers of tiled convolutional filters to mimic the effects of receptive fields . Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q . This instability comes from the correlations present in the sequence of observations , the fact that small updates to Q may significantly change the policy and the data distribution , and the correlations between Q and the target values . The technique used experience replay , a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed .[ 2 ] This removes correlations in the observation sequence and smooths changes in the data distribution . Iterative update adjusts Q towards target values that are only periodically updated , further reducing correlations with the target .[ 17 ] see also : https : // deepmind . com / research / publications / human - level - control - through - deep - reinforcement - learning \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Descendants easyagents.agents.DoubleDqnAgent easyagents.agents.DuelingDqnAgent easyagents.agents.SacAgent Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context DuelingDqnAgent class DuelingDqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581). View Source class DuelingDqnAgent ( DqnAgent ): \"\"\"Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).\"\"\" Ancestors (in MRO) easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context EasyAgent class EasyAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Abstract base class for all easy reinforcment learning agents. View Source class EasyAgent ( ABC ) : \"\"\" Abstract base class for all easy reinforcment learning agents. \"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ...]] = None , backend : str = None ) : \"\"\" Args : gym_env_name : name of an OpenAI gym environment to be used for training and evaluation fc_layers : defines the neural network to be used , a sequence of fully connected layers of the given size . Eg ( 75 , 40 ) yields a neural network consisting out of 2 hidden layers , the first one containing 75 and the second layer containing 40 neurons . backend = the backend to be used ( eg ' tfagents ' ) , if None a default implementation is used . call get_backends () to get a list of the available backends . \"\"\" self . _initialize ( gym_env_name = gym_env_name , fc_layers = fc_layers , backend_name = backend ) return def _initialize ( self , gym_env_name : str = None , fc_layers : Tuple [ int , ...] = None , model_config : core . ModelConfig = None , backend_name : str = None ) : if model_config is None : model_config = core . ModelConfig ( gym_env_name = gym_env_name , fc_layers = fc_layers ) if backend_name is None : backend_name = easyagents . backends . default . BackendAgentFactory . backend_name backend : bcore . BackendAgentFactory = _get_backend ( backend_name ) assert model_config is not None , \" model_config not set. \" assert backend , f ' Backend \"{backend_name}\" not found. The registered backends are {get_backends()}. ' self . _model_config : core . ModelConfig = model_config backend_agent = backend . create_agent ( easyagent_type = type ( self ) , model_config = model_config ) assert backend_agent , f ' Backend \"{backend_name}\" does not implement \"{type(self).__name__}\". ' + \\ f ' Choose one of the following backend {get_backends(type(self))}. ' self . _backend_agent : Optional [ bcore . _BackendAgent ] = backend_agent return def _prepare_callbacks ( self , callbacks : List [ core . AgentCallback ], default_plots : Optional [ bool ], default_plot_callbacks : List [ plot . _PlotCallback ], ) -> List [ core . AgentCallback ]: \"\"\" Adds the default callbacks and sorts all callbacks in the order _PreProcessCallbacks , AgentCallbacks , _PostProcessCallbacks . Args : callbacks : existing callbacks to prepare default_plots : if set or if None and callbacks does not contain plots then the default plots are added default_plot_callbacks : plot callbacks to add . \"\"\" pre_process : List [ core . AgentCallback ] = [ plot . _PreProcess () ] agent : List [ core . AgentCallback ] = [] post_process : List [ core . AgentCallback ] = [ plot . _PostProcess () ] if default_plots is None : default_plots = True for c in callbacks : default_plots = default_plots and ( not isinstance ( c , plot . _PlotCallback )) if default_plots : agent = default_plot_callbacks for c in callbacks : if isinstance ( c , core . _PreProcessCallback ) : pre_process . append ( c ) else : if isinstance ( c , core . _PostProcessCallback ) : post_process . append ( c ) else : agent . append ( c ) result : List [ core . AgentCallback ] = pre_process + agent + post_process return result def _play ( self , play_context : core . PlayContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ] ) : \"\"\" Plays episodes with the current policy according to play_context. Hints : o updates rewards in play_context Args : play_context : specifies the num of episodes to play callbacks : list of callbacks called during the play of the episodes default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : play_context containing the actions taken and the rewards received during training \"\"\" assert play_context , \" play_context not set. \" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ) : assert isinstance ( callbacks , core . AgentCallback ) , \" callback not an AgentCallback or a list thereof. \" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Steps () , plot . Rewards () ] ) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all def train ( self , train_context : core . TrainContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ] ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during the training and evaluation train_context : training configuration to be used ( num_iterations , num_episodes_per_iteration ,... ) default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \" train_context not set. \" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ) : assert isinstance ( callbacks , core . AgentCallback ) , \" callback not a AgentCallback or a list thereof. \" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Loss () , plot . Steps () , plot . Rewards () ] ) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks ) Ancestors (in MRO) abc.ABC Descendants easyagents.agents.CemAgent easyagents.agents.DqnAgent easyagents.agents.PpoAgent easyagents.agents.RandomAgent easyagents.agents.ReinforceAgent Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , train_context : easyagents . core . TrainContext , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ], default_plots : Union [ bool , NoneType ] ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty View Source def train ( self , train_context : core . TrainContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ] ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during the training and evaluation train_context : training configuration to be used ( num_iterations , num_episodes_per_iteration ,... ) default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \" train_context not set. \" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ) : assert isinstance ( callbacks , core . AgentCallback ) , \" callback not a AgentCallback or a list thereof. \" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Loss () , plot . Steps () , plot . Rewards () ] ) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks ) PpoAgent class PpoAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html View Source class PpoAgent ( EasyAgent ) : \"\"\" creates a new agent based on the PPO algorithm. PPO is an actor - critic algorithm using 2 neural networks . The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in ( the expected , discounted sum of future rewards when following the current actor network ) . see also : https : // spinningup . openai . com / en / latest / algorithms / ppo . html \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played per training iteration max_steps_per_episode : maximum number of steps per episode num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . PpoTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played per training iteration max_steps_per_episode : maximum number of steps per episode num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context RandomAgent class RandomAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Agent which always chooses uniform random actions. View Source class RandomAgent ( EasyAgent ) : \"\"\" Agent which always chooses uniform random actions. \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ) : \"\"\" Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes . Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times a batch of num_episodes_per_eval episodes is evaluated . max_steps_per_episode : maximum number of steps per episode num_episodes_per_eval : number of episodes played to estimate the average return and steps train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : easyagents . core . TrainContext = None , default_plots : bool = None ) Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ) : \"\"\" Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes . Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times a batch of num_episodes_per_eval episodes is evaluated . max_steps_per_episode : maximum number of steps per episode num_episodes_per_eval : number of episodes played to estimate the average return and steps train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context ReinforceAgent class ReinforceAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf View Source class ReinforceAgent ( EasyAgent ) : \"\"\" creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network . see also : www - anw . cs . umass . edu /~ barto / courses / cs687 / williams92simple . pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played per training iteration max_steps_per_episode : maximum number of steps per episode num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . EpisodesTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played per training iteration max_steps_per_episode : maximum number of steps per episode num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context SacAgent class SacAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905). View Source class SacAgent ( DqnAgent ): \"\"\"Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).\"\"\" Ancestors (in MRO) easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC Methods play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context score def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"Agents"},{"location":"reference/easyagents/agents/#module-easyagentsagents","text":"This module contains the public api of the EasyAgents reinforcement learning library. It consist mainly of the class hierarchy of the available agents (algorithms), registrations and the management of the available backends. In their implementation the agents forward their calls to the chosen backend. View Source \"\"\"This module contains the public api of the EasyAgents reinforcement learning library. It consist mainly of the class hierarchy of the available agents (algorithms), registrations and the management of the available backends. In their implementation the agents forward their calls to the chosen backend. \"\"\" from abc import ABC from typing import List , Tuple , Optional , Union , Type from easyagents import core from easyagents.callbacks import plot from easyagents.backends import core as bcore import easyagents.backends.default import easyagents.backends.kerasrl import easyagents.backends.tfagents import easyagents.backends.tforce import statistics _backends : [ bcore . BackendAgentFactory ] = [] \"\"\"The seed used for all agents and gym environments. If None no seed is set (default).\"\"\" seed : Optional [ int ] = None def register_backend ( backend : bcore . BackendAgentFactory ): \"\"\"registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. \"\"\" assert backend old_backends = [ b for b in _backends if b . backend_name == backend . backend_name ] for old_backend in old_backends : _backends . remove ( old_backend ) _backends . append ( backend ) # register all backends deployed with easyagents register_backend ( easyagents . backends . default . BackendAgentFactory ()) register_backend ( easyagents . backends . tfagents . TfAgentAgentFactory ()) register_backend ( easyagents . backends . tforce . TensorforceAgentFactory ()) register_backend ( easyagents . backends . kerasrl . KerasRlAgentFactory ()) class EasyAgent ( ABC ): \"\"\"Abstract base class for all easy reinforcment learning agents.\"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ... ]] = None , backend : str = None ): \"\"\" Args: gym_env_name: name of an OpenAI gym environment to be used for training and evaluation fc_layers: defines the neural network to be used, a sequence of fully connected layers of the given size. Eg (75,40) yields a neural network consisting out of 2 hidden layers, the first one containing 75 and the second layer containing 40 neurons. backend=the backend to be used (eg 'tfagents'), if None a default implementation is used. call get_backends() to get a list of the available backends. \"\"\" self . _initialize ( gym_env_name = gym_env_name , fc_layers = fc_layers , backend_name = backend ) return def _initialize ( self , gym_env_name : str = None , fc_layers : Tuple [ int , ... ] = None , model_config : core . ModelConfig = None , backend_name : str = None ): if model_config is None : model_config = core . ModelConfig ( gym_env_name = gym_env_name , fc_layers = fc_layers ) if backend_name is None : backend_name = easyagents . backends . default . BackendAgentFactory . backend_name backend : bcore . BackendAgentFactory = _get_backend ( backend_name ) assert model_config is not None , \"model_config not set.\" assert backend , f 'Backend \"{backend_name}\" not found. The registered backends are {get_backends()}.' self . _model_config : core . ModelConfig = model_config backend_agent = backend . create_agent ( easyagent_type = type ( self ), model_config = model_config ) assert backend_agent , f 'Backend \"{backend_name}\" does not implement \"{type(self).__name__}\". ' + \\ f 'Choose one of the following backend {get_backends(type(self))}.' self . _backend_agent : Optional [ bcore . _BackendAgent ] = backend_agent return def _prepare_callbacks ( self , callbacks : List [ core . AgentCallback ], default_plots : Optional [ bool ], default_plot_callbacks : List [ plot . _PlotCallback ], ) -> List [ core . AgentCallback ]: \"\"\"Adds the default callbacks and sorts all callbacks in the order _PreProcessCallbacks, AgentCallbacks, _PostProcessCallbacks. Args: callbacks: existing callbacks to prepare default_plots: if set or if None and callbacks does not contain plots then the default plots are added default_plot_callbacks: plot callbacks to add. \"\"\" pre_process : List [ core . AgentCallback ] = [ plot . _PreProcess ()] agent : List [ core . AgentCallback ] = [] post_process : List [ core . AgentCallback ] = [ plot . _PostProcess ()] if default_plots is None : default_plots = True for c in callbacks : default_plots = default_plots and ( not isinstance ( c , plot . _PlotCallback )) if default_plots : agent = default_plot_callbacks for c in callbacks : if isinstance ( c , core . _PreProcessCallback ): pre_process . append ( c ) else : if isinstance ( c , core . _PostProcessCallback ): post_process . append ( c ) else : agent . append ( c ) result : List [ core . AgentCallback ] = pre_process + agent + post_process return result def _play ( self , play_context : core . PlayContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ]): \"\"\"Plays episodes with the current policy according to play_context. Hints: o updates rewards in play_context Args: play_context: specifies the num of episodes to play callbacks: list of callbacks called during the play of the episodes default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: play_context containing the actions taken and the rewards received during training \"\"\" assert play_context , \"play_context not set.\" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ): assert isinstance ( callbacks , core . AgentCallback ), \"callback not an AgentCallback or a list thereof.\" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ): \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ), statistics . stdev ( all ), min ( all ), max ( all ), all def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ): \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ), statistics . stdev ( all ), min ( all ), max ( all ), all def train ( self , train_context : core . TrainContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ]): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \"train_context not set.\" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ): assert isinstance ( callbacks , core . AgentCallback ), \"callback not a AgentCallback or a list thereof.\" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Loss (), plot . Steps (), plot . Rewards ()]) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks ) def get_backends ( agent : Optional [ Type [ EasyAgent ]] = None , skip_v1 : bool = False ): \"\"\"returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. skip_v1: if set only backends compatible with tensorflow v2 compatibility mode and eager execution are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. \"\"\" backends = [ b for b in _backends if ( not skip_v1 ) or b . tensorflow_v2_eager_compatible ] result = [ b . backend_name for b in backends ] if agent : result = [ b . backend_name for b in backends if agent in b . get_algorithms ()] return result def _get_backend ( backend_name : str ): \"\"\"Yields the backend with the given name. Returns: the backend instance or None if no backend is found.\"\"\" assert backend_name backends = [ b for b in _backends if b . backend_name == backend_name ] assert len ( backends ) <= 1 , f 'no backend found with name \"{backend_name}\". Available backends = {get_backends()}' result = None if backends : result = backends [ 0 ] return result class CemAgent ( EasyAgent ): \"\"\"creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0.1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class DqnAgent ( EasyAgent ): \"\"\"creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class DoubleDqnAgent ( DqnAgent ): \"\"\"Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)\"\"\" class DuelingDqnAgent ( DqnAgent ): \"\"\"Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).\"\"\" class PpoAgent ( EasyAgent ): \"\"\"creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class RandomAgent ( EasyAgent ): \"\"\"Agent which always chooses uniform random actions.\"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ): \"\"\"Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class ReinforceAgent ( EasyAgent ): \"\"\"creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class SacAgent ( DqnAgent ): \"\"\"Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).\"\"\"","title":"Module easyagents.agents"},{"location":"reference/easyagents/agents/#variables","text":"seed","title":"Variables"},{"location":"reference/easyagents/agents/#functions","text":"","title":"Functions"},{"location":"reference/easyagents/agents/#get_backends","text":"def get_backends ( agent : Union [ Type [ easyagents . agents . EasyAgent ], NoneType ] = None , skip_v1 : bool = False ) returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. skip_v1: if set only backends compatible with tensorflow v2 compatibility mode and eager execution are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. View Source def get_backends ( agent : Optional [ Type [ EasyAgent ]] = None , skip_v1 : bool = False ) : \"\"\" returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args : agent : type deriving from EasyAgent for which the backend identifiers are returned . skip_v1 : if set only backends compatible with tensorflow v2 compatibility mode and eager execution are returned . Returns : a list of admissible values for the ' backend ' argument of EazyAgents constructors or a list of all available backends if agent is None . \"\"\" backends = [ b for b in _backends if ( not skip_v1 ) or b . tensorflow_v2_eager_compatible ] result = [ b . backend_name for b in backends ] if agent : result = [ b . backend_name for b in backends if agent in b . get_algorithms () ] return result","title":"get_backends"},{"location":"reference/easyagents/agents/#register_backend","text":"def register_backend ( backend : easyagents . backends . core . BackendAgentFactory ) registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. View Source def register_backend ( backend : bcore . BackendAgentFactory ) : \"\"\" registers a backend as a factory for agent implementations. If another backend with the same name is already registered , the old backend is replaced by backend . \"\"\" assert backend old_backends = [ b for b in _backends if b . backend_name == backend . backend_name ] for old_backend in old_backends : _backends . remove ( old_backend ) _backends . append ( backend )","title":"register_backend"},{"location":"reference/easyagents/agents/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/agents/#cemagent","text":"class CemAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf View Source class CemAgent ( EasyAgent ) : \"\"\" creates a new agent based on the cross-entropy-method algorithm. From https : // learning . mpi - sws . org / mlss2016 / slides / 2016 - MLSS - RL . pdf : Initialize \u00b5 \u2208 Rd ,\u03c3 \u2208 Rd for iteration = 1 , 2 ,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8 i \u223c N ( \u00b5, diag ( \u03c3 )) Perform a noisy evaluation Ri \u223c \u03b8 i Select the top elite_set_fraction of samples ( e . g . p = 0 . 2 ) , which we \u2019 ll call the elite set Fit a Gaussian distribution , with diagonal covariance , to the elite set , obtaining a new \u00b5,\u03c3. end for Return the \ufb01 nal \u00b5. see https : // citeseerx . ist . psu . edu / viewdoc / download ? doi = 10 . 1 . 1 . 81 . 6579 & rep = rep1 & type = pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0 . 1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played in each iteration . for each episode a new policy is sampled from the current weight distribution . max_steps_per_episode : maximum number of steps per episode elite_set_fraction : the fraction of policies which are members of the elite set . These policies are used to fit a new weight distribution in each iteration . num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"CemAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0.1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : easyagents . core . CemTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0 . 1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played in each iteration . for each episode a new policy is sampled from the current weight distribution . max_steps_per_episode : maximum number of steps per episode elite_set_fraction : the fraction of policies which are members of the elite set . These policies are used to fit a new weight distribution in each iteration . num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#doubledqnagent","text":"class DoubleDqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461) View Source class DoubleDqnAgent ( DqnAgent ): \"\"\"Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)\"\"\"","title":"DoubleDqnAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_1","text":"easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play_1","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score_1","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train_1","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#dqnagent","text":"class DqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning View Source class DqnAgent ( EasyAgent ) : \"\"\" creates a new agent based on the Dqn algorithm. From wikipedia : The DeepMind system used a deep convolutional neural network , with layers of tiled convolutional filters to mimic the effects of receptive fields . Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q . This instability comes from the correlations present in the sequence of observations , the fact that small updates to Q may significantly change the policy and the data distribution , and the correlations between Q and the target values . The technique used experience replay , a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed .[ 2 ] This removes correlations in the observation sequence and smooths changes in the data distribution . Iterative update adjusts Q towards target values that are only periodically updated , further reducing correlations with the target .[ 17 ] see also : https : // deepmind . com / research / publications / human - level - control - through - deep - reinforcement - learning \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"DqnAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_2","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#descendants","text":"easyagents.agents.DoubleDqnAgent easyagents.agents.DuelingDqnAgent easyagents.agents.SacAgent","title":"Descendants"},{"location":"reference/easyagents/agents/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play_2","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score_2","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train_2","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#duelingdqnagent","text":"class DuelingDqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581). View Source class DuelingDqnAgent ( DqnAgent ): \"\"\"Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).\"\"\"","title":"DuelingDqnAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_3","text":"easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play_3","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score_3","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train_3","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#easyagent","text":"class EasyAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Abstract base class for all easy reinforcment learning agents. View Source class EasyAgent ( ABC ) : \"\"\" Abstract base class for all easy reinforcment learning agents. \"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ...]] = None , backend : str = None ) : \"\"\" Args : gym_env_name : name of an OpenAI gym environment to be used for training and evaluation fc_layers : defines the neural network to be used , a sequence of fully connected layers of the given size . Eg ( 75 , 40 ) yields a neural network consisting out of 2 hidden layers , the first one containing 75 and the second layer containing 40 neurons . backend = the backend to be used ( eg ' tfagents ' ) , if None a default implementation is used . call get_backends () to get a list of the available backends . \"\"\" self . _initialize ( gym_env_name = gym_env_name , fc_layers = fc_layers , backend_name = backend ) return def _initialize ( self , gym_env_name : str = None , fc_layers : Tuple [ int , ...] = None , model_config : core . ModelConfig = None , backend_name : str = None ) : if model_config is None : model_config = core . ModelConfig ( gym_env_name = gym_env_name , fc_layers = fc_layers ) if backend_name is None : backend_name = easyagents . backends . default . BackendAgentFactory . backend_name backend : bcore . BackendAgentFactory = _get_backend ( backend_name ) assert model_config is not None , \" model_config not set. \" assert backend , f ' Backend \"{backend_name}\" not found. The registered backends are {get_backends()}. ' self . _model_config : core . ModelConfig = model_config backend_agent = backend . create_agent ( easyagent_type = type ( self ) , model_config = model_config ) assert backend_agent , f ' Backend \"{backend_name}\" does not implement \"{type(self).__name__}\". ' + \\ f ' Choose one of the following backend {get_backends(type(self))}. ' self . _backend_agent : Optional [ bcore . _BackendAgent ] = backend_agent return def _prepare_callbacks ( self , callbacks : List [ core . AgentCallback ], default_plots : Optional [ bool ], default_plot_callbacks : List [ plot . _PlotCallback ], ) -> List [ core . AgentCallback ]: \"\"\" Adds the default callbacks and sorts all callbacks in the order _PreProcessCallbacks , AgentCallbacks , _PostProcessCallbacks . Args : callbacks : existing callbacks to prepare default_plots : if set or if None and callbacks does not contain plots then the default plots are added default_plot_callbacks : plot callbacks to add . \"\"\" pre_process : List [ core . AgentCallback ] = [ plot . _PreProcess () ] agent : List [ core . AgentCallback ] = [] post_process : List [ core . AgentCallback ] = [ plot . _PostProcess () ] if default_plots is None : default_plots = True for c in callbacks : default_plots = default_plots and ( not isinstance ( c , plot . _PlotCallback )) if default_plots : agent = default_plot_callbacks for c in callbacks : if isinstance ( c , core . _PreProcessCallback ) : pre_process . append ( c ) else : if isinstance ( c , core . _PostProcessCallback ) : post_process . append ( c ) else : agent . append ( c ) result : List [ core . AgentCallback ] = pre_process + agent + post_process return result def _play ( self , play_context : core . PlayContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ] ) : \"\"\" Plays episodes with the current policy according to play_context. Hints : o updates rewards in play_context Args : play_context : specifies the num of episodes to play callbacks : list of callbacks called during the play of the episodes default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : play_context containing the actions taken and the rewards received during training \"\"\" assert play_context , \" play_context not set. \" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ) : assert isinstance ( callbacks , core . AgentCallback ) , \" callback not an AgentCallback or a list thereof. \" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Steps () , plot . Rewards () ] ) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all def train ( self , train_context : core . TrainContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ] ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during the training and evaluation train_context : training configuration to be used ( num_iterations , num_episodes_per_iteration ,... ) default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \" train_context not set. \" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ) : assert isinstance ( callbacks , core . AgentCallback ) , \" callback not a AgentCallback or a list thereof. \" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Loss () , plot . Steps () , plot . Rewards () ] ) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks )","title":"EasyAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_4","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#descendants_1","text":"easyagents.agents.CemAgent easyagents.agents.DqnAgent easyagents.agents.PpoAgent easyagents.agents.RandomAgent easyagents.agents.ReinforceAgent","title":"Descendants"},{"location":"reference/easyagents/agents/#methods_4","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play_4","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score_4","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train_4","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ], default_plots : Union [ bool , NoneType ] ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty View Source def train ( self , train_context : core . TrainContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ] ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during the training and evaluation train_context : training configuration to be used ( num_iterations , num_episodes_per_iteration ,... ) default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \" train_context not set. \" if callbacks is None : callbacks = [] if not isinstance ( callbacks , list ) : assert isinstance ( callbacks , core . AgentCallback ) , \" callback not a AgentCallback or a list thereof. \" callbacks = [ callbacks ] callbacks = self . _prepare_callbacks ( callbacks , default_plots , [ plot . Loss () , plot . Steps () , plot . Rewards () ] ) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks )","title":"train"},{"location":"reference/easyagents/agents/#ppoagent","text":"class PpoAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html View Source class PpoAgent ( EasyAgent ) : \"\"\" creates a new agent based on the PPO algorithm. PPO is an actor - critic algorithm using 2 neural networks . The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in ( the expected , discounted sum of future rewards when following the current actor network ) . see also : https : // spinningup . openai . com / en / latest / algorithms / ppo . html \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played per training iteration max_steps_per_episode : maximum number of steps per episode num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"PpoAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_5","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_5","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play_5","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score_5","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train_5","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . PpoTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played per training iteration max_steps_per_episode : maximum number of steps per episode num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#randomagent","text":"class RandomAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Agent which always chooses uniform random actions. View Source class RandomAgent ( EasyAgent ) : \"\"\" Agent which always chooses uniform random actions. \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ) : \"\"\" Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes . Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times a batch of num_episodes_per_eval episodes is evaluated . max_steps_per_episode : maximum number of steps per episode num_episodes_per_eval : number of episodes played to estimate the average return and steps train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"RandomAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_6","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_6","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play_6","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score_6","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train_6","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : easyagents . core . TrainContext = None , default_plots : bool = None ) Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ) : \"\"\" Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes . Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times a batch of num_episodes_per_eval episodes is evaluated . max_steps_per_episode : maximum number of steps per episode num_episodes_per_eval : number of episodes played to estimate the average return and steps train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#reinforceagent","text":"class ReinforceAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf View Source class ReinforceAgent ( EasyAgent ) : \"\"\" creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network . see also : www - anw . cs . umass . edu /~ barto / courses / cs687 / williams92simple . pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played per training iteration max_steps_per_episode : maximum number of steps per episode num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"ReinforceAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_7","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_7","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play_7","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score_7","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train_7","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . EpisodesTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) num_episodes_per_iteration : number of episodes played per training iteration max_steps_per_episode : maximum number of steps per episode num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#sacagent","text":"class SacAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905). View Source class SacAgent ( DqnAgent ): \"\"\"Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).\"\"\"","title":"SacAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_8","text":"easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_8","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#play_8","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\" Plays num_episodes with the current policy. Args : callbacks : list of callbacks called during each episode play num_episodes : number of episodes to play max_steps_per_episode : max steps per episode play_context : play configuration to be used . If set override all other play context arguments default_plots : if set addes a set of default callbacks ( plot . State , plot . Rewards , ... ) Returns : play_context containg the actions taken and the rewards received during training \"\"\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . _play ( play_context = play_context , callbacks = callbacks , default_plots = default_plots ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#score_8","text":"def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: score metrics - mean, std, min, max, all View Source def score ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\" Plays num_episodes with the current policy and computes metrics on rewards. Args : num_episodes : number of episodes to play max_steps_per_episode : max steps per episode Returns : score metrics - mean , std , min , max , all \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) all = list ( play_context . sum_of_rewards . values ()) return statistics . mean ( all ) , statistics . stdev ( all ) , min ( all ) , max ( all ) , all","title":"score"},{"location":"reference/easyagents/agents/#train_8","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ) : \"\"\" Trains a new model using the gym environment passed during instantiation. Args : callbacks : list of callbacks called during training and evaluation num_iterations : number of times the training is repeated ( with additional data ) max_steps_per_episode : maximum number of steps per episode num_steps_per_iteration : number of steps played per training iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training num_iterations_between_eval : number of training iterations before the current policy is evaluated . if 0 no evaluation is performed . num_episodes_per_eval : number of episodes played to estimate the average return and steps learning_rate : the learning rate used in the next iteration ' s policy training (0,1] train_context : training configuration to be used . if set overrides all other training context arguments . default_plots : if set adds a set of default callbacks ( plot . State , plot . Rewards , plot . Loss ,... ) . if None default callbacks are only added if the callbacks list is empty Returns : train_context : the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/core/","text":"Module easyagents.core This module contains the core datastructures shared between fronten and backend like the definition of all callbacks and agent configurations. View Source \"\"\"This module contains the core datastructures shared between fronten and backend like the definition of all callbacks and agent configurations. \"\"\" from abc import ABC from typing import Optional , Dict , Tuple , List from enum import Flag , auto import math import easyagents.env import easyagents.backends.monitor import gym.core import matplotlib.pyplot as plt class GymContext ( object ): \"\"\"Contains the context for gym api call Attributes: gym_env: the target gym instance of a pending gym api call \"\"\" def __init__ ( self ): self . _monitor_env : Optional [ easyagents . backends . monitor . _MonitorEnv ] = None self . _totals = None def __str__ ( self ): return f 'MonitorEnv={self._monitor_env} Totals={self._totals}' @property def gym_env ( self ) -> Optional [ gym . core . Env ]: result = None if self . _monitor_env : result = self . _monitor_env . env return result class PlotType ( Flag ): \"\"\"Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. \"\"\" NONE = 0 PLAY_EPISODE = auto () PLAY_STEP = auto () TRAIN_EVAL = auto () TRAIN_ITERATION = auto () class PyPlotContext ( object ): \"\"\"Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure \"\"\" def __init__ ( self ): self . _created_subplots = PlotType . NONE self . figure : Optional [ plt . Figure ] = None self . figsize : ( float , float ) = ( 17 , 6 ) self . _call_jupyter_display = False self . is_jupyter_active = False self . max_columns = 3 def __str__ ( self ): figure_number = None figure_axes_len = 0 if self . figure : figure_number = self . figure . number if self . figure . axes : figure_axes_len = len ( self . figure . axes ) return f 'is_jupyter_active={self.is_jupyter_active} max_columns={self.max_columns} ' + \\ f '_created_subplots={self._created_subplots} figure={figure_number} axes={figure_axes_len} ' def _is_subplot_created ( self , plot_type : PlotType ): \"\"\"Yields true if a subplot of type plot_type was created by a plot callback.\"\"\" result = (( self . _created_subplots & plot_type ) != PlotType . NONE ) return result class ModelConfig ( object ): \"\"\"The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed \"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ... ]] = None ): \"\"\" Args: gym_env_name: the name of the registered gym environment to use, eg 'CartPole-v0' fc_layers: int tuple defining the number and size of each fully connected layer. \"\"\" if fc_layers is None : fc_layers = ( 100 , 100 ) if isinstance ( fc_layers , int ): fc_layers = ( fc_layers ,) assert isinstance ( gym_env_name , str ), \"passed gym_env_name not a string.\" assert gym_env_name != \"\" , \"gym environment name is empty.\" assert easyagents . env . _is_registered_with_gym ( gym_env_name ), \\ f '\"{gym_env_name}\" is not the name of an environment registered with OpenAI gym.' + \\ 'Consider using easyagents.env.register_with_gym to register your environment.' assert fc_layers is not None , \"fc_layers not set\" assert isinstance ( fc_layers , tuple ), \"fc_layers not a tuple\" assert fc_layers , \"fc_layers must contain at least 1 int\" for i in fc_layers : assert isinstance ( i , int ) and i >= 1 , f '{i} is not a valid size for a hidden layer' self . original_env_name = gym_env_name self . gym_env_name = None self . fc_layers = fc_layers self . seed = easyagents . agents . seed def __str__ ( self ): return f 'fc_layers={self.fc_layers} seed={self.seed} gym_env_name={self.gym_env_name}' class TrainContext ( object ): \"\"\"Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done: if true the train loop is terminated at the end of the current iteration iterations_done_in_training: the number of iterations completed so far (during training) episodes_done_in_iteration: the number of episodes completed in the current iteration episodes_done_in_training: the number of episodes completed over all iterations so far. The episodes played for evaluation are not included in this count. steps_done_in_training: the number of steps taken over all iterations so far steps_done_in_iteration: the number of steps taken in the current iteration num_iterations_between_plot: number of training iterations before plots is updated. num_iterations_between_eval: number of training iterations before the current policy is evaluated. num_episodes_per_eval: number of episodes played to estimate the average return and steps eval_rewards: dict containg the rewards statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the sum of rewards over all episodes played for the current evaluation. The dict is indexed by the current_episode. eval_steps: dict containg the steps statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the number of step over all episodes played for the current evaluation. The dict is indexed by the current_episode. loss: dict containing the loss for each iteration training. The dict is indexed by the current_episode. \"\"\" def __init__ ( self ): self . num_iterations : Optional [ int ] = None self . max_steps_per_episode : Optional = 1000 self . num_iterations_between_eval : int = 10 self . num_episodes_per_eval : int = 10 self . learning_rate : float = 0.001 self . reward_discount_gamma : float = 1.0 self . max_steps_in_buffer : int = 100000 self . training_done : bool self . iterations_done_in_training : int self . episodes_done_in_iteration : int self . episodes_done_in_training : int self . steps_done_in_training : int self . steps_done_in_iteration = 0 self . loss : Dict [ int , float ] self . eval_rewards : Dict [ int , Tuple [ float , float , float ]] self . eval_steps : Dict [ int , Tuple [ float , float , float ]] self . _reset () def __str__ ( self ): return f 'training_done={self.training_done} ' + \\ f '#iterations_done_in_training={self.iterations_done_in_training} ' + \\ f '#episodes_done_in_iteration={self.episodes_done_in_iteration} ' + \\ f '#steps_done_in_iteration={self.steps_done_in_iteration} ' + \\ f '#iterations={self.num_iterations} ' + \\ f '#max_steps_per_episode={self.max_steps_per_episode} ' + \\ f '#iterations_between_plot={self.num_iterations_between_plot} ' + \\ f '#iterations_between_eval={self.num_iterations_between_eval} ' + \\ f '#episodes_per_eval={self.num_episodes_per_eval} ' + \\ f '#learning_rate={self.learning_rate} ' + \\ f '#reward_discount_gamma={self.reward_discount_gamma} ' + \\ f '#max_steps_in_buffer={self.max_steps_in_buffer} ' def _reset ( self ): \"\"\"Clears all values modified during a train() call.\"\"\" self . training_done = False self . iterations_done_in_training = 0 self . episodes_done_in_iteration = 0 self . episodes_done_in_training = 0 self . steps_done_in_training = 0 self . steps_done_in_iteration = 0 self . loss = dict () self . eval_rewards = dict () self . eval_steps = dict () def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert self . num_iterations is None or self . num_iterations > 0 , \"num_iterations not admissible\" assert self . max_steps_per_episode > 0 , \"max_steps_per_episode not admissible\" assert self . num_iterations_between_eval > 0 , \"num_iterations_between_eval not admissible\" assert self . num_episodes_per_eval > 0 , \"num_episodes_per_eval not admissible\" assert 0 < self . learning_rate <= 1 , \"learning_rate not in interval (0,1]\" assert 0 < self . reward_discount_gamma <= 1 , \"reward_discount_gamma not in interval (0,1]\" @property def num_iterations_between_plot ( self ): \"\"\"number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. \"\"\" result = 0 if self . num_iterations_between_eval : result = math . ceil ( self . num_iterations_between_eval / 3 ) return result class EpisodesTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy \"\"\" def __init__ ( self ): self . num_episodes_per_iteration : int = 10 self . num_epochs_per_iteration : int = 10 super () . __init__ () def __str__ ( self ): return super () . __str__ () + \\ f '#episodes_per_iteration={self.num_episodes_per_iteration} ' + \\ f '#epochs_per_iteration={self.num_epochs_per_iteration} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert self . num_episodes_per_iteration > 0 , \"num_episodes_per_iteration not admissible\" assert self . num_epochs_per_iteration > 0 , \"num_epochs_per_iteration not admissible\" class CemTrainContext ( EpisodesTrainContext ): \"\"\"Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer \"\"\" def __init__ ( self ): super () . __init__ () self . num_iterations = 100 self . num_episodes_per_iteration : int = 50 self . elite_set_fraction : float = 0.1 self . num_steps_buffer_preload : int = 2000 def __str__ ( self ): return super () . __str__ () + f '#elite_set_fraction={self.elite_set_fraction} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert 1 >= self . elite_set_fraction > 0 , \"elite_set_fraction must be in interval (0,1]\" class PpoTrainContext ( EpisodesTrainContext ): \"\"\"TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. \"\"\" def __init__ ( self ): super () . __init__ () self . actor_loss : Dict [ int , float ] self . critic_loss : Dict [ int , float ] def _reset ( self ): self . actor_loss = dict () self . critic_loss = dict () super () . _reset () class StepsTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training \"\"\" def __init__ ( self ): super () . __init__ () self . num_iterations = 20000 self . num_iterations_between_eval = 1000 self . num_steps_per_iteration : int = 1 self . num_steps_buffer_preload : int = 1000 self . num_steps_sampled_from_buffer : int = 64 self . max_steps_in_buffer = 100000 def __str__ ( self ): return super () . __str__ () + \\ f '#steps_per_iteration={self.num_steps_per_iteration} ' + \\ f '#steps_buffer_preload={self.num_steps_buffer_preload} ' + \\ f '#steps_sampled_from_buffer={self.num_steps_sampled_from_buffer} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert self . num_steps_per_iteration > 0 , \"num_steps_per_iteration not admissible\" class PlayContext ( object ): \"\"\"Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions: dict containing for each episode the actions taken in each step rewards: dict containing for each episode the rewards received in each step sum_of_rewards: dict containing for each episode the sum of rewards over all steps gym_env: the gym environment used to play \"\"\" def __init__ ( self , train_context : Optional [ TrainContext ] = None ): \"\"\" Args: train_context: if set num_episodes, max_steps_per_episode and seed are set from train_context \"\"\" self . num_episodes : Optional [ int ] = None self . max_steps_per_episode : Optional [ int ] = None if train_context is not None : self . num_episodes = train_context . num_episodes_per_eval self . max_steps_per_episode = train_context . max_steps_per_episode self . play_done : bool self . episodes_done : int self . steps_done_in_episode : int self . steps_done : int self . actions : Dict [ int , List [ object ]] self . rewards : Dict [ int , List [ float ]] self . sum_of_rewards : Dict [ int , float ] self . gym_env : Optional [ gym . core . Env ] self . _reset () def __str__ ( self ): return f '#episodes={self.num_episodes} ' + \\ f 'max_steps_per_episode={self.max_steps_per_episode} ' + \\ f 'play_done={self.play_done} ' + \\ f 'episodes_done={self.episodes_done} ' + \\ f 'steps_done_in_episode={self.steps_done_in_episode} ' + \\ f 'steps_done={self.steps_done} ' def _reset ( self ): \"\"\"Clears all values modified during a train() call.\"\"\" self . play_done : bool = False self . episodes_done : int = 0 self . steps_done_in_episode : int = 0 self . steps_done : int = 0 self . actions : Dict [ int , List [ object ]] = dict () self . rewards : Dict [ int , List [ float ]] = dict () self . sum_of_rewards : Dict [ int , float ] = dict () self . gym_env : Optional [ gym . core . Env ] = None def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert ( self . num_episodes is None ) or ( self . num_episodes > 0 ), \"num_episodes not admissible\" assert ( self . max_steps_per_episode is None ) or self . max_steps_per_episode > 0 , \"max_steps_per_episode not admissible\" class AgentContext ( object ): \"\"\"Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing \"\"\" def __init__ ( self , model : ModelConfig ): \"\"\" Args: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. \"\"\" assert isinstance ( model , ModelConfig ), \"model not set\" self . model : ModelConfig = model self . train : Optional [ TrainContext ] = None self . play : Optional [ PlayContext ] = None self . gym : GymContext = GymContext () self . pyplot : PyPlotContext = PyPlotContext () def __str__ ( self ): result = f 'agent_context:' result += f ' \\n api =[{self.gym}]' if self . train is not None : result += f ' \\n train =[{self.train}] ' if self . play is not None : result += f ' \\n play =[{self.play}] ' if self . pyplot is not None : result += f ' \\n pyplot=[{self.pyplot}] ' result += f ' \\n model =[{self.model}] ' return result @property def is_eval ( self ) -> bool : \"\"\"Yields true if a policy evaluation inside an agent.train(...) call is in progress.\"\"\" return ( self . play is not None ) and ( self . train is not None ) @property def is_play ( self ) -> bool : \"\"\"Yields true if an agent.play(...) call is in progress, but not a policy evaluation\"\"\" return ( self . play is not None ) and ( self . train is None ) def _is_plot_ready ( self , plot_type : PlotType ) -> bool : \"\"\"Yields true if any of the plots in plot_type is ready to be plotted. A plot_type is ready if a plot callback was registered for this type (like TRAIN_EVAL), the agent is in runtime state corresponding to the plot type (like in training and at the end of an evaluation period) and any frequency condition is met (like num_episodes_between_plot) \"\"\" result = False if ( plot_type & PlotType . PLAY_EPISODE ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_EPISODE )) if ( plot_type & PlotType . PLAY_STEP ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_STEP )) if ( plot_type & PlotType . TRAIN_EVAL ) != PlotType . NONE : train_result = self . is_eval train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_EVAL ) train_result = train_result and ( self . play . episodes_done == self . train . num_episodes_per_eval ) result = result | train_result if ( plot_type & PlotType . TRAIN_ITERATION ) != PlotType . NONE : train_result = self . is_train train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_ITERATION ) train_result = train_result and \\ self . train . num_iterations_between_plot > 0 and \\ (( self . train . iterations_done_in_training % self . train . num_iterations_between_plot ) == 0 ) result = result | train_result return result @property def is_train ( self ) -> bool : \"\"\"Yields true if an agent.tain(...) call is in progress, but not a policy evaluation.\"\"\" return ( self . train is not None ) and ( self . play is None ) class AgentCallback ( ABC ): \"\"\"Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment\"\"\" def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" class _PostProcessCallback ( AgentCallback ): pass class _PreProcessCallback ( AgentCallback ): pass Classes AgentCallback class AgentCallback ( / , * args , ** kwargs ) Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment View Source class AgentCallback ( ABC ) : \"\"\" Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment \"\"\" def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ) : \"\"\" Logs a call to the api of the agents implementation library / framework. \"\"\" pass def on_log ( self , agent_context : AgentContext , log_msg : str ) : \"\"\" Logs a general message \"\"\" pass def on_gym_init_begin ( self , agent_context : AgentContext ) : \"\"\" called when the monitored environment begins the instantiation of a new gym environment. Args : agent_context : api_context passed to calling agent \"\"\" def on_gym_init_end ( self , agent_context : AgentContext ) : \"\"\" called when the monitored environment completed the instantiation of a new gym environment. Args : agent_context : api_context passed to calling agent \"\"\" pass def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ) : \"\"\" Before a call to gym.reset Args : agent_context : api_context passed to calling agent kwargs : the args to be passed to the underlying environment \"\"\" def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ) : \"\"\" After a call to gym.reset was completed Args : agent_context : api_context passed to calling agent reset_result : object returned by gym . reset kwargs : args passed to gym . reset \"\"\" pass def on_gym_step_begin ( self , agent_context : AgentContext , action ) : \"\"\" Before a call to gym.step Args : agent_context : api_context passed to calling agent action : the action to be passed to the underlying environment \"\"\" pass def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ) : \"\"\" After a call to gym.step was completed Args : agent_context : api_context passed to calling agent action : the action to be passed to the underlying environment step_result : ( observation , reward , done , info ) tuple returned by gym . step \"\"\" pass def on_play_episode_begin ( self , agent_context : AgentContext ) : \"\"\" Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" def on_play_episode_end ( self , agent_context : AgentContext ) : \"\"\" Called once after an episode is done or stopped (during play or eval, but not during train). \"\"\" def on_play_begin ( self , agent_context : AgentContext ) : \"\"\" Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" def on_play_end ( self , agent_context : AgentContext ) : \"\"\" Called once before exiting an agent.play() call (during play or eval, but not during train) \"\"\" def on_play_step_begin ( self , agent_context : AgentContext , action ) : \"\"\" Called once before a new step is taken in the current episode (during play or eval, but not during train). Args : agent_context : the context describing the agents current configuration action : the action to be passed to the upcoming gym_env . step call \"\"\" def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ) : \"\"\" Called once after a step is completed in the current episode (during play or eval, but not during train). \"\"\" def on_train_begin ( self , agent_context : AgentContext ) : \"\"\" Called once at the entry of an agent.train() call. \"\"\" def on_train_end ( self , agent_context : AgentContext ) : \"\"\" Called once before exiting an agent.train() call \"\"\" def on_train_iteration_begin ( self , agent_context : AgentContext ) : \"\"\" Called once at the start of a new iteration. \"\"\" def on_train_iteration_end ( self , agent_context : AgentContext ) : \"\"\" Called once after the current iteration is completed \"\"\" Ancestors (in MRO) abc.ABC Descendants easyagents.core._PostProcessCallback easyagents.core._PreProcessCallback easyagents.callbacks.plot._PlotCallback easyagents.callbacks.plot.Clear easyagents.backends.core._BackendEvalCallback easyagents.callbacks.duration.Fast easyagents.callbacks.log._LogCallbackBase easyagents.callbacks.log._CallbackCounts Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" AgentContext class AgentContext ( model : easyagents . core . ModelConfig ) Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing View Source class AgentContext ( object ) : \"\"\" Collection of state and configuration settings for a EasyAgent instance. Attributes : model : model configuration including the name of the underlying gym_environment and the policy ' s neural network archtitecture. train : training configuration and current train state . None if not inside a train call . play : play / eval configuration and current state . None if not inside a play call ( directly or due to a evaluation inside a train loop ) gym : context for gym environment related calls . pyplot : the context containing the matplotlib . pyplot figure to plot to during training or playing \"\"\" def __init__ ( self , model : ModelConfig ) : \"\"\" Args : model : model configuration including the name of the underlying gym_environment and the policy ' s neural network archtitecture. \"\"\" assert isinstance ( model , ModelConfig ) , \" model not set \" self . model : ModelConfig = model self . train : Optional [ TrainContext ] = None self . play : Optional [ PlayContext ] = None self . gym : GymContext = GymContext () self . pyplot : PyPlotContext = PyPlotContext () def __str__ ( self ) : result = f ' agent_context: ' result += f ' \\n api =[{self.gym}] ' if self . train is not None : result += f ' \\n train =[{self.train}] ' if self . play is not None : result += f ' \\n play =[{self.play}] ' if self . pyplot is not None : result += f ' \\n pyplot=[{self.pyplot}] ' result += f ' \\n model =[{self.model}] ' return result @ property def is_eval ( self ) -> bool : \"\"\" Yields true if a policy evaluation inside an agent.train(...) call is in progress. \"\"\" return ( self . play is not None ) and ( self . train is not None ) @ property def is_play ( self ) -> bool : \"\"\" Yields true if an agent.play(...) call is in progress, but not a policy evaluation \"\"\" return ( self . play is not None ) and ( self . train is None ) def _is_plot_ready ( self , plot_type : PlotType ) -> bool : \"\"\" Yields true if any of the plots in plot_type is ready to be plotted. A plot_type is ready if a plot callback was registered for this type ( like TRAIN_EVAL ) , the agent is in runtime state corresponding to the plot type ( like in training and at the end of an evaluation period ) and any frequency condition is met ( like num_episodes_between_plot ) \"\"\" result = False if ( plot_type & PlotType . PLAY_EPISODE ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_EPISODE )) if ( plot_type & PlotType . PLAY_STEP ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_STEP )) if ( plot_type & PlotType . TRAIN_EVAL ) != PlotType . NONE : train_result = self . is_eval train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_EVAL ) train_result = train_result and ( self . play . episodes_done == self . train . num_episodes_per_eval ) result = result | train_result if ( plot_type & PlotType . TRAIN_ITERATION ) != PlotType . NONE : train_result = self . is_train train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_ITERATION ) train_result = train_result and \\ self . train . num_iterations_between_plot > 0 and \\ (( self . train . iterations_done_in_training % self . train . num_iterations_between_plot ) == 0 ) result = result | train_result return result @ property def is_train ( self ) -> bool : \"\"\" Yields true if an agent.tain(...) call is in progress, but not a policy evaluation. \"\"\" return ( self . train is not None ) and ( self . play is None ) Instance variables is_eval Yields true if a policy evaluation inside an agent.train(...) call is in progress. is_play Yields true if an agent.play(...) call is in progress, but not a policy evaluation is_train Yields true if an agent.tain(...) call is in progress, but not a policy evaluation. CemTrainContext class CemTrainContext ( ) Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer View Source class CemTrainContext ( EpisodesTrainContext ) : \"\"\" Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes : elite_set_fraction : fraction of the elite policy set . num_steps_buffer_preload : number of steps performed to initially load the policy buffer \"\"\" def __init__ ( self ) : super () . __init__ () self . num_iterations = 100 self . num_episodes_per_iteration : int = 50 self . elite_set_fraction : float = 0 . 1 self . num_steps_buffer_preload : int = 2000 def __str__ ( self ) : return super () . __str__ () + f ' #elite_set_fraction={self.elite_set_fraction} ' def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" super () . _validate () assert 1 >= self . elite_set_fraction > 0 , \" elite_set_fraction must be in interval (0,1] \" Ancestors (in MRO) easyagents.core.EpisodesTrainContext easyagents.core.TrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. EpisodesTrainContext class EpisodesTrainContext ( ) Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy View Source class EpisodesTrainContext ( TrainContext ) : \"\"\" Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows : for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0 : evaluate policy if training_done break Attributes : num_episodes_per_iteration : number of episodes played per training iteration num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy \"\"\" def __init__ ( self ) : self . num_episodes_per_iteration : int = 10 self . num_epochs_per_iteration : int = 10 super () . __init__ () def __str__ ( self ) : return super () . __str__ () + \\ f ' #episodes_per_iteration={self.num_episodes_per_iteration} ' + \\ f ' #epochs_per_iteration={self.num_epochs_per_iteration} ' def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" super () . _validate () assert self . num_episodes_per_iteration > 0 , \" num_episodes_per_iteration not admissible \" assert self . num_epochs_per_iteration > 0 , \" num_epochs_per_iteration not admissible \" Ancestors (in MRO) easyagents.core.TrainContext Descendants easyagents.core.CemTrainContext easyagents.core.PpoTrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. GymContext class GymContext ( ) Contains the context for gym api call Attributes: gym_env: the target gym instance of a pending gym api call View Source class GymContext ( object ) : \"\"\" Contains the context for gym api call Attributes : gym_env : the target gym instance of a pending gym api call \"\"\" def __init__ ( self ) : self . _monitor_env : Optional [ easyagents . backends . monitor . _MonitorEnv ] = None self . _totals = None def __str__ ( self ) : return f ' MonitorEnv={self._monitor_env} Totals={self._totals} ' @ property def gym_env ( self ) -> Optional [ gym . core . Env ]: result = None if self . _monitor_env : result = self . _monitor_env . env return result Instance variables gym_env ModelConfig class ModelConfig ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None ) The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed View Source class ModelConfig ( object ) : \"\"\" The model configurations, containing the name of the gym environment and the neural network architecture. Attributes : original_env_name : the name of the underlying gym environment , eg ' CartPole-v0 ' gym_env_name : the name of the actual gym environment used ( a wrapper around the environment given by original_env_name ) fc_layers : int tuple defining the number and size of each fully connected layer . seed : the seed to be used for example for the gym_env or None for no seed \"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ...]] = None ) : \"\"\" Args : gym_env_name : the name of the registered gym environment to use , eg ' CartPole-v0 ' fc_layers : int tuple defining the number and size of each fully connected layer . \"\"\" if fc_layers is None : fc_layers = ( 100 , 100 ) if isinstance ( fc_layers , int ) : fc_layers = ( fc_layers , ) assert isinstance ( gym_env_name , str ) , \" passed gym_env_name not a string. \" assert gym_env_name != \"\" , \" gym environment name is empty. \" assert easyagents . env . _is_registered_with_gym ( gym_env_name ) , \\ f ' \"{gym_env_name}\" is not the name of an environment registered with OpenAI gym. ' + \\ ' Consider using easyagents.env.register_with_gym to register your environment. ' assert fc_layers is not None , \" fc_layers not set \" assert isinstance ( fc_layers , tuple ) , \" fc_layers not a tuple \" assert fc_layers , \" fc_layers must contain at least 1 int \" for i in fc_layers : assert isinstance ( i , int ) and i >= 1 , f ' {i} is not a valid size for a hidden layer ' self . original_env_name = gym_env_name self . gym_env_name = None self . fc_layers = fc_layers self . seed = easyagents . agents . seed def __str__ ( self ) : return f ' fc_layers={self.fc_layers} seed={self.seed} gym_env_name={self.gym_env_name} ' PlayContext class PlayContext ( train_context : Union [ easyagents . core . TrainContext , NoneType ] = None ) Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions : dict containing for each episode the actions taken in each step rewards : dict containing for each episode the rewards received in each step sum_of_rewards : dict containing for each episode the sum of rewards over all steps gym_env : the gym environment used to play View Source class PlayContext ( object ) : \"\"\" Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode . The EasyAgent . play () method proceeds ( roughly ) as follow : for e in num_episodes play ( while steps_done_in_episode < max_steps_per_episode ) if playing_done break Attributes : num_episodes : number of episodes to play , unlimited if None max_steps_per_episode : maximum number of steps per episode , unlimited if None play_done : if true the play loop is terminated at the end of the current episode episodes_done : the number of episodes played ( including the current episode ) . steps_done_in_episode : the number of steps taken in the current episode . steps_done : the number of steps played ( over all episodes so far ) actions : dict containing for each episode the actions taken in each step rewards : dict containing for each episode the rewards received in each step sum_of_rewards : dict containing for each episode the sum of rewards over all steps gym_env : the gym environment used to play \"\"\" def __init__ ( self , train_context : Optional [ TrainContext ] = None ) : \"\"\" Args : train_context : if set num_episodes , max_steps_per_episode and seed are set from train_context \"\"\" self . num_episodes : Optional [ int ] = None self . max_steps_per_episode : Optional [ int ] = None if train_context is not None : self . num_episodes = train_context . num_episodes_per_eval self . max_steps_per_episode = train_context . max_steps_per_episode self . play_done : bool self . episodes_done : int self . steps_done_in_episode : int self . steps_done : int self . actions : Dict [ int , List [ object ]] self . rewards : Dict [ int , List [ float ]] self . sum_of_rewards : Dict [ int , float ] self . gym_env : Optional [ gym . core . Env ] self . _reset () def __str__ ( self ) : return f ' #episodes={self.num_episodes} ' + \\ f ' max_steps_per_episode={self.max_steps_per_episode} ' + \\ f ' play_done={self.play_done} ' + \\ f ' episodes_done={self.episodes_done} ' + \\ f ' steps_done_in_episode={self.steps_done_in_episode} ' + \\ f ' steps_done={self.steps_done} ' def _reset ( self ) : \"\"\" Clears all values modified during a train() call. \"\"\" self . play_done : bool = False self . episodes_done : int = 0 self . steps_done_in_episode : int = 0 self . steps_done : int = 0 self . actions : Dict [ int , List [ object ]] = dict () self . rewards : Dict [ int , List [ float ]] = dict () self . sum_of_rewards : Dict [ int , float ] = dict () self . gym_env : Optional [ gym . core . Env ] = None def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" assert ( self . num_episodes is None ) or ( self . num_episodes > 0 ) , \" num_episodes not admissible \" assert ( self . max_steps_per_episode is None ) or self . max_steps_per_episode > 0 , \" max_steps_per_episode not admissible \" PlotType class PlotType ( / , * args , ** kwargs ) Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. View Source class PlotType ( Flag ): \"\"\"Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. \"\"\" NONE = 0 PLAY_EPISODE = auto () PLAY_STEP = auto () TRAIN_EVAL = auto () TRAIN_ITERATION = auto () Ancestors (in MRO) enum.Flag enum.Enum Class variables NONE PLAY_EPISODE PLAY_STEP TRAIN_EVAL TRAIN_ITERATION PpoTrainContext class PpoTrainContext ( ) TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. View Source class PpoTrainContext ( EpisodesTrainContext ) : \"\"\" TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes : actor_loss : loss observed during training of the actor network . dict is indexed by the current_episode . critic_loss : loss observed during training of the critic network . dict is indexed by the current_episode . \"\"\" def __init__ ( self ) : super () . __init__ () self . actor_loss : Dict [ int , float ] self . critic_loss : Dict [ int , float ] def _reset ( self ) : self . actor_loss = dict () self . critic_loss = dict () super () . _reset () Ancestors (in MRO) easyagents.core.EpisodesTrainContext easyagents.core.TrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. PyPlotContext class PyPlotContext ( ) Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure View Source class PyPlotContext ( object ) : \"\"\" Contain the context for the maplotlib.pyplot figure plotting. Attributes figure : the figure to plot to figsize : figure ( width , height ) in inches for the figure to be created . is_jupyter_active : True if we plot to jupyter notebook cell , False otherwise . max_columns : the max number of subplot columns in the pyplot figure \"\"\" def __init__ ( self ) : self . _created_subplots = PlotType . NONE self . figure : Optional [ plt . Figure ] = None self . figsize : ( float , float ) = ( 17 , 6 ) self . _call_jupyter_display = False self . is_jupyter_active = False self . max_columns = 3 def __str__ ( self ) : figure_number = None figure_axes_len = 0 if self . figure : figure_number = self . figure . number if self . figure . axes : figure_axes_len = len ( self . figure . axes ) return f ' is_jupyter_active={self.is_jupyter_active} max_columns={self.max_columns} ' + \\ f ' _created_subplots={self._created_subplots} figure={figure_number} axes={figure_axes_len} ' def _is_subplot_created ( self , plot_type : PlotType ) : \"\"\" Yields true if a subplot of type plot_type was created by a plot callback. \"\"\" result = (( self . _created_subplots & plot_type ) != PlotType . NONE ) return result StepsTrainContext class StepsTrainContext ( ) Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training View Source class StepsTrainContext ( TrainContext ) : \"\"\" Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows : for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0 : evaluate policy if training_done break Attributes : num_steps_per_iteration : number of steps played for each iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training \"\"\" def __init__ ( self ) : super () . __init__ () self . num_iterations = 20000 self . num_iterations_between_eval = 1000 self . num_steps_per_iteration : int = 1 self . num_steps_buffer_preload : int = 1000 self . num_steps_sampled_from_buffer : int = 64 self . max_steps_in_buffer = 100000 def __str__ ( self ) : return super () . __str__ () + \\ f ' #steps_per_iteration={self.num_steps_per_iteration} ' + \\ f ' #steps_buffer_preload={self.num_steps_buffer_preload} ' + \\ f ' #steps_sampled_from_buffer={self.num_steps_sampled_from_buffer} ' def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" super () . _validate () assert self . num_steps_per_iteration > 0 , \" num_steps_per_iteration not admissible \" Ancestors (in MRO) easyagents.core.TrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. TrainContext class TrainContext ( ) Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done : if true the train loop is terminated at the end of the current iteration iterations_done_in_training : the number of iterations completed so far ( during training ) episodes_done_in_iteration : the number of episodes completed in the current iteration episodes_done_in_training : the number of episodes completed over all iterations so far . The episodes played for evaluation are not included in this count . steps_done_in_training : the number of steps taken over all iterations so far steps_done_in_iteration : the number of steps taken in the current iteration num_iterations_between_plot : number of training iterations before plots is updated . num_iterations_between_eval : number of training iterations before the current policy is evaluated . num_episodes_per_eval : number of episodes played to estimate the average return and steps eval_rewards : dict containg the rewards statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the sum of rewards over all episodes played for the current evaluation . The dict is indexed by the current_episode . eval_steps : dict containg the steps statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the number of step over all episodes played for the current evaluation . The dict is indexed by the current_episode . loss : dict containing the loss for each iteration training . The dict is indexed by the current_episode . View Source class TrainContext ( object ) : \"\"\" Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations . Hints : o TrainContext contains all the parameters needed to control the train loop . o Subclasses of TrainContext may contain additional Agent ( but not backend ) specific parameters . Attributes : num_iterations : number of times the training is repeated ( with additional data ) , unlimited if None max_steps_per_episode : maximum number of steps per episode learning_rate : the learning rate used in the next iteration ' s policy training (0,1] reward_discount_gamma : the factor by which a reward is discounted for each step ( 0 , 1 ] max_steps_in_buffer : size of the agents buffer in steps training_done : if true the train loop is terminated at the end of the current iteration iterations_done_in_training : the number of iterations completed so far ( during training ) episodes_done_in_iteration : the number of episodes completed in the current iteration episodes_done_in_training : the number of episodes completed over all iterations so far . The episodes played for evaluation are not included in this count . steps_done_in_training : the number of steps taken over all iterations so far steps_done_in_iteration : the number of steps taken in the current iteration num_iterations_between_plot : number of training iterations before plots is updated . num_iterations_between_eval : number of training iterations before the current policy is evaluated . num_episodes_per_eval : number of episodes played to estimate the average return and steps eval_rewards : dict containg the rewards statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the sum of rewards over all episodes played for the current evaluation . The dict is indexed by the current_episode . eval_steps : dict containg the steps statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the number of step over all episodes played for the current evaluation . The dict is indexed by the current_episode . loss : dict containing the loss for each iteration training . The dict is indexed by the current_episode . \"\"\" def __init__ ( self ) : self . num_iterations : Optional [ int ] = None self . max_steps_per_episode : Optional = 1000 self . num_iterations_between_eval : int = 10 self . num_episodes_per_eval : int = 10 self . learning_rate : float = 0 . 001 self . reward_discount_gamma : float = 1 . 0 self . max_steps_in_buffer : int = 100000 self . training_done : bool self . iterations_done_in_training : int self . episodes_done_in_iteration : int self . episodes_done_in_training : int self . steps_done_in_training : int self . steps_done_in_iteration = 0 self . loss : Dict [ int , float ] self . eval_rewards : Dict [ int , Tuple [ float , float , float ]] self . eval_steps : Dict [ int , Tuple [ float , float , float ]] self . _reset () def __str__ ( self ) : return f ' training_done={self.training_done} ' + \\ f ' #iterations_done_in_training={self.iterations_done_in_training} ' + \\ f ' #episodes_done_in_iteration={self.episodes_done_in_iteration} ' + \\ f ' #steps_done_in_iteration={self.steps_done_in_iteration} ' + \\ f ' #iterations={self.num_iterations} ' + \\ f ' #max_steps_per_episode={self.max_steps_per_episode} ' + \\ f ' #iterations_between_plot={self.num_iterations_between_plot} ' + \\ f ' #iterations_between_eval={self.num_iterations_between_eval} ' + \\ f ' #episodes_per_eval={self.num_episodes_per_eval} ' + \\ f ' #learning_rate={self.learning_rate} ' + \\ f ' #reward_discount_gamma={self.reward_discount_gamma} ' + \\ f ' #max_steps_in_buffer={self.max_steps_in_buffer} ' def _reset ( self ) : \"\"\" Clears all values modified during a train() call. \"\"\" self . training_done = False self . iterations_done_in_training = 0 self . episodes_done_in_iteration = 0 self . episodes_done_in_training = 0 self . steps_done_in_training = 0 self . steps_done_in_iteration = 0 self . loss = dict () self . eval_rewards = dict () self . eval_steps = dict () def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" assert self . num_iterations is None or self . num_iterations > 0 , \" num_iterations not admissible \" assert self . max_steps_per_episode > 0 , \" max_steps_per_episode not admissible \" assert self . num_iterations_between_eval > 0 , \" num_iterations_between_eval not admissible \" assert self . num_episodes_per_eval > 0 , \" num_episodes_per_eval not admissible \" assert 0 < self . learning_rate <= 1 , \" learning_rate not in interval (0,1] \" assert 0 < self . reward_discount_gamma <= 1 , \" reward_discount_gamma not in interval (0,1] \" @ property def num_iterations_between_plot ( self ) : \"\"\" number of iterations between 2 plot updates during training. Returns : number of iterations or 0 if no plot updates should take place . \"\"\" result = 0 if self . num_iterations_between_eval : result = math . ceil ( self . num_iterations_between_eval / 3 ) return result Descendants easyagents.core.EpisodesTrainContext easyagents.core.StepsTrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Core"},{"location":"reference/easyagents/core/#module-easyagentscore","text":"This module contains the core datastructures shared between fronten and backend like the definition of all callbacks and agent configurations. View Source \"\"\"This module contains the core datastructures shared between fronten and backend like the definition of all callbacks and agent configurations. \"\"\" from abc import ABC from typing import Optional , Dict , Tuple , List from enum import Flag , auto import math import easyagents.env import easyagents.backends.monitor import gym.core import matplotlib.pyplot as plt class GymContext ( object ): \"\"\"Contains the context for gym api call Attributes: gym_env: the target gym instance of a pending gym api call \"\"\" def __init__ ( self ): self . _monitor_env : Optional [ easyagents . backends . monitor . _MonitorEnv ] = None self . _totals = None def __str__ ( self ): return f 'MonitorEnv={self._monitor_env} Totals={self._totals}' @property def gym_env ( self ) -> Optional [ gym . core . Env ]: result = None if self . _monitor_env : result = self . _monitor_env . env return result class PlotType ( Flag ): \"\"\"Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. \"\"\" NONE = 0 PLAY_EPISODE = auto () PLAY_STEP = auto () TRAIN_EVAL = auto () TRAIN_ITERATION = auto () class PyPlotContext ( object ): \"\"\"Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure \"\"\" def __init__ ( self ): self . _created_subplots = PlotType . NONE self . figure : Optional [ plt . Figure ] = None self . figsize : ( float , float ) = ( 17 , 6 ) self . _call_jupyter_display = False self . is_jupyter_active = False self . max_columns = 3 def __str__ ( self ): figure_number = None figure_axes_len = 0 if self . figure : figure_number = self . figure . number if self . figure . axes : figure_axes_len = len ( self . figure . axes ) return f 'is_jupyter_active={self.is_jupyter_active} max_columns={self.max_columns} ' + \\ f '_created_subplots={self._created_subplots} figure={figure_number} axes={figure_axes_len} ' def _is_subplot_created ( self , plot_type : PlotType ): \"\"\"Yields true if a subplot of type plot_type was created by a plot callback.\"\"\" result = (( self . _created_subplots & plot_type ) != PlotType . NONE ) return result class ModelConfig ( object ): \"\"\"The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed \"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ... ]] = None ): \"\"\" Args: gym_env_name: the name of the registered gym environment to use, eg 'CartPole-v0' fc_layers: int tuple defining the number and size of each fully connected layer. \"\"\" if fc_layers is None : fc_layers = ( 100 , 100 ) if isinstance ( fc_layers , int ): fc_layers = ( fc_layers ,) assert isinstance ( gym_env_name , str ), \"passed gym_env_name not a string.\" assert gym_env_name != \"\" , \"gym environment name is empty.\" assert easyagents . env . _is_registered_with_gym ( gym_env_name ), \\ f '\"{gym_env_name}\" is not the name of an environment registered with OpenAI gym.' + \\ 'Consider using easyagents.env.register_with_gym to register your environment.' assert fc_layers is not None , \"fc_layers not set\" assert isinstance ( fc_layers , tuple ), \"fc_layers not a tuple\" assert fc_layers , \"fc_layers must contain at least 1 int\" for i in fc_layers : assert isinstance ( i , int ) and i >= 1 , f '{i} is not a valid size for a hidden layer' self . original_env_name = gym_env_name self . gym_env_name = None self . fc_layers = fc_layers self . seed = easyagents . agents . seed def __str__ ( self ): return f 'fc_layers={self.fc_layers} seed={self.seed} gym_env_name={self.gym_env_name}' class TrainContext ( object ): \"\"\"Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done: if true the train loop is terminated at the end of the current iteration iterations_done_in_training: the number of iterations completed so far (during training) episodes_done_in_iteration: the number of episodes completed in the current iteration episodes_done_in_training: the number of episodes completed over all iterations so far. The episodes played for evaluation are not included in this count. steps_done_in_training: the number of steps taken over all iterations so far steps_done_in_iteration: the number of steps taken in the current iteration num_iterations_between_plot: number of training iterations before plots is updated. num_iterations_between_eval: number of training iterations before the current policy is evaluated. num_episodes_per_eval: number of episodes played to estimate the average return and steps eval_rewards: dict containg the rewards statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the sum of rewards over all episodes played for the current evaluation. The dict is indexed by the current_episode. eval_steps: dict containg the steps statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the number of step over all episodes played for the current evaluation. The dict is indexed by the current_episode. loss: dict containing the loss for each iteration training. The dict is indexed by the current_episode. \"\"\" def __init__ ( self ): self . num_iterations : Optional [ int ] = None self . max_steps_per_episode : Optional = 1000 self . num_iterations_between_eval : int = 10 self . num_episodes_per_eval : int = 10 self . learning_rate : float = 0.001 self . reward_discount_gamma : float = 1.0 self . max_steps_in_buffer : int = 100000 self . training_done : bool self . iterations_done_in_training : int self . episodes_done_in_iteration : int self . episodes_done_in_training : int self . steps_done_in_training : int self . steps_done_in_iteration = 0 self . loss : Dict [ int , float ] self . eval_rewards : Dict [ int , Tuple [ float , float , float ]] self . eval_steps : Dict [ int , Tuple [ float , float , float ]] self . _reset () def __str__ ( self ): return f 'training_done={self.training_done} ' + \\ f '#iterations_done_in_training={self.iterations_done_in_training} ' + \\ f '#episodes_done_in_iteration={self.episodes_done_in_iteration} ' + \\ f '#steps_done_in_iteration={self.steps_done_in_iteration} ' + \\ f '#iterations={self.num_iterations} ' + \\ f '#max_steps_per_episode={self.max_steps_per_episode} ' + \\ f '#iterations_between_plot={self.num_iterations_between_plot} ' + \\ f '#iterations_between_eval={self.num_iterations_between_eval} ' + \\ f '#episodes_per_eval={self.num_episodes_per_eval} ' + \\ f '#learning_rate={self.learning_rate} ' + \\ f '#reward_discount_gamma={self.reward_discount_gamma} ' + \\ f '#max_steps_in_buffer={self.max_steps_in_buffer} ' def _reset ( self ): \"\"\"Clears all values modified during a train() call.\"\"\" self . training_done = False self . iterations_done_in_training = 0 self . episodes_done_in_iteration = 0 self . episodes_done_in_training = 0 self . steps_done_in_training = 0 self . steps_done_in_iteration = 0 self . loss = dict () self . eval_rewards = dict () self . eval_steps = dict () def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert self . num_iterations is None or self . num_iterations > 0 , \"num_iterations not admissible\" assert self . max_steps_per_episode > 0 , \"max_steps_per_episode not admissible\" assert self . num_iterations_between_eval > 0 , \"num_iterations_between_eval not admissible\" assert self . num_episodes_per_eval > 0 , \"num_episodes_per_eval not admissible\" assert 0 < self . learning_rate <= 1 , \"learning_rate not in interval (0,1]\" assert 0 < self . reward_discount_gamma <= 1 , \"reward_discount_gamma not in interval (0,1]\" @property def num_iterations_between_plot ( self ): \"\"\"number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. \"\"\" result = 0 if self . num_iterations_between_eval : result = math . ceil ( self . num_iterations_between_eval / 3 ) return result class EpisodesTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy \"\"\" def __init__ ( self ): self . num_episodes_per_iteration : int = 10 self . num_epochs_per_iteration : int = 10 super () . __init__ () def __str__ ( self ): return super () . __str__ () + \\ f '#episodes_per_iteration={self.num_episodes_per_iteration} ' + \\ f '#epochs_per_iteration={self.num_epochs_per_iteration} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert self . num_episodes_per_iteration > 0 , \"num_episodes_per_iteration not admissible\" assert self . num_epochs_per_iteration > 0 , \"num_epochs_per_iteration not admissible\" class CemTrainContext ( EpisodesTrainContext ): \"\"\"Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer \"\"\" def __init__ ( self ): super () . __init__ () self . num_iterations = 100 self . num_episodes_per_iteration : int = 50 self . elite_set_fraction : float = 0.1 self . num_steps_buffer_preload : int = 2000 def __str__ ( self ): return super () . __str__ () + f '#elite_set_fraction={self.elite_set_fraction} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert 1 >= self . elite_set_fraction > 0 , \"elite_set_fraction must be in interval (0,1]\" class PpoTrainContext ( EpisodesTrainContext ): \"\"\"TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. \"\"\" def __init__ ( self ): super () . __init__ () self . actor_loss : Dict [ int , float ] self . critic_loss : Dict [ int , float ] def _reset ( self ): self . actor_loss = dict () self . critic_loss = dict () super () . _reset () class StepsTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training \"\"\" def __init__ ( self ): super () . __init__ () self . num_iterations = 20000 self . num_iterations_between_eval = 1000 self . num_steps_per_iteration : int = 1 self . num_steps_buffer_preload : int = 1000 self . num_steps_sampled_from_buffer : int = 64 self . max_steps_in_buffer = 100000 def __str__ ( self ): return super () . __str__ () + \\ f '#steps_per_iteration={self.num_steps_per_iteration} ' + \\ f '#steps_buffer_preload={self.num_steps_buffer_preload} ' + \\ f '#steps_sampled_from_buffer={self.num_steps_sampled_from_buffer} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert self . num_steps_per_iteration > 0 , \"num_steps_per_iteration not admissible\" class PlayContext ( object ): \"\"\"Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions: dict containing for each episode the actions taken in each step rewards: dict containing for each episode the rewards received in each step sum_of_rewards: dict containing for each episode the sum of rewards over all steps gym_env: the gym environment used to play \"\"\" def __init__ ( self , train_context : Optional [ TrainContext ] = None ): \"\"\" Args: train_context: if set num_episodes, max_steps_per_episode and seed are set from train_context \"\"\" self . num_episodes : Optional [ int ] = None self . max_steps_per_episode : Optional [ int ] = None if train_context is not None : self . num_episodes = train_context . num_episodes_per_eval self . max_steps_per_episode = train_context . max_steps_per_episode self . play_done : bool self . episodes_done : int self . steps_done_in_episode : int self . steps_done : int self . actions : Dict [ int , List [ object ]] self . rewards : Dict [ int , List [ float ]] self . sum_of_rewards : Dict [ int , float ] self . gym_env : Optional [ gym . core . Env ] self . _reset () def __str__ ( self ): return f '#episodes={self.num_episodes} ' + \\ f 'max_steps_per_episode={self.max_steps_per_episode} ' + \\ f 'play_done={self.play_done} ' + \\ f 'episodes_done={self.episodes_done} ' + \\ f 'steps_done_in_episode={self.steps_done_in_episode} ' + \\ f 'steps_done={self.steps_done} ' def _reset ( self ): \"\"\"Clears all values modified during a train() call.\"\"\" self . play_done : bool = False self . episodes_done : int = 0 self . steps_done_in_episode : int = 0 self . steps_done : int = 0 self . actions : Dict [ int , List [ object ]] = dict () self . rewards : Dict [ int , List [ float ]] = dict () self . sum_of_rewards : Dict [ int , float ] = dict () self . gym_env : Optional [ gym . core . Env ] = None def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert ( self . num_episodes is None ) or ( self . num_episodes > 0 ), \"num_episodes not admissible\" assert ( self . max_steps_per_episode is None ) or self . max_steps_per_episode > 0 , \"max_steps_per_episode not admissible\" class AgentContext ( object ): \"\"\"Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing \"\"\" def __init__ ( self , model : ModelConfig ): \"\"\" Args: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. \"\"\" assert isinstance ( model , ModelConfig ), \"model not set\" self . model : ModelConfig = model self . train : Optional [ TrainContext ] = None self . play : Optional [ PlayContext ] = None self . gym : GymContext = GymContext () self . pyplot : PyPlotContext = PyPlotContext () def __str__ ( self ): result = f 'agent_context:' result += f ' \\n api =[{self.gym}]' if self . train is not None : result += f ' \\n train =[{self.train}] ' if self . play is not None : result += f ' \\n play =[{self.play}] ' if self . pyplot is not None : result += f ' \\n pyplot=[{self.pyplot}] ' result += f ' \\n model =[{self.model}] ' return result @property def is_eval ( self ) -> bool : \"\"\"Yields true if a policy evaluation inside an agent.train(...) call is in progress.\"\"\" return ( self . play is not None ) and ( self . train is not None ) @property def is_play ( self ) -> bool : \"\"\"Yields true if an agent.play(...) call is in progress, but not a policy evaluation\"\"\" return ( self . play is not None ) and ( self . train is None ) def _is_plot_ready ( self , plot_type : PlotType ) -> bool : \"\"\"Yields true if any of the plots in plot_type is ready to be plotted. A plot_type is ready if a plot callback was registered for this type (like TRAIN_EVAL), the agent is in runtime state corresponding to the plot type (like in training and at the end of an evaluation period) and any frequency condition is met (like num_episodes_between_plot) \"\"\" result = False if ( plot_type & PlotType . PLAY_EPISODE ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_EPISODE )) if ( plot_type & PlotType . PLAY_STEP ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_STEP )) if ( plot_type & PlotType . TRAIN_EVAL ) != PlotType . NONE : train_result = self . is_eval train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_EVAL ) train_result = train_result and ( self . play . episodes_done == self . train . num_episodes_per_eval ) result = result | train_result if ( plot_type & PlotType . TRAIN_ITERATION ) != PlotType . NONE : train_result = self . is_train train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_ITERATION ) train_result = train_result and \\ self . train . num_iterations_between_plot > 0 and \\ (( self . train . iterations_done_in_training % self . train . num_iterations_between_plot ) == 0 ) result = result | train_result return result @property def is_train ( self ) -> bool : \"\"\"Yields true if an agent.tain(...) call is in progress, but not a policy evaluation.\"\"\" return ( self . train is not None ) and ( self . play is None ) class AgentCallback ( ABC ): \"\"\"Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment\"\"\" def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" class _PostProcessCallback ( AgentCallback ): pass class _PreProcessCallback ( AgentCallback ): pass","title":"Module easyagents.core"},{"location":"reference/easyagents/core/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/core/#agentcallback","text":"class AgentCallback ( / , * args , ** kwargs ) Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment View Source class AgentCallback ( ABC ) : \"\"\" Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment \"\"\" def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ) : \"\"\" Logs a call to the api of the agents implementation library / framework. \"\"\" pass def on_log ( self , agent_context : AgentContext , log_msg : str ) : \"\"\" Logs a general message \"\"\" pass def on_gym_init_begin ( self , agent_context : AgentContext ) : \"\"\" called when the monitored environment begins the instantiation of a new gym environment. Args : agent_context : api_context passed to calling agent \"\"\" def on_gym_init_end ( self , agent_context : AgentContext ) : \"\"\" called when the monitored environment completed the instantiation of a new gym environment. Args : agent_context : api_context passed to calling agent \"\"\" pass def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ) : \"\"\" Before a call to gym.reset Args : agent_context : api_context passed to calling agent kwargs : the args to be passed to the underlying environment \"\"\" def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ) : \"\"\" After a call to gym.reset was completed Args : agent_context : api_context passed to calling agent reset_result : object returned by gym . reset kwargs : args passed to gym . reset \"\"\" pass def on_gym_step_begin ( self , agent_context : AgentContext , action ) : \"\"\" Before a call to gym.step Args : agent_context : api_context passed to calling agent action : the action to be passed to the underlying environment \"\"\" pass def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ) : \"\"\" After a call to gym.step was completed Args : agent_context : api_context passed to calling agent action : the action to be passed to the underlying environment step_result : ( observation , reward , done , info ) tuple returned by gym . step \"\"\" pass def on_play_episode_begin ( self , agent_context : AgentContext ) : \"\"\" Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" def on_play_episode_end ( self , agent_context : AgentContext ) : \"\"\" Called once after an episode is done or stopped (during play or eval, but not during train). \"\"\" def on_play_begin ( self , agent_context : AgentContext ) : \"\"\" Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" def on_play_end ( self , agent_context : AgentContext ) : \"\"\" Called once before exiting an agent.play() call (during play or eval, but not during train) \"\"\" def on_play_step_begin ( self , agent_context : AgentContext , action ) : \"\"\" Called once before a new step is taken in the current episode (during play or eval, but not during train). Args : agent_context : the context describing the agents current configuration action : the action to be passed to the upcoming gym_env . step call \"\"\" def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ) : \"\"\" Called once after a step is completed in the current episode (during play or eval, but not during train). \"\"\" def on_train_begin ( self , agent_context : AgentContext ) : \"\"\" Called once at the entry of an agent.train() call. \"\"\" def on_train_end ( self , agent_context : AgentContext ) : \"\"\" Called once before exiting an agent.train() call \"\"\" def on_train_iteration_begin ( self , agent_context : AgentContext ) : \"\"\" Called once at the start of a new iteration. \"\"\" def on_train_iteration_end ( self , agent_context : AgentContext ) : \"\"\" Called once after the current iteration is completed \"\"\"","title":"AgentCallback"},{"location":"reference/easyagents/core/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#descendants","text":"easyagents.core._PostProcessCallback easyagents.core._PreProcessCallback easyagents.callbacks.plot._PlotCallback easyagents.callbacks.plot.Clear easyagents.backends.core._BackendEvalCallback easyagents.callbacks.duration.Fast easyagents.callbacks.log._LogCallbackBase easyagents.callbacks.log._CallbackCounts","title":"Descendants"},{"location":"reference/easyagents/core/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/core/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/core/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/core/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/core/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/core/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/core/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/core/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/core/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/core/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/core/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/core/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/core/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/core/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/core/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/core/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/core/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/core/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/core/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/core/#agentcontext","text":"class AgentContext ( model : easyagents . core . ModelConfig ) Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing View Source class AgentContext ( object ) : \"\"\" Collection of state and configuration settings for a EasyAgent instance. Attributes : model : model configuration including the name of the underlying gym_environment and the policy ' s neural network archtitecture. train : training configuration and current train state . None if not inside a train call . play : play / eval configuration and current state . None if not inside a play call ( directly or due to a evaluation inside a train loop ) gym : context for gym environment related calls . pyplot : the context containing the matplotlib . pyplot figure to plot to during training or playing \"\"\" def __init__ ( self , model : ModelConfig ) : \"\"\" Args : model : model configuration including the name of the underlying gym_environment and the policy ' s neural network archtitecture. \"\"\" assert isinstance ( model , ModelConfig ) , \" model not set \" self . model : ModelConfig = model self . train : Optional [ TrainContext ] = None self . play : Optional [ PlayContext ] = None self . gym : GymContext = GymContext () self . pyplot : PyPlotContext = PyPlotContext () def __str__ ( self ) : result = f ' agent_context: ' result += f ' \\n api =[{self.gym}] ' if self . train is not None : result += f ' \\n train =[{self.train}] ' if self . play is not None : result += f ' \\n play =[{self.play}] ' if self . pyplot is not None : result += f ' \\n pyplot=[{self.pyplot}] ' result += f ' \\n model =[{self.model}] ' return result @ property def is_eval ( self ) -> bool : \"\"\" Yields true if a policy evaluation inside an agent.train(...) call is in progress. \"\"\" return ( self . play is not None ) and ( self . train is not None ) @ property def is_play ( self ) -> bool : \"\"\" Yields true if an agent.play(...) call is in progress, but not a policy evaluation \"\"\" return ( self . play is not None ) and ( self . train is None ) def _is_plot_ready ( self , plot_type : PlotType ) -> bool : \"\"\" Yields true if any of the plots in plot_type is ready to be plotted. A plot_type is ready if a plot callback was registered for this type ( like TRAIN_EVAL ) , the agent is in runtime state corresponding to the plot type ( like in training and at the end of an evaluation period ) and any frequency condition is met ( like num_episodes_between_plot ) \"\"\" result = False if ( plot_type & PlotType . PLAY_EPISODE ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_EPISODE )) if ( plot_type & PlotType . PLAY_STEP ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_STEP )) if ( plot_type & PlotType . TRAIN_EVAL ) != PlotType . NONE : train_result = self . is_eval train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_EVAL ) train_result = train_result and ( self . play . episodes_done == self . train . num_episodes_per_eval ) result = result | train_result if ( plot_type & PlotType . TRAIN_ITERATION ) != PlotType . NONE : train_result = self . is_train train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_ITERATION ) train_result = train_result and \\ self . train . num_iterations_between_plot > 0 and \\ (( self . train . iterations_done_in_training % self . train . num_iterations_between_plot ) == 0 ) result = result | train_result return result @ property def is_train ( self ) -> bool : \"\"\" Yields true if an agent.tain(...) call is in progress, but not a policy evaluation. \"\"\" return ( self . train is not None ) and ( self . play is None )","title":"AgentContext"},{"location":"reference/easyagents/core/#instance-variables","text":"is_eval Yields true if a policy evaluation inside an agent.train(...) call is in progress. is_play Yields true if an agent.play(...) call is in progress, but not a policy evaluation is_train Yields true if an agent.tain(...) call is in progress, but not a policy evaluation.","title":"Instance variables"},{"location":"reference/easyagents/core/#cemtraincontext","text":"class CemTrainContext ( ) Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer View Source class CemTrainContext ( EpisodesTrainContext ) : \"\"\" Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes : elite_set_fraction : fraction of the elite policy set . num_steps_buffer_preload : number of steps performed to initially load the policy buffer \"\"\" def __init__ ( self ) : super () . __init__ () self . num_iterations = 100 self . num_episodes_per_iteration : int = 50 self . elite_set_fraction : float = 0 . 1 self . num_steps_buffer_preload : int = 2000 def __str__ ( self ) : return super () . __str__ () + f ' #elite_set_fraction={self.elite_set_fraction} ' def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" super () . _validate () assert 1 >= self . elite_set_fraction > 0 , \" elite_set_fraction must be in interval (0,1] \"","title":"CemTrainContext"},{"location":"reference/easyagents/core/#ancestors-in-mro_1","text":"easyagents.core.EpisodesTrainContext easyagents.core.TrainContext","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#instance-variables_1","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/core/#episodestraincontext","text":"class EpisodesTrainContext ( ) Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy View Source class EpisodesTrainContext ( TrainContext ) : \"\"\" Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows : for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0 : evaluate policy if training_done break Attributes : num_episodes_per_iteration : number of episodes played per training iteration num_epochs_per_iteration : number of times the data collected for the current iteration is used to retrain the current policy \"\"\" def __init__ ( self ) : self . num_episodes_per_iteration : int = 10 self . num_epochs_per_iteration : int = 10 super () . __init__ () def __str__ ( self ) : return super () . __str__ () + \\ f ' #episodes_per_iteration={self.num_episodes_per_iteration} ' + \\ f ' #epochs_per_iteration={self.num_epochs_per_iteration} ' def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" super () . _validate () assert self . num_episodes_per_iteration > 0 , \" num_episodes_per_iteration not admissible \" assert self . num_epochs_per_iteration > 0 , \" num_epochs_per_iteration not admissible \"","title":"EpisodesTrainContext"},{"location":"reference/easyagents/core/#ancestors-in-mro_2","text":"easyagents.core.TrainContext","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#descendants_1","text":"easyagents.core.CemTrainContext easyagents.core.PpoTrainContext","title":"Descendants"},{"location":"reference/easyagents/core/#instance-variables_2","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/core/#gymcontext","text":"class GymContext ( ) Contains the context for gym api call Attributes: gym_env: the target gym instance of a pending gym api call View Source class GymContext ( object ) : \"\"\" Contains the context for gym api call Attributes : gym_env : the target gym instance of a pending gym api call \"\"\" def __init__ ( self ) : self . _monitor_env : Optional [ easyagents . backends . monitor . _MonitorEnv ] = None self . _totals = None def __str__ ( self ) : return f ' MonitorEnv={self._monitor_env} Totals={self._totals} ' @ property def gym_env ( self ) -> Optional [ gym . core . Env ]: result = None if self . _monitor_env : result = self . _monitor_env . env return result","title":"GymContext"},{"location":"reference/easyagents/core/#instance-variables_3","text":"gym_env","title":"Instance variables"},{"location":"reference/easyagents/core/#modelconfig","text":"class ModelConfig ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None ) The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed View Source class ModelConfig ( object ) : \"\"\" The model configurations, containing the name of the gym environment and the neural network architecture. Attributes : original_env_name : the name of the underlying gym environment , eg ' CartPole-v0 ' gym_env_name : the name of the actual gym environment used ( a wrapper around the environment given by original_env_name ) fc_layers : int tuple defining the number and size of each fully connected layer . seed : the seed to be used for example for the gym_env or None for no seed \"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ...]] = None ) : \"\"\" Args : gym_env_name : the name of the registered gym environment to use , eg ' CartPole-v0 ' fc_layers : int tuple defining the number and size of each fully connected layer . \"\"\" if fc_layers is None : fc_layers = ( 100 , 100 ) if isinstance ( fc_layers , int ) : fc_layers = ( fc_layers , ) assert isinstance ( gym_env_name , str ) , \" passed gym_env_name not a string. \" assert gym_env_name != \"\" , \" gym environment name is empty. \" assert easyagents . env . _is_registered_with_gym ( gym_env_name ) , \\ f ' \"{gym_env_name}\" is not the name of an environment registered with OpenAI gym. ' + \\ ' Consider using easyagents.env.register_with_gym to register your environment. ' assert fc_layers is not None , \" fc_layers not set \" assert isinstance ( fc_layers , tuple ) , \" fc_layers not a tuple \" assert fc_layers , \" fc_layers must contain at least 1 int \" for i in fc_layers : assert isinstance ( i , int ) and i >= 1 , f ' {i} is not a valid size for a hidden layer ' self . original_env_name = gym_env_name self . gym_env_name = None self . fc_layers = fc_layers self . seed = easyagents . agents . seed def __str__ ( self ) : return f ' fc_layers={self.fc_layers} seed={self.seed} gym_env_name={self.gym_env_name} '","title":"ModelConfig"},{"location":"reference/easyagents/core/#playcontext","text":"class PlayContext ( train_context : Union [ easyagents . core . TrainContext , NoneType ] = None ) Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions : dict containing for each episode the actions taken in each step rewards : dict containing for each episode the rewards received in each step sum_of_rewards : dict containing for each episode the sum of rewards over all steps gym_env : the gym environment used to play View Source class PlayContext ( object ) : \"\"\" Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode . The EasyAgent . play () method proceeds ( roughly ) as follow : for e in num_episodes play ( while steps_done_in_episode < max_steps_per_episode ) if playing_done break Attributes : num_episodes : number of episodes to play , unlimited if None max_steps_per_episode : maximum number of steps per episode , unlimited if None play_done : if true the play loop is terminated at the end of the current episode episodes_done : the number of episodes played ( including the current episode ) . steps_done_in_episode : the number of steps taken in the current episode . steps_done : the number of steps played ( over all episodes so far ) actions : dict containing for each episode the actions taken in each step rewards : dict containing for each episode the rewards received in each step sum_of_rewards : dict containing for each episode the sum of rewards over all steps gym_env : the gym environment used to play \"\"\" def __init__ ( self , train_context : Optional [ TrainContext ] = None ) : \"\"\" Args : train_context : if set num_episodes , max_steps_per_episode and seed are set from train_context \"\"\" self . num_episodes : Optional [ int ] = None self . max_steps_per_episode : Optional [ int ] = None if train_context is not None : self . num_episodes = train_context . num_episodes_per_eval self . max_steps_per_episode = train_context . max_steps_per_episode self . play_done : bool self . episodes_done : int self . steps_done_in_episode : int self . steps_done : int self . actions : Dict [ int , List [ object ]] self . rewards : Dict [ int , List [ float ]] self . sum_of_rewards : Dict [ int , float ] self . gym_env : Optional [ gym . core . Env ] self . _reset () def __str__ ( self ) : return f ' #episodes={self.num_episodes} ' + \\ f ' max_steps_per_episode={self.max_steps_per_episode} ' + \\ f ' play_done={self.play_done} ' + \\ f ' episodes_done={self.episodes_done} ' + \\ f ' steps_done_in_episode={self.steps_done_in_episode} ' + \\ f ' steps_done={self.steps_done} ' def _reset ( self ) : \"\"\" Clears all values modified during a train() call. \"\"\" self . play_done : bool = False self . episodes_done : int = 0 self . steps_done_in_episode : int = 0 self . steps_done : int = 0 self . actions : Dict [ int , List [ object ]] = dict () self . rewards : Dict [ int , List [ float ]] = dict () self . sum_of_rewards : Dict [ int , float ] = dict () self . gym_env : Optional [ gym . core . Env ] = None def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" assert ( self . num_episodes is None ) or ( self . num_episodes > 0 ) , \" num_episodes not admissible \" assert ( self . max_steps_per_episode is None ) or self . max_steps_per_episode > 0 , \" max_steps_per_episode not admissible \"","title":"PlayContext"},{"location":"reference/easyagents/core/#plottype","text":"class PlotType ( / , * args , ** kwargs ) Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. View Source class PlotType ( Flag ): \"\"\"Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. \"\"\" NONE = 0 PLAY_EPISODE = auto () PLAY_STEP = auto () TRAIN_EVAL = auto () TRAIN_ITERATION = auto ()","title":"PlotType"},{"location":"reference/easyagents/core/#ancestors-in-mro_3","text":"enum.Flag enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#class-variables","text":"NONE PLAY_EPISODE PLAY_STEP TRAIN_EVAL TRAIN_ITERATION","title":"Class variables"},{"location":"reference/easyagents/core/#ppotraincontext","text":"class PpoTrainContext ( ) TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. View Source class PpoTrainContext ( EpisodesTrainContext ) : \"\"\" TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes : actor_loss : loss observed during training of the actor network . dict is indexed by the current_episode . critic_loss : loss observed during training of the critic network . dict is indexed by the current_episode . \"\"\" def __init__ ( self ) : super () . __init__ () self . actor_loss : Dict [ int , float ] self . critic_loss : Dict [ int , float ] def _reset ( self ) : self . actor_loss = dict () self . critic_loss = dict () super () . _reset ()","title":"PpoTrainContext"},{"location":"reference/easyagents/core/#ancestors-in-mro_4","text":"easyagents.core.EpisodesTrainContext easyagents.core.TrainContext","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#instance-variables_4","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/core/#pyplotcontext","text":"class PyPlotContext ( ) Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure View Source class PyPlotContext ( object ) : \"\"\" Contain the context for the maplotlib.pyplot figure plotting. Attributes figure : the figure to plot to figsize : figure ( width , height ) in inches for the figure to be created . is_jupyter_active : True if we plot to jupyter notebook cell , False otherwise . max_columns : the max number of subplot columns in the pyplot figure \"\"\" def __init__ ( self ) : self . _created_subplots = PlotType . NONE self . figure : Optional [ plt . Figure ] = None self . figsize : ( float , float ) = ( 17 , 6 ) self . _call_jupyter_display = False self . is_jupyter_active = False self . max_columns = 3 def __str__ ( self ) : figure_number = None figure_axes_len = 0 if self . figure : figure_number = self . figure . number if self . figure . axes : figure_axes_len = len ( self . figure . axes ) return f ' is_jupyter_active={self.is_jupyter_active} max_columns={self.max_columns} ' + \\ f ' _created_subplots={self._created_subplots} figure={figure_number} axes={figure_axes_len} ' def _is_subplot_created ( self , plot_type : PlotType ) : \"\"\" Yields true if a subplot of type plot_type was created by a plot callback. \"\"\" result = (( self . _created_subplots & plot_type ) != PlotType . NONE ) return result","title":"PyPlotContext"},{"location":"reference/easyagents/core/#stepstraincontext","text":"class StepsTrainContext ( ) Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training View Source class StepsTrainContext ( TrainContext ) : \"\"\" Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows : for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0 : evaluate policy if training_done break Attributes : num_steps_per_iteration : number of steps played for each iteration num_steps_buffer_preload : number of initial collect steps to preload the buffer num_steps_sampled_from_buffer : the number of steps sampled from buffer for each iteration training \"\"\" def __init__ ( self ) : super () . __init__ () self . num_iterations = 20000 self . num_iterations_between_eval = 1000 self . num_steps_per_iteration : int = 1 self . num_steps_buffer_preload : int = 1000 self . num_steps_sampled_from_buffer : int = 64 self . max_steps_in_buffer = 100000 def __str__ ( self ) : return super () . __str__ () + \\ f ' #steps_per_iteration={self.num_steps_per_iteration} ' + \\ f ' #steps_buffer_preload={self.num_steps_buffer_preload} ' + \\ f ' #steps_sampled_from_buffer={self.num_steps_sampled_from_buffer} ' def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" super () . _validate () assert self . num_steps_per_iteration > 0 , \" num_steps_per_iteration not admissible \"","title":"StepsTrainContext"},{"location":"reference/easyagents/core/#ancestors-in-mro_5","text":"easyagents.core.TrainContext","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#instance-variables_5","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/core/#traincontext","text":"class TrainContext ( ) Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done : if true the train loop is terminated at the end of the current iteration iterations_done_in_training : the number of iterations completed so far ( during training ) episodes_done_in_iteration : the number of episodes completed in the current iteration episodes_done_in_training : the number of episodes completed over all iterations so far . The episodes played for evaluation are not included in this count . steps_done_in_training : the number of steps taken over all iterations so far steps_done_in_iteration : the number of steps taken in the current iteration num_iterations_between_plot : number of training iterations before plots is updated . num_iterations_between_eval : number of training iterations before the current policy is evaluated . num_episodes_per_eval : number of episodes played to estimate the average return and steps eval_rewards : dict containg the rewards statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the sum of rewards over all episodes played for the current evaluation . The dict is indexed by the current_episode . eval_steps : dict containg the steps statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the number of step over all episodes played for the current evaluation . The dict is indexed by the current_episode . loss : dict containing the loss for each iteration training . The dict is indexed by the current_episode . View Source class TrainContext ( object ) : \"\"\" Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations . Hints : o TrainContext contains all the parameters needed to control the train loop . o Subclasses of TrainContext may contain additional Agent ( but not backend ) specific parameters . Attributes : num_iterations : number of times the training is repeated ( with additional data ) , unlimited if None max_steps_per_episode : maximum number of steps per episode learning_rate : the learning rate used in the next iteration ' s policy training (0,1] reward_discount_gamma : the factor by which a reward is discounted for each step ( 0 , 1 ] max_steps_in_buffer : size of the agents buffer in steps training_done : if true the train loop is terminated at the end of the current iteration iterations_done_in_training : the number of iterations completed so far ( during training ) episodes_done_in_iteration : the number of episodes completed in the current iteration episodes_done_in_training : the number of episodes completed over all iterations so far . The episodes played for evaluation are not included in this count . steps_done_in_training : the number of steps taken over all iterations so far steps_done_in_iteration : the number of steps taken in the current iteration num_iterations_between_plot : number of training iterations before plots is updated . num_iterations_between_eval : number of training iterations before the current policy is evaluated . num_episodes_per_eval : number of episodes played to estimate the average return and steps eval_rewards : dict containg the rewards statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the sum of rewards over all episodes played for the current evaluation . The dict is indexed by the current_episode . eval_steps : dict containg the steps statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the number of step over all episodes played for the current evaluation . The dict is indexed by the current_episode . loss : dict containing the loss for each iteration training . The dict is indexed by the current_episode . \"\"\" def __init__ ( self ) : self . num_iterations : Optional [ int ] = None self . max_steps_per_episode : Optional = 1000 self . num_iterations_between_eval : int = 10 self . num_episodes_per_eval : int = 10 self . learning_rate : float = 0 . 001 self . reward_discount_gamma : float = 1 . 0 self . max_steps_in_buffer : int = 100000 self . training_done : bool self . iterations_done_in_training : int self . episodes_done_in_iteration : int self . episodes_done_in_training : int self . steps_done_in_training : int self . steps_done_in_iteration = 0 self . loss : Dict [ int , float ] self . eval_rewards : Dict [ int , Tuple [ float , float , float ]] self . eval_steps : Dict [ int , Tuple [ float , float , float ]] self . _reset () def __str__ ( self ) : return f ' training_done={self.training_done} ' + \\ f ' #iterations_done_in_training={self.iterations_done_in_training} ' + \\ f ' #episodes_done_in_iteration={self.episodes_done_in_iteration} ' + \\ f ' #steps_done_in_iteration={self.steps_done_in_iteration} ' + \\ f ' #iterations={self.num_iterations} ' + \\ f ' #max_steps_per_episode={self.max_steps_per_episode} ' + \\ f ' #iterations_between_plot={self.num_iterations_between_plot} ' + \\ f ' #iterations_between_eval={self.num_iterations_between_eval} ' + \\ f ' #episodes_per_eval={self.num_episodes_per_eval} ' + \\ f ' #learning_rate={self.learning_rate} ' + \\ f ' #reward_discount_gamma={self.reward_discount_gamma} ' + \\ f ' #max_steps_in_buffer={self.max_steps_in_buffer} ' def _reset ( self ) : \"\"\" Clears all values modified during a train() call. \"\"\" self . training_done = False self . iterations_done_in_training = 0 self . episodes_done_in_iteration = 0 self . episodes_done_in_training = 0 self . steps_done_in_training = 0 self . steps_done_in_iteration = 0 self . loss = dict () self . eval_rewards = dict () self . eval_steps = dict () def _validate ( self ) : \"\"\" Validates the consistency of all values, raising an exception if an inadmissible combination is detected. \"\"\" assert self . num_iterations is None or self . num_iterations > 0 , \" num_iterations not admissible \" assert self . max_steps_per_episode > 0 , \" max_steps_per_episode not admissible \" assert self . num_iterations_between_eval > 0 , \" num_iterations_between_eval not admissible \" assert self . num_episodes_per_eval > 0 , \" num_episodes_per_eval not admissible \" assert 0 < self . learning_rate <= 1 , \" learning_rate not in interval (0,1] \" assert 0 < self . reward_discount_gamma <= 1 , \" reward_discount_gamma not in interval (0,1] \" @ property def num_iterations_between_plot ( self ) : \"\"\" number of iterations between 2 plot updates during training. Returns : number of iterations or 0 if no plot updates should take place . \"\"\" result = 0 if self . num_iterations_between_eval : result = math . ceil ( self . num_iterations_between_eval / 3 ) return result","title":"TrainContext"},{"location":"reference/easyagents/core/#descendants_2","text":"easyagents.core.EpisodesTrainContext easyagents.core.StepsTrainContext","title":"Descendants"},{"location":"reference/easyagents/core/#instance-variables_6","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/backends/core/","text":"Module easyagents.backends.core This module contains backend core classes like Backend and BackendAgent. The concrete backends like tfagent or baselines are implemented in seprate modules. View Source \"\"\"This module contains backend core classes like Backend and BackendAgent. The concrete backends like tfagent or baselines are implemented in seprate modules. \"\"\" from abc import ABC , ABCMeta , abstractmethod from typing import List , Optional , Tuple , Type , Dict import gym import tensorflow import numpy import random from easyagents import core from easyagents.backends import monitor from easyagents.callbacks import plot _tensorflow_v2_eager_enabled : Optional [ bool ] = None class _BackendEvalCallback ( core . AgentCallback ): \"\"\"Evaluates an agents current policy and updates its train_context accordingly.\"\"\" def __init__ ( self , train_context : core . TrainContext ): assert train_context , \"train_context not set\" assert train_context . num_episodes_per_eval > 0 , \"num_episodes_per_eval is 0.\" self . _train_contex = train_context def on_play_episode_end ( self , agent_context : core . AgentContext ): pc = agent_context . play tc = self . _train_contex sum_of_r = pc . sum_of_rewards . values () tc . eval_rewards [ tc . episodes_done_in_training ] = ( min ( sum_of_r ), sum ( sum_of_r ) / len ( sum_of_r ), max ( sum_of_r )) steps = [ len ( episode_rewards ) for episode_rewards in pc . rewards . values ()] tc . eval_steps [ tc . episodes_done_in_training ] = ( min ( steps ), sum ( steps ) / len ( steps ), max ( steps )) class _BackendAgent ( ABC ): \"\"\"Base class for all backend agent implementations. Implements the train loop and calls the Callbacks. \"\"\" def __init__ ( self , model_config : core . ModelConfig , backend_name : str , tensorflow_v2_eager : bool = True ): \"\"\" Args: model_config: defines the model and environment to be used backend_name: id of the backend to which this agent belongs to. tensorflow_v2_eager: the execution mode, enforced for this and all other backend agents. \"\"\" assert model_config is not None , \"model_config not set.\" assert backend_name self . _backend_name : str = backend_name self . _tensorflow_v2_eager = tensorflow_v2_eager self . model_config = model_config self . _agent_context : core . AgentContext = core . AgentContext ( self . model_config ) self . _agent_context . gym . _totals = monitor . _register_gym_monitor ( self . model_config . original_env_name ) self . model_config . gym_env_name = self . _agent_context . gym . _totals . gym_env_name self . _preprocess_callbacks : List [ core . _PreProcessCallback ] = [ plot . _PreProcess ()] self . _callbacks : List [ core . AgentCallback ] = [] self . _postprocess_callbacks : List [ core . _PostProcessCallback ] = [ plot . _PostProcess ()] self . _train_total_episodes_on_iteration_begin : int = 0 self . _initialize_tensorflow () def _initialize_tensorflow ( self ): \"\"\" v2 behavior and eager execution mode. if a previous backend selected a different mode an exceptionis raised.\"\"\" global _tensorflow_v2_eager_enabled if _tensorflow_v2_eager_enabled is None : _tensorflow_v2_eager_enabled = self . _tensorflow_v2_eager if _tensorflow_v2_eager_enabled : self . log_api ( 'tf.compat.v1.enable_v2_behavior' ) tensorflow . compat . v1 . enable_v2_behavior () self . log_api ( 'tf.compat.v1.enable_eager_execution' ) tensorflow . compat . v1 . enable_eager_execution () assert _tensorflow_v2_eager_enabled == self . _tensorflow_v2_eager , \\ \"v2 behavior and eager execution mode already selected by another backend does not match \" + \\ \"the requirements of this backend. \" + \\ \"To avoid the conflict, do not combine both backend types in the same python / jupyter kernel instance. \" return def _set_seed ( self ): \"\"\" sets the random seeds for all dependent packages \"\"\" if not self . model_config . seed is None : seed = self . model_config . seed self . log_api ( f 'tf.compat.v1.set_random_seed' , f '({seed})' ) tensorflow . compat . v1 . set_random_seed ( seed ) self . log_api ( f 'tf.random.set_random_seed' , f '(seed={seed})' ) tensorflow . random . set_random_seed ( seed = seed ) self . log_api ( f 'numpy.random.seed' , f '({seed})' ) numpy . random . seed ( seed ) self . log_api ( f 'random.seed' , f '({seed})' ) random . seed ( seed ) return def _eval_current_policy ( self ): \"\"\"Evaluates the current policy using play and updates the train_context If num_episodes_per_eval or num_iterations_per_eval is 0 no evaluation is performed. \"\"\" tc = self . _agent_context . train assert tc , \"train_context not set\" if tc . num_episodes_per_eval and tc . num_iterations_between_eval : callbacks = [ _BackendEvalCallback ( self . _agent_context . train )] + self . _callbacks self . play ( play_context = core . PlayContext ( self . _agent_context . train ), callbacks = callbacks ) def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ): \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) def _on_gym_init_begin ( self ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Hint: the total instances count is not incremented yet.\"\"\" self . _agent_context . gym . _monitor_env = None for c in self . _callbacks : c . on_gym_init_begin ( self . _agent_context ) self . _agent_context . gym . _monitor_env = None def _on_gym_init_end ( self , env : monitor . _MonitorEnv ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Hint: o the total instances count is incremented by now o the new env (and its action space) is seeded with the api_context's seed \"\"\" self . _agent_context . gym . _monitor_env = env if self . _agent_context . model . seed is not None : env = self . _agent_context . gym . gym_env seed = self . _agent_context . model . seed env . seed ( seed ) for c in self . _callbacks : c . on_gym_init_end ( self . _agent_context ) self . _agent_context . gym . _monitor_env = None def _on_gym_reset_begin ( self , env : monitor . _MonitorEnv , ** kwargs ): \"\"\"called when the monitored environment begins a reset. Hint: the total reset count is not incremented yet.\"\"\" self . _agent_context . gym . _monitor_env = env for c in self . _callbacks : c . on_gym_reset_begin ( self . _agent_context , ** kwargs ) self . _agent_context . gym . _monitor_env = None def _on_gym_reset_end ( self , env : monitor . _MonitorEnv , reset_result : Tuple , ** kwargs ): \"\"\"called when the monitored environment completed a reset. Hint: the total episode count is incremented by now (if a step was performed before the last reset).\"\"\" self . _agent_context . gym . _monitor_env = env for c in self . _callbacks : c . on_gym_reset_end ( self . _agent_context , reset_result , ** kwargs ) self . _agent_context . gym . _monitor_env = None def _on_gym_step_begin ( self , env : monitor . _MonitorEnv , action ): \"\"\"called when the monitored environment begins a step. Hint: o sets env.max_steps_per_episode if we are in train / play. Thus the episode is ended by the MonitorEnv if the step limit is exceeded \"\"\" ac = self . _agent_context ac . gym . _monitor_env = env env . max_steps_per_episode = None if ac . is_play or ac . is_eval : env . max_steps_per_episode = ac . play . max_steps_per_episode self . _on_play_step_begin ( action ) if ac . is_train : env . max_steps_per_episode = ac . train . max_steps_per_episode self . _on_train_step_begin ( action ) for c in self . _callbacks : c . on_gym_step_begin ( self . _agent_context , action ) self . _agent_context . gym . _monitor_env = None def _on_gym_step_end ( self , env : monitor . _MonitorEnv , action , step_result : Tuple ): \"\"\"called when the monitored environment completed a step. Args: env: the gym_env the last step was done on step_result: the result (state, reward, done, info) of the last step call \"\"\" ac = self . _agent_context ac . gym . _monitor_env = env if ac . is_play or ac . is_eval : self . _on_play_step_end ( action , step_result ) if ac . is_train : self . _on_train_step_end ( action , step_result ) for c in self . _callbacks : c . on_gym_step_end ( self . _agent_context , action , step_result ) self . _agent_context . gym . _monitor_env = None env . max_steps_per_episode = None def _on_play_begin ( self ): \"\"\"Must NOT be called by play_implementation\"\"\" for c in self . _callbacks : c . on_play_begin ( self . _agent_context ) def _on_play_end ( self ): \"\"\"Must NOT be called by play_implementation\"\"\" for c in self . _callbacks : c . on_play_end ( self . _agent_context ) self . _agent_context . play . gym_env = None def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) def _on_play_step_begin ( self , action ): \"\"\"Called before each call to gym.step on the current play env (agent_context.play.gym_env) Args: action: the action to be passed to the upcoming gym_env.step call \"\"\" for c in self . _callbacks : c . on_play_step_begin ( self . _agent_context , action ) def _on_play_step_end ( self , action , step_result : Tuple ): \"\"\"Called after each call to gym.step on the current play env (agent_context.play.gym_env) Args: step_result: the result (state, reward, done, info) of the last step call \"\"\" ( state , reward , done , info ) = step_result pc = self . _agent_context . play pc . steps_done_in_episode += 1 pc . steps_done += 1 pc . actions [ pc . episodes_done + 1 ] . append ( action ) pc . rewards [ pc . episodes_done + 1 ] . append ( reward ) pc . sum_of_rewards [ pc . episodes_done + 1 ] += reward for c in self . _callbacks : c . on_play_step_end ( self . _agent_context , action , step_result ) def _on_train_begin ( self ): \"\"\"Must NOT be called by train_implementation\"\"\" for c in self . _callbacks : c . on_train_begin ( self . _agent_context ) def _on_train_end ( self ): \"\"\"Must NOT be called by train_implementation\"\"\" tc = self . _agent_context . train if tc . episodes_done_in_training not in tc . eval_rewards : self . _eval_current_policy () for c in self . _callbacks : c . on_train_end ( self . _agent_context ) def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) def on_train_iteration_end ( self , loss : float , ** kwargs ): \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs: for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ): prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ): self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) def _on_train_step_begin ( self , action ): \"\"\"Called before each call to gym.step on the current train env (agent_context.train.gym_env) Args: action: the action to be passed to the upcoming gym_env.step call \"\"\" pass # noinspection PyUnusedLocal def _on_train_step_end ( self , action : object , step_result : Tuple ): \"\"\"Called after each call to gym.step on the current train env (agent_context.train.gym_env) Args: step_result: the result (state, reward, done, info) of the last step call \"\"\" tc = self . _agent_context . train tc . steps_done_in_iteration += 1 tc . steps_done_in_training += 1 def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None @abstractmethod def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episode with the current policy. For implementation details see BackendBaseAgent. \"\"\" def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None @abstractmethod def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Agent specific implementation of the train loop. For implementational details see BackendBaseAgent. \"\"\" class BackendAgent ( _BackendAgent , metaclass = ABCMeta ): \"\"\"Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. \"\"\" @abstractmethod def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used \"\"\" @abstractmethod def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\" class BackendAgentFactory ( ABC ): \"\"\"Backend agent factory defining the currently available agents (algorithms). \"\"\" backend_name : str = 'abstract_BackendAgentFactory' tensorflow_v2_eager_compatible : bool = True def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ]( model_config = model_config ) return result def get_algorithms ( self ) -> Dict [ Type , Type [ _BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return {} Classes BackendAgent class BackendAgent ( model_config : easyagents . core . ModelConfig , backend_name : str , tensorflow_v2_eager : bool = True ) Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. View Source class BackendAgent ( _BackendAgent , metaclass = ABCMeta ) : \"\"\" Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent . \"\"\" @ abstractmethod def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form : while True : on_play_episode_begin ( env ) state = env . reset () while True : action = _trained_policy . action ( state ) ( state , reward , done , info ) = env . step ( action ) if done : break on_play_episode_end () if play_context . play_done : break Args : play_context : play configuration to be used \"\"\" @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\" Ancestors (in MRO) easyagents.backends.core._BackendAgent abc.ABC Descendants easyagents.backends.tfagents.TfAgent easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.tforce.TforceAgent Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used View Source @ abstractmethod def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form : while True : on_play_episode_begin ( env ) state = env . reset () while True : action = _trained_policy . action ( state ) ( state , reward , done , info ) = env . step ( action ) if done : break on_play_episode_end () if play_context . play_done : break Args : play_context : play configuration to be used \"\"\" train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\" BackendAgentFactory class BackendAgentFactory ( / , * args , ** kwargs ) Backend agent factory defining the currently available agents (algorithms). View Source class BackendAgentFactory ( ABC ) : \"\"\" Backend agent factory defining the currently available agents (algorithms). \"\"\" backend_name : str = ' abstract_BackendAgentFactory ' tensorflow_v2_eager_compatible : bool = True def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result def get_algorithms ( self ) -> Dict [ Type , Type [ _BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return {} Ancestors (in MRO) abc.ABC Descendants easyagents.backends.tfagents.TfAgentAgentFactory easyagents.backends.kerasrl.KerasRlAgentFactory easyagents.backends.default.BackendAgentFactory easyagents.backends.tforce.TensorforceAgentFactory Class variables backend_name tensorflow_v2_eager_compatible Methods create_agent def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result get_algorithms def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . _BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ _BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return {}","title":"Core"},{"location":"reference/easyagents/backends/core/#module-easyagentsbackendscore","text":"This module contains backend core classes like Backend and BackendAgent. The concrete backends like tfagent or baselines are implemented in seprate modules. View Source \"\"\"This module contains backend core classes like Backend and BackendAgent. The concrete backends like tfagent or baselines are implemented in seprate modules. \"\"\" from abc import ABC , ABCMeta , abstractmethod from typing import List , Optional , Tuple , Type , Dict import gym import tensorflow import numpy import random from easyagents import core from easyagents.backends import monitor from easyagents.callbacks import plot _tensorflow_v2_eager_enabled : Optional [ bool ] = None class _BackendEvalCallback ( core . AgentCallback ): \"\"\"Evaluates an agents current policy and updates its train_context accordingly.\"\"\" def __init__ ( self , train_context : core . TrainContext ): assert train_context , \"train_context not set\" assert train_context . num_episodes_per_eval > 0 , \"num_episodes_per_eval is 0.\" self . _train_contex = train_context def on_play_episode_end ( self , agent_context : core . AgentContext ): pc = agent_context . play tc = self . _train_contex sum_of_r = pc . sum_of_rewards . values () tc . eval_rewards [ tc . episodes_done_in_training ] = ( min ( sum_of_r ), sum ( sum_of_r ) / len ( sum_of_r ), max ( sum_of_r )) steps = [ len ( episode_rewards ) for episode_rewards in pc . rewards . values ()] tc . eval_steps [ tc . episodes_done_in_training ] = ( min ( steps ), sum ( steps ) / len ( steps ), max ( steps )) class _BackendAgent ( ABC ): \"\"\"Base class for all backend agent implementations. Implements the train loop and calls the Callbacks. \"\"\" def __init__ ( self , model_config : core . ModelConfig , backend_name : str , tensorflow_v2_eager : bool = True ): \"\"\" Args: model_config: defines the model and environment to be used backend_name: id of the backend to which this agent belongs to. tensorflow_v2_eager: the execution mode, enforced for this and all other backend agents. \"\"\" assert model_config is not None , \"model_config not set.\" assert backend_name self . _backend_name : str = backend_name self . _tensorflow_v2_eager = tensorflow_v2_eager self . model_config = model_config self . _agent_context : core . AgentContext = core . AgentContext ( self . model_config ) self . _agent_context . gym . _totals = monitor . _register_gym_monitor ( self . model_config . original_env_name ) self . model_config . gym_env_name = self . _agent_context . gym . _totals . gym_env_name self . _preprocess_callbacks : List [ core . _PreProcessCallback ] = [ plot . _PreProcess ()] self . _callbacks : List [ core . AgentCallback ] = [] self . _postprocess_callbacks : List [ core . _PostProcessCallback ] = [ plot . _PostProcess ()] self . _train_total_episodes_on_iteration_begin : int = 0 self . _initialize_tensorflow () def _initialize_tensorflow ( self ): \"\"\" v2 behavior and eager execution mode. if a previous backend selected a different mode an exceptionis raised.\"\"\" global _tensorflow_v2_eager_enabled if _tensorflow_v2_eager_enabled is None : _tensorflow_v2_eager_enabled = self . _tensorflow_v2_eager if _tensorflow_v2_eager_enabled : self . log_api ( 'tf.compat.v1.enable_v2_behavior' ) tensorflow . compat . v1 . enable_v2_behavior () self . log_api ( 'tf.compat.v1.enable_eager_execution' ) tensorflow . compat . v1 . enable_eager_execution () assert _tensorflow_v2_eager_enabled == self . _tensorflow_v2_eager , \\ \"v2 behavior and eager execution mode already selected by another backend does not match \" + \\ \"the requirements of this backend. \" + \\ \"To avoid the conflict, do not combine both backend types in the same python / jupyter kernel instance. \" return def _set_seed ( self ): \"\"\" sets the random seeds for all dependent packages \"\"\" if not self . model_config . seed is None : seed = self . model_config . seed self . log_api ( f 'tf.compat.v1.set_random_seed' , f '({seed})' ) tensorflow . compat . v1 . set_random_seed ( seed ) self . log_api ( f 'tf.random.set_random_seed' , f '(seed={seed})' ) tensorflow . random . set_random_seed ( seed = seed ) self . log_api ( f 'numpy.random.seed' , f '({seed})' ) numpy . random . seed ( seed ) self . log_api ( f 'random.seed' , f '({seed})' ) random . seed ( seed ) return def _eval_current_policy ( self ): \"\"\"Evaluates the current policy using play and updates the train_context If num_episodes_per_eval or num_iterations_per_eval is 0 no evaluation is performed. \"\"\" tc = self . _agent_context . train assert tc , \"train_context not set\" if tc . num_episodes_per_eval and tc . num_iterations_between_eval : callbacks = [ _BackendEvalCallback ( self . _agent_context . train )] + self . _callbacks self . play ( play_context = core . PlayContext ( self . _agent_context . train ), callbacks = callbacks ) def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ): \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) def _on_gym_init_begin ( self ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Hint: the total instances count is not incremented yet.\"\"\" self . _agent_context . gym . _monitor_env = None for c in self . _callbacks : c . on_gym_init_begin ( self . _agent_context ) self . _agent_context . gym . _monitor_env = None def _on_gym_init_end ( self , env : monitor . _MonitorEnv ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Hint: o the total instances count is incremented by now o the new env (and its action space) is seeded with the api_context's seed \"\"\" self . _agent_context . gym . _monitor_env = env if self . _agent_context . model . seed is not None : env = self . _agent_context . gym . gym_env seed = self . _agent_context . model . seed env . seed ( seed ) for c in self . _callbacks : c . on_gym_init_end ( self . _agent_context ) self . _agent_context . gym . _monitor_env = None def _on_gym_reset_begin ( self , env : monitor . _MonitorEnv , ** kwargs ): \"\"\"called when the monitored environment begins a reset. Hint: the total reset count is not incremented yet.\"\"\" self . _agent_context . gym . _monitor_env = env for c in self . _callbacks : c . on_gym_reset_begin ( self . _agent_context , ** kwargs ) self . _agent_context . gym . _monitor_env = None def _on_gym_reset_end ( self , env : monitor . _MonitorEnv , reset_result : Tuple , ** kwargs ): \"\"\"called when the monitored environment completed a reset. Hint: the total episode count is incremented by now (if a step was performed before the last reset).\"\"\" self . _agent_context . gym . _monitor_env = env for c in self . _callbacks : c . on_gym_reset_end ( self . _agent_context , reset_result , ** kwargs ) self . _agent_context . gym . _monitor_env = None def _on_gym_step_begin ( self , env : monitor . _MonitorEnv , action ): \"\"\"called when the monitored environment begins a step. Hint: o sets env.max_steps_per_episode if we are in train / play. Thus the episode is ended by the MonitorEnv if the step limit is exceeded \"\"\" ac = self . _agent_context ac . gym . _monitor_env = env env . max_steps_per_episode = None if ac . is_play or ac . is_eval : env . max_steps_per_episode = ac . play . max_steps_per_episode self . _on_play_step_begin ( action ) if ac . is_train : env . max_steps_per_episode = ac . train . max_steps_per_episode self . _on_train_step_begin ( action ) for c in self . _callbacks : c . on_gym_step_begin ( self . _agent_context , action ) self . _agent_context . gym . _monitor_env = None def _on_gym_step_end ( self , env : monitor . _MonitorEnv , action , step_result : Tuple ): \"\"\"called when the monitored environment completed a step. Args: env: the gym_env the last step was done on step_result: the result (state, reward, done, info) of the last step call \"\"\" ac = self . _agent_context ac . gym . _monitor_env = env if ac . is_play or ac . is_eval : self . _on_play_step_end ( action , step_result ) if ac . is_train : self . _on_train_step_end ( action , step_result ) for c in self . _callbacks : c . on_gym_step_end ( self . _agent_context , action , step_result ) self . _agent_context . gym . _monitor_env = None env . max_steps_per_episode = None def _on_play_begin ( self ): \"\"\"Must NOT be called by play_implementation\"\"\" for c in self . _callbacks : c . on_play_begin ( self . _agent_context ) def _on_play_end ( self ): \"\"\"Must NOT be called by play_implementation\"\"\" for c in self . _callbacks : c . on_play_end ( self . _agent_context ) self . _agent_context . play . gym_env = None def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) def _on_play_step_begin ( self , action ): \"\"\"Called before each call to gym.step on the current play env (agent_context.play.gym_env) Args: action: the action to be passed to the upcoming gym_env.step call \"\"\" for c in self . _callbacks : c . on_play_step_begin ( self . _agent_context , action ) def _on_play_step_end ( self , action , step_result : Tuple ): \"\"\"Called after each call to gym.step on the current play env (agent_context.play.gym_env) Args: step_result: the result (state, reward, done, info) of the last step call \"\"\" ( state , reward , done , info ) = step_result pc = self . _agent_context . play pc . steps_done_in_episode += 1 pc . steps_done += 1 pc . actions [ pc . episodes_done + 1 ] . append ( action ) pc . rewards [ pc . episodes_done + 1 ] . append ( reward ) pc . sum_of_rewards [ pc . episodes_done + 1 ] += reward for c in self . _callbacks : c . on_play_step_end ( self . _agent_context , action , step_result ) def _on_train_begin ( self ): \"\"\"Must NOT be called by train_implementation\"\"\" for c in self . _callbacks : c . on_train_begin ( self . _agent_context ) def _on_train_end ( self ): \"\"\"Must NOT be called by train_implementation\"\"\" tc = self . _agent_context . train if tc . episodes_done_in_training not in tc . eval_rewards : self . _eval_current_policy () for c in self . _callbacks : c . on_train_end ( self . _agent_context ) def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) def on_train_iteration_end ( self , loss : float , ** kwargs ): \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs: for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ): prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ): self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) def _on_train_step_begin ( self , action ): \"\"\"Called before each call to gym.step on the current train env (agent_context.train.gym_env) Args: action: the action to be passed to the upcoming gym_env.step call \"\"\" pass # noinspection PyUnusedLocal def _on_train_step_end ( self , action : object , step_result : Tuple ): \"\"\"Called after each call to gym.step on the current train env (agent_context.train.gym_env) Args: step_result: the result (state, reward, done, info) of the last step call \"\"\" tc = self . _agent_context . train tc . steps_done_in_iteration += 1 tc . steps_done_in_training += 1 def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None @abstractmethod def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episode with the current policy. For implementation details see BackendBaseAgent. \"\"\" def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None @abstractmethod def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Agent specific implementation of the train loop. For implementational details see BackendBaseAgent. \"\"\" class BackendAgent ( _BackendAgent , metaclass = ABCMeta ): \"\"\"Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. \"\"\" @abstractmethod def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used \"\"\" @abstractmethod def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\" class BackendAgentFactory ( ABC ): \"\"\"Backend agent factory defining the currently available agents (algorithms). \"\"\" backend_name : str = 'abstract_BackendAgentFactory' tensorflow_v2_eager_compatible : bool = True def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ]( model_config = model_config ) return result def get_algorithms ( self ) -> Dict [ Type , Type [ _BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return {}","title":"Module easyagents.backends.core"},{"location":"reference/easyagents/backends/core/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/backends/core/#backendagent","text":"class BackendAgent ( model_config : easyagents . core . ModelConfig , backend_name : str , tensorflow_v2_eager : bool = True ) Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. View Source class BackendAgent ( _BackendAgent , metaclass = ABCMeta ) : \"\"\" Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent . \"\"\" @ abstractmethod def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form : while True : on_play_episode_begin ( env ) state = env . reset () while True : action = _trained_policy . action ( state ) ( state , reward , done , info ) = env . step ( action ) if done : break on_play_episode_end () if play_context . play_done : break Args : play_context : play configuration to be used \"\"\" @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\"","title":"BackendAgent"},{"location":"reference/easyagents/backends/core/#ancestors-in-mro","text":"easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/core/#descendants","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.tforce.TforceAgent","title":"Descendants"},{"location":"reference/easyagents/backends/core/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/backends/core/#log","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/core/#log_api","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/core/#on_play_episode_begin","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/core/#on_play_episode_end","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/core/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/core/#on_train_iteration_end","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/core/#play","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/core/#play_implementation","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used View Source @ abstractmethod def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form : while True : on_play_episode_begin ( env ) state = env . reset () while True : action = _trained_policy . action ( state ) ( state , reward , done , info ) = env . step ( action ) if done : break on_play_episode_end () if play_context . play_done : break Args : play_context : play configuration to be used \"\"\"","title":"play_implementation"},{"location":"reference/easyagents/backends/core/#train","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/core/#train_implementation","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\"","title":"train_implementation"},{"location":"reference/easyagents/backends/core/#backendagentfactory","text":"class BackendAgentFactory ( / , * args , ** kwargs ) Backend agent factory defining the currently available agents (algorithms). View Source class BackendAgentFactory ( ABC ) : \"\"\" Backend agent factory defining the currently available agents (algorithms). \"\"\" backend_name : str = ' abstract_BackendAgentFactory ' tensorflow_v2_eager_compatible : bool = True def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result def get_algorithms ( self ) -> Dict [ Type , Type [ _BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return {}","title":"BackendAgentFactory"},{"location":"reference/easyagents/backends/core/#ancestors-in-mro_1","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/core/#descendants_1","text":"easyagents.backends.tfagents.TfAgentAgentFactory easyagents.backends.kerasrl.KerasRlAgentFactory easyagents.backends.default.BackendAgentFactory easyagents.backends.tforce.TensorforceAgentFactory","title":"Descendants"},{"location":"reference/easyagents/backends/core/#class-variables","text":"backend_name tensorflow_v2_eager_compatible","title":"Class variables"},{"location":"reference/easyagents/backends/core/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/backends/core/#create_agent","text":"def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result","title":"create_agent"},{"location":"reference/easyagents/backends/core/#get_algorithms","text":"def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . _BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ _BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return {}","title":"get_algorithms"},{"location":"reference/easyagents/backends/kerasrl/","text":"Module easyagents.backends.kerasrl This module contains the backend implementation for keras-rl (see https://github.com/keras-rl/keras-rl) View Source \"\"\"This module contains the backend implementation for keras-rl (see https://github.com/keras-rl/keras-rl)\"\"\" from abc import ABCMeta from typing import Optional , Dict , Type import math # noinspection PyUnresolvedReferences import easyagents.agents from easyagents import core from easyagents.backends import core as bcore import keras.backend as K from keras.layers import Activation , Flatten , Lambda , Dense from keras.models import Sequential , Model from keras.optimizers import Adam import rl.core from rl.agents.dqn import DQNAgent from rl.agents.cem import CEMAgent from rl.callbacks import Callback from rl.policy import BoltzmannQPolicy , EpsGreedyQPolicy , GreedyQPolicy from rl.memory import EpisodeParameterMemory , SequentialMemory import gym import gym.spaces # noinspection PyUnresolvedReferences,PyAbstractClass class KerasRlAgent ( bcore . BackendAgent , metaclass = ABCMeta ): \"\"\"Reinforcement learning agents based on keras-rl originally developed by matthias plappert https://github.com/keras-rl/keras-rl \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config , backend_name = KerasRlAgentFactory . backend_name , tensorflow_v2_eager = False ) self . _agent : Optional [ rl . core . Agent ] = None self . _play_env : Optional [ gym . Env ] = None def _create_env ( self ) -> gym . Env : \"\"\"Creates a new gym instance.\"\"\" self . log_api ( f 'gym.make' , f '(\"{self.model_config.original_env_name}\")' ) result = gym . make ( self . model_config . gym_env_name ) return result def _create_model ( self , gym_env : gym . Env , activation : str ) -> Sequential : \"\"\"Creates a model consisting of fully connected layers as given by self.model_config.fc_layers with relu as activation function. Args: gym_env: gym_env whose observation shape ofdefines the size of the input layer and whose action_space defines the size of the output layer. activation: output activation function eg 'linear' or 'softmax' Returns: Keras Sequential model according to model_config.fc_layers \"\"\" assert gym_env num_actions = gym_env . action_space . n self . log_api ( f 'Sequential' , f '()' ) result = Sequential () input_shape = ( 1 ,) + gym_env . observation_space . shape self . log_api ( f 'model.add' , f '(Flatten(input_shape={input_shape}))' ) result . add ( Flatten ( input_shape = input_shape )) for layer_size in self . model_config . fc_layers : self . log_api ( f 'model.add' , f '(Dense({layer_size}))' ) result . add ( Dense ( layer_size )) self . log_api ( f 'model.add' , f '(Activation(\"relu\"))' ) result . add ( Activation ( 'relu' )) self . log_api ( f 'model.add' , f '(Dense({num_actions}))' ) result . add ( Dense ( num_actions )) self . log_api ( f 'model.add' , f '(Activation(\"{activation}\"))' ) result . add ( Activation ( activation )) return result def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _agent , \"_agent not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break class KerasRlCemAgent ( KerasRlAgent ): \"\"\"Keras-rl implementation of the cross-entropy method algorithm. see \"https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf\" and \"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf\" \"\"\" class CemCallback ( rl . callbacks . Callback ): \"\"\"Callback registered with keras rl agents to propagate iteration and episode updates.\"\"\" def __init__ ( self , cem_agent : KerasRlAgent , cem_context : core . CemTrainContext , nb_steps : int ): \"\"\" Args: cem_agent: the agent to propagate iteration begn/end events to. cem_context: the train_context containing the iteration definitions nb_steps: value set in the keras cem agent. \"\"\" assert cem_agent assert cem_context assert nb_steps self . _cem_agent : KerasRlAgent = cem_agent self . _cem_context : core . CemTrainContext = cem_context self . _nb_steps = nb_steps super () . __init__ () def on_episode_end ( self , episode , logs = None ): \"\"\"Signals the base class the end / begin of a training iteration.\"\"\" cc : core . CemTrainContext = self . _cem_context episode = episode + 1 if episode % cc . num_episodes_per_iteration == 0 : self . _cem_agent . on_train_iteration_end ( math . nan ) if self . _cem_context . training_done : self . _cem_agent . _agent . step = self . _nb_steps else : self . _cem_agent . on_train_iteration_begin () def train_implementation ( self , train_context : core . CemTrainContext ): assert train_context cc : core . CemTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = 'softmax' ) policy_buffer_size = 5 * cc . num_episodes_per_iteration self . log_api ( f 'EpisodeParameterMemory' , f '(limit={policy_buffer_size}, window_length=1)' ) memory = EpisodeParameterMemory ( limit = policy_buffer_size , window_length = 1 ) num_actions = train_env . action_space . n self . log_api ( f 'CEMAgent' , f '(model=..., nb_actions={num_actions}, memory=..., ' + \\ f 'nb_steps_warmup={cc.num_steps_buffer_preload}, ' + \\ f 'train_interval={cc.num_episodes_per_iteration}, ' + \\ f 'batch_size={cc.num_episodes_per_iteration}, ' + \\ f 'elite_frac={cc.elite_set_fraction})' ) self . _agent = CEMAgent ( model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = cc . num_steps_buffer_preload , batch_size = cc . num_episodes_per_iteration , train_interval = cc . num_episodes_per_iteration , elite_frac = cc . elite_set_fraction ) self . log_api ( f 'agent.compile' , '()' ) self . _agent . compile () nb_steps = cc . num_iterations * cc . num_episodes_per_iteration * cc . max_steps_per_episode callback = KerasRlCemAgent . CemCallback ( self , cc , nb_steps ) self . on_train_iteration_begin () self . log_api ( f 'agent.fit' , f '(train_env, nb_steps={nb_steps})' ) self . _agent . fit ( train_env , nb_steps = nb_steps , visualize = False , verbose = 0 , callbacks = [ callback ]) if not cc . training_done : self . on_train_iteration_end ( math . nan ) class KerasRlDqnAgent ( KerasRlAgent ): \"\"\"Keras-rl implementation of the algorithm described in in Mnih (2013) and Mnih (2015). http://arxiv.org/pdf/1312.5602.pdf and http://arxiv.org/abs/1509.06461 includes implementations for the double dqn and dueling dqn variations. \"\"\" class DQNAgentWrapper ( DQNAgent ): \"\"\"Override of the KerasRl DqnAgennt instantiation due to a conflict with tensorflow 1.15. Essentially a copy of https://raw.githubusercontent.com/keras-rl/keras-rl/master/rl/agents/dqn.py \"\"\" def __init__ ( self , model , policy = None , test_policy = None , enable_double_dqn = False , enable_dueling_network = False , dueling_type = 'avg' , * args , ** kwargs ): super ( DQNAgent , self ) . __init__ ( * args , ** kwargs ) if model . output . _keras_shape != ( None , self . nb_actions ): raise ValueError ( f 'Model output \"{model.output}\" has invalid shape. Dqn expects ' + f 'a model that has one dimension for each action, in this case {self.nb_actions}.' ) self . enable_double_dqn = enable_double_dqn self . enable_dueling_network = enable_dueling_network self . dueling_type = dueling_type if self . enable_dueling_network : layer = model . layers [ - 2 ] nb_action = model . output . _keras_shape [ - 1 ] y = Dense ( nb_action + 1 , activation = 'linear' )( layer . output ) if self . dueling_type == 'avg' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :] - K . mean ( a [:, 1 :], keepdims = True ), output_shape = ( nb_action ,))( y ) elif self . dueling_type == 'max' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :] - K . max ( a [:, 1 :], keepdims = True ), output_shape = ( nb_action ,))( y ) elif self . dueling_type == 'naive' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :], output_shape = ( nb_action ,))( y ) else : assert False , \"dueling_type must be one of {'avg','max','naive'}\" model = Model ( inputs = model . input , outputs = outputlayer ) self . model = model if policy is None : policy = EpsGreedyQPolicy () if test_policy is None : test_policy = GreedyQPolicy () self . policy = policy self . test_policy = test_policy self . reset_states () class DqnCallback ( rl . callbacks . Callback ): \"\"\"Callback registered with keras rl agents to propagate iteration and episode updates.\"\"\" def __init__ ( self , agent : bcore . BackendAgent , dqn_context : core . StepsTrainContext , loss_metric_idx : Optional [ int ]): \"\"\" Args: agent: the agent to propagate iteration begn/end events to. dqn_context: the train_context containing the iteration definitions loss_metric_idx: the index of the loss in the metrics list, or None \"\"\" assert agent assert dqn_context self . _agent = agent self . _dqn_context = dqn_context self . _loss_metric_idx = loss_metric_idx super () . __init__ () def on_step_end ( self , step , logs = None ): \"\"\"Signals the base class the end / begin of a training iteration.\"\"\" steps_done = self . _dqn_context . steps_done_in_training - self . _dqn_context . num_steps_buffer_preload if steps_done > 0 and steps_done % self . _dqn_context . num_steps_per_iteration == 0 : loss = math . nan if logs and 'metrics' in logs and ( self . _loss_metric_idx is not None ): metrics = logs [ 'metrics' ] if len ( metrics ) > self . _loss_metric_idx : loss = metrics [ self . _loss_metric_idx ] self . _agent . on_train_iteration_end ( loss ) if not self . _dqn_context . training_done : self . _agent . on_train_iteration_begin () def __init__ ( self , model_config : core . ModelConfig , enable_dueling_dqn : bool = False , enable_double_dqn = False ): \"\"\" creates a new agent based on the DQN algorithm using the keras-rl implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: use the double dqn algorithm instead enable_dueling_dqn: use the dueling dqn algorithm instead \"\"\" super () . __init__ ( model_config = model_config ) self . _enable_double_dqn : bool = enable_double_dqn self . _enable_dueling_network : bool = enable_dueling_dqn def train_implementation ( self , train_context : core . StepsTrainContext ): assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = 'linear' ) self . log_api ( f 'SequentialMemory' , f '(limit={dc.max_steps_in_buffer}, window_length=1)' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f 'BoltzmannQPolicy' , f '()' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f 'DQNAgent' , f '(nb_actions={num_actions}, ' + f 'enable_double_dqn={self._enable_double_dqn}, ' + f 'enable_dueling_network={self._enable_dueling_network}, ' + f 'nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2,' + f 'gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f 'train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...)' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1e-2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f 'agent.compile' , f '(Adam(lr=1e-3), metrics=[\"mae\"]' ) self . _agent . compile ( Adam ( lr = 1e-3 ), metrics = [ 'mae' ]) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if 'loss' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \"loss\" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f 'agent.fit' , f '(train_env, nb_steps={num_steps})' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ]) if not dc . training_done : self . on_train_iteration_end ( math . nan ) class KerasRlDoubleDqnAgent ( KerasRlDqnAgent ): \"\"\"Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1509.06461 \"\"\" def __init__ ( self , model_config : core . ModelConfig ): \"\"\"Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" super () . __init__ ( model_config = model_config , enable_double_dqn = True ) class KerasRlDuelingDqnAgent ( KerasRlDqnAgent ): \"\"\"Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1511.06581 \"\"\" def __init__ ( self , model_config : core . ModelConfig ): \"\"\" creates a new agent based on the DQN algorithm using the keras-rl implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: \"\"\" super () . __init__ ( model_config = model_config , enable_dueling_dqn = True ) class CemKerasRlAgent ( KerasRlAgent ): \"\"\" creates a new agent based on the CEM algorithm using the keras-rl implementation. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) def train_implementation ( self , train_context : core . StepsTrainContext ): \"\"\" train_env = self._create_env() assert isinstance(train_env,gym.spaces.Discrete), \"Only discrete actions environment are supported.\" action_space : gym.spaces.Discrete = train_env.action_space memory = EpisodeParameterMemory(limit=train_context.max_steps_in_buffer, window_length=1) cem = CEMAgent(model=model, nb_actions=action_space.n, memory=memory, batch_size=train_context.num_steps_sampled_from_buffer, nb_steps_warmup=train_context.num_steps_buffer_preload, train_interval=50, elite_frac=0.05) cem.compile() \"\"\" class KerasRlAgentFactory ( bcore . BackendAgentFactory ): \"\"\"Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the keras-rl implementations. \"\"\" backend_name : str = 'kerasrl' tensorflow_v2_eager_compatible : bool = False def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . CemAgent : KerasRlCemAgent , easyagents . agents . DqnAgent : KerasRlDqnAgent , easyagents . agents . DoubleDqnAgent : KerasRlDoubleDqnAgent , easyagents . agents . DuelingDqnAgent : KerasRlDuelingDqnAgent , } Classes CemKerasRlAgent class CemKerasRlAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the CEM algorithm using the keras-rl implementation. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class CemKerasRlAgent ( KerasRlAgent ): \"\"\" creates a new agent based on the CEM algorithm using the keras-rl implementation. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) def train_implementation ( self , train_context: core . StepsTrainContext ): \"\"\" train_env = self._create_env() assert isinstance(train_env,gym.spaces.Discrete), \" Only discrete actions environment are supported . \" action_space : gym.spaces.Discrete = train_env.action_space memory = EpisodeParameterMemory(limit=train_context.max_steps_in_buffer, window_length=1) cem = CEMAgent(model=model, nb_actions=action_space.n, memory=memory, batch_size=train_context.num_steps_sampled_from_buffer, nb_steps_warmup=train_context.num_steps_buffer_preload, train_interval=50, elite_frac=0.05) cem.compile() \"\"\" Ancestors (in MRO) easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) train_env = self._create_env() assert isinstance(train_env,gym.spaces.Discrete), \"Only discrete actions environment are supported.\" action_space : gym.spaces.Discrete = train_env.action_space memory = EpisodeParameterMemory(limit=train_context.max_steps_in_buffer, window_length=1) cem = CEMAgent(model=model, nb_actions=action_space.n, memory=memory, batch_size=train_context.num_steps_sampled_from_buffer, nb_steps_warmup=train_context.num_steps_buffer_preload, train_interval=50, elite_frac=0.05) cem.compile() View Source def train_implementation ( self , train_context : core . StepsTrainContext ): \"\"\" train_env = self._create_env() assert isinstance(train_env,gym.spaces.Discrete), \" Only discrete actions environment are supported . \" action_space : gym.spaces.Discrete = train_env.action_space memory = EpisodeParameterMemory(limit=train_context.max_steps_in_buffer, window_length=1) cem = CEMAgent(model=model, nb_actions=action_space.n, memory=memory, batch_size=train_context.num_steps_sampled_from_buffer, nb_steps_warmup=train_context.num_steps_buffer_preload, train_interval=50, elite_frac=0.05) cem.compile() \"\"\" KerasRlAgent class KerasRlAgent ( model_config : easyagents . core . ModelConfig ) Reinforcement learning agents based on keras-rl originally developed by matthias plappert https://github.com/keras-rl/keras-rl View Source class KerasRlAgent ( bcore . BackendAgent , metaclass = ABCMeta ) : \"\"\" Reinforcement learning agents based on keras-rl originally developed by matthias plappert https : // github . com / keras - rl / keras - rl \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config , backend_name = KerasRlAgentFactory . backend_name , tensorflow_v2_eager = False ) self . _agent : Optional [ rl . core . Agent ] = None self . _play_env : Optional [ gym . Env ] = None def _create_env ( self ) -> gym . Env : \"\"\" Creates a new gym instance. \"\"\" self . log_api ( f ' gym.make ' , f ' (\"{self.model_config.original_env_name}\") ' ) result = gym . make ( self . model_config . gym_env_name ) return result def _create_model ( self , gym_env : gym . Env , activation : str ) -> Sequential : \"\"\" Creates a model consisting of fully connected layers as given by self.model_config.fc_layers with relu as activation function . Args : gym_env : gym_env whose observation shape ofdefines the size of the input layer and whose action_space defines the size of the output layer . activation : output activation function eg ' linear ' or ' softmax ' Returns : Keras Sequential model according to model_config . fc_layers \"\"\" assert gym_env num_actions = gym_env . action_space . n self . log_api ( f ' Sequential ' , f ' () ' ) result = Sequential () input_shape = ( 1 , ) + gym_env . observation_space . shape self . log_api ( f ' model.add ' , f ' (Flatten(input_shape={input_shape})) ' ) result . add ( Flatten ( input_shape = input_shape )) for layer_size in self . model_config . fc_layers : self . log_api ( f ' model.add ' , f ' (Dense({layer_size})) ' ) result . add ( Dense ( layer_size )) self . log_api ( f ' model.add ' , f ' (Activation(\"relu\")) ' ) result . add ( Activation ( ' relu ' )) self . log_api ( f ' model.add ' , f ' (Dense({num_actions})) ' ) result . add ( Dense ( num_actions )) self . log_api ( f ' model.add ' , f ' (Activation(\"{activation}\")) ' ) result . add ( Activation ( activation )) return result def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break Ancestors (in MRO) easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Descendants easyagents.backends.kerasrl.KerasRlCemAgent easyagents.backends.kerasrl.KerasRlDqnAgent easyagents.backends.kerasrl.CemKerasRlAgent Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\" KerasRlAgentFactory class KerasRlAgentFactory ( / , * args , ** kwargs ) Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the keras-rl implementations. View Source class KerasRlAgentFactory ( bcore . BackendAgentFactory ) : \"\"\" Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the keras - rl implementations . \"\"\" backend_name : str = ' kerasrl ' tensorflow_v2_eager_compatible : bool = False def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . CemAgent : KerasRlCemAgent , easyagents . agents . DqnAgent : KerasRlDqnAgent , easyagents . agents . DoubleDqnAgent : KerasRlDoubleDqnAgent , easyagents . agents . DuelingDqnAgent : KerasRlDuelingDqnAgent , } Ancestors (in MRO) easyagents.backends.core.BackendAgentFactory abc.ABC Class variables backend_name tensorflow_v2_eager_compatible Methods create_agent def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result get_algorithms def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . CemAgent : KerasRlCemAgent , easyagents . agents . DqnAgent : KerasRlDqnAgent , easyagents . agents . DoubleDqnAgent : KerasRlDoubleDqnAgent , easyagents . agents . DuelingDqnAgent : KerasRlDuelingDqnAgent , } KerasRlCemAgent class KerasRlCemAgent ( model_config : easyagents . core . ModelConfig ) Keras-rl implementation of the cross-entropy method algorithm. see \"https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf\" and \"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf\" View Source class KerasRlCemAgent ( KerasRlAgent ) : \"\"\" Keras-rl implementation of the cross-entropy method algorithm. see \" https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf \" and \" https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf \" \"\"\" class CemCallback ( rl . callbacks . Callback ) : \"\"\" Callback registered with keras rl agents to propagate iteration and episode updates. \"\"\" def __init__ ( self , cem_agent : KerasRlAgent , cem_context : core . CemTrainContext , nb_steps : int ) : \"\"\" Args : cem_agent : the agent to propagate iteration begn / end events to . cem_context : the train_context containing the iteration definitions nb_steps : value set in the keras cem agent . \"\"\" assert cem_agent assert cem_context assert nb_steps self . _cem_agent : KerasRlAgent = cem_agent self . _cem_context : core . CemTrainContext = cem_context self . _nb_steps = nb_steps super () . __init__ () def on_episode_end ( self , episode , logs = None ) : \"\"\" Signals the base class the end / begin of a training iteration. \"\"\" cc : core . CemTrainContext = self . _cem_context episode = episode + 1 if episode % cc . num_episodes_per_iteration == 0 : self . _cem_agent . on_train_iteration_end ( math . nan ) if self . _cem_context . training_done : self . _cem_agent . _agent . step = self . _nb_steps else : self . _cem_agent . on_train_iteration_begin () def train_implementation ( self , train_context : core . CemTrainContext ) : assert train_context cc : core . CemTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' softmax ' ) policy_buffer_size = 5 * cc . num_episodes_per_iteration self . log_api ( f ' EpisodeParameterMemory ' , f ' (limit={policy_buffer_size}, window_length=1) ' ) memory = EpisodeParameterMemory ( limit = policy_buffer_size , window_length = 1 ) num_actions = train_env . action_space . n self . log_api ( f ' CEMAgent ' , f ' (model=..., nb_actions={num_actions}, memory=..., ' + \\ f ' nb_steps_warmup={cc.num_steps_buffer_preload}, ' + \\ f ' train_interval={cc.num_episodes_per_iteration}, ' + \\ f ' batch_size={cc.num_episodes_per_iteration}, ' + \\ f ' elite_frac={cc.elite_set_fraction}) ' ) self . _agent = CEMAgent ( model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = cc . num_steps_buffer_preload , batch_size = cc . num_episodes_per_iteration , train_interval = cc . num_episodes_per_iteration , elite_frac = cc . elite_set_fraction ) self . log_api ( f ' agent.compile ' , ' () ' ) self . _agent . compile () nb_steps = cc . num_iterations * cc . num_episodes_per_iteration * cc . max_steps_per_episode callback = KerasRlCemAgent . CemCallback ( self , cc , nb_steps ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={nb_steps}) ' ) self . _agent . fit ( train_env , nb_steps = nb_steps , visualize = False , verbose = 0 , callbacks = [ callback ] ) if not cc . training_done : self . on_train_iteration_end ( math . nan ) Ancestors (in MRO) easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Class variables CemCallback Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . CemTrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . CemTrainContext ) : assert train_context cc : core . CemTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' softmax ' ) policy_buffer_size = 5 * cc . num_episodes_per_iteration self . log_api ( f ' EpisodeParameterMemory ' , f ' (limit={policy_buffer_size}, window_length=1) ' ) memory = EpisodeParameterMemory ( limit = policy_buffer_size , window_length = 1 ) num_actions = train_env . action_space . n self . log_api ( f ' CEMAgent ' , f ' (model=..., nb_actions={num_actions}, memory=..., ' + \\ f ' nb_steps_warmup={cc.num_steps_buffer_preload}, ' + \\ f ' train_interval={cc.num_episodes_per_iteration}, ' + \\ f ' batch_size={cc.num_episodes_per_iteration}, ' + \\ f ' elite_frac={cc.elite_set_fraction}) ' ) self . _agent = CEMAgent ( model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = cc . num_steps_buffer_preload , batch_size = cc . num_episodes_per_iteration , train_interval = cc . num_episodes_per_iteration , elite_frac = cc . elite_set_fraction ) self . log_api ( f ' agent.compile ' , ' () ' ) self . _agent . compile () nb_steps = cc . num_iterations * cc . num_episodes_per_iteration * cc . max_steps_per_episode callback = KerasRlCemAgent . CemCallback ( self , cc , nb_steps ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={nb_steps}) ' ) self . _agent . fit ( train_env , nb_steps = nb_steps , visualize = False , verbose = 0 , callbacks = [ callback ] ) if not cc . training_done : self . on_train_iteration_end ( math . nan ) KerasRlDoubleDqnAgent class KerasRlDoubleDqnAgent ( model_config : easyagents . core . ModelConfig ) Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1509.06461 View Source class KerasRlDoubleDqnAgent ( KerasRlDqnAgent ): \"\"\"Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1509.06461 \"\"\" def __init__ ( self , model_config: core . ModelConfig ): \"\"\"Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" super (). __init__ ( model_config = model_config , enable_double_dqn = True ) Ancestors (in MRO) easyagents.backends.kerasrl.KerasRlDqnAgent easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Class variables DQNAgentWrapper DqnCallback Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . StepsTrainContext ) : assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' linear ' ) self . log_api ( f ' SequentialMemory ' , f ' (limit={dc.max_steps_in_buffer}, window_length=1) ' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f ' BoltzmannQPolicy ' , f ' () ' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f ' DQNAgent ' , f ' (nb_actions={num_actions}, ' + f ' enable_double_dqn={self._enable_double_dqn}, ' + f ' enable_dueling_network={self._enable_dueling_network}, ' + f ' nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2, ' + f ' gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f ' train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...) ' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1 e - 2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f ' agent.compile ' , f ' (Adam(lr=1e-3), metrics=[\"mae\"] ' ) self . _agent . compile ( Adam ( lr = 1 e - 3 ) , metrics = [ ' mae ' ] ) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if ' loss ' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \" loss \" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={num_steps}) ' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ] ) if not dc . training_done : self . on_train_iteration_end ( math . nan ) KerasRlDqnAgent class KerasRlDqnAgent ( model_config : easyagents . core . ModelConfig , enable_dueling_dqn : bool = False , enable_double_dqn = False ) Keras-rl implementation of the algorithm described in in Mnih (2013) and Mnih (2015). http://arxiv.org/pdf/1312.5602.pdf and http://arxiv.org/abs/1509.06461 includes implementations for the double dqn and dueling dqn variations. View Source class KerasRlDqnAgent ( KerasRlAgent ) : \"\"\" Keras-rl implementation of the algorithm described in in Mnih (2013) and Mnih (2015). http : // arxiv . org / pdf / 1312 . 5602 . pdf and http : // arxiv . org / abs / 1509 . 06461 includes implementations for the double dqn and dueling dqn variations . \"\"\" class DQNAgentWrapper ( DQNAgent ) : \"\"\" Override of the KerasRl DqnAgennt instantiation due to a conflict with tensorflow 1.15. Essentially a copy of https : // raw . githubusercontent . com / keras - rl / keras - rl / master / rl / agents / dqn . py \"\"\" def __init__ ( self , model , policy = None , test_policy = None , enable_double_dqn = False , enable_dueling_network = False , dueling_type = ' avg ' , * args , ** kwargs ) : super ( DQNAgent , self ) . __init__ ( * args , ** kwargs ) if model . output . _keras_shape != ( None , self . nb_actions ) : raise ValueError ( f ' Model output \"{model.output}\" has invalid shape. Dqn expects ' + f ' a model that has one dimension for each action, in this case {self.nb_actions}. ' ) self . enable_double_dqn = enable_double_dqn self . enable_dueling_network = enable_dueling_network self . dueling_type = dueling_type if self . enable_dueling_network : layer = model . layers [ - 2 ] nb_action = model . output . _keras_shape [ - 1 ] y = Dense ( nb_action + 1 , activation = ' linear ' )( layer . output ) if self . dueling_type == ' avg ' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :] - K . mean ( a [:, 1 :], keepdims = True ) , output_shape = ( nb_action , ))( y ) elif self . dueling_type == ' max ' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :] - K . max ( a [:, 1 :], keepdims = True ) , output_shape = ( nb_action , ))( y ) elif self . dueling_type == ' naive ' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :], output_shape = ( nb_action , ))( y ) else : assert False , \" dueling_type must be one of {'avg','max','naive'} \" model = Model ( inputs = model . input , outputs = outputlayer ) self . model = model if policy is None : policy = EpsGreedyQPolicy () if test_policy is None : test_policy = GreedyQPolicy () self . policy = policy self . test_policy = test_policy self . reset_states () class DqnCallback ( rl . callbacks . Callback ) : \"\"\" Callback registered with keras rl agents to propagate iteration and episode updates. \"\"\" def __init__ ( self , agent : bcore . BackendAgent , dqn_context : core . StepsTrainContext , loss_metric_idx : Optional [ int ] ) : \"\"\" Args : agent : the agent to propagate iteration begn / end events to . dqn_context : the train_context containing the iteration definitions loss_metric_idx : the index of the loss in the metrics list , or None \"\"\" assert agent assert dqn_context self . _agent = agent self . _dqn_context = dqn_context self . _loss_metric_idx = loss_metric_idx super () . __init__ () def on_step_end ( self , step , logs = None ) : \"\"\" Signals the base class the end / begin of a training iteration. \"\"\" steps_done = self . _dqn_context . steps_done_in_training - self . _dqn_context . num_steps_buffer_preload if steps_done > 0 and steps_done % self . _dqn_context . num_steps_per_iteration == 0 : loss = math . nan if logs and ' metrics ' in logs and ( self . _loss_metric_idx is not None ) : metrics = logs [ ' metrics ' ] if len ( metrics ) > self . _loss_metric_idx : loss = metrics [ self . _loss_metric_idx ] self . _agent . on_train_iteration_end ( loss ) if not self . _dqn_context . training_done : self . _agent . on_train_iteration_begin () def __init__ ( self , model_config : core . ModelConfig , enable_dueling_dqn : bool = False , enable_double_dqn = False ) : \"\"\" creates a new agent based on the DQN algorithm using the keras-rl implementation. Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . enable_double_dqn : use the double dqn algorithm instead enable_dueling_dqn : use the dueling dqn algorithm instead \"\"\" super () . __init__ ( model_config = model_config ) self . _enable_double_dqn : bool = enable_double_dqn self . _enable_dueling_network : bool = enable_dueling_dqn def train_implementation ( self , train_context : core . StepsTrainContext ) : assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' linear ' ) self . log_api ( f ' SequentialMemory ' , f ' (limit={dc.max_steps_in_buffer}, window_length=1) ' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f ' BoltzmannQPolicy ' , f ' () ' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f ' DQNAgent ' , f ' (nb_actions={num_actions}, ' + f ' enable_double_dqn={self._enable_double_dqn}, ' + f ' enable_dueling_network={self._enable_dueling_network}, ' + f ' nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2, ' + f ' gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f ' train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...) ' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1 e - 2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f ' agent.compile ' , f ' (Adam(lr=1e-3), metrics=[\"mae\"] ' ) self . _agent . compile ( Adam ( lr = 1 e - 3 ) , metrics = [ ' mae ' ] ) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if ' loss ' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \" loss \" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={num_steps}) ' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ] ) if not dc . training_done : self . on_train_iteration_end ( math . nan ) Ancestors (in MRO) easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Descendants easyagents.backends.kerasrl.KerasRlDoubleDqnAgent easyagents.backends.kerasrl.KerasRlDuelingDqnAgent Class variables DQNAgentWrapper DqnCallback Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . StepsTrainContext ) : assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' linear ' ) self . log_api ( f ' SequentialMemory ' , f ' (limit={dc.max_steps_in_buffer}, window_length=1) ' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f ' BoltzmannQPolicy ' , f ' () ' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f ' DQNAgent ' , f ' (nb_actions={num_actions}, ' + f ' enable_double_dqn={self._enable_double_dqn}, ' + f ' enable_dueling_network={self._enable_dueling_network}, ' + f ' nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2, ' + f ' gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f ' train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...) ' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1 e - 2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f ' agent.compile ' , f ' (Adam(lr=1e-3), metrics=[\"mae\"] ' ) self . _agent . compile ( Adam ( lr = 1 e - 3 ) , metrics = [ ' mae ' ] ) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if ' loss ' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \" loss \" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={num_steps}) ' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ] ) if not dc . training_done : self . on_train_iteration_end ( math . nan ) KerasRlDuelingDqnAgent class KerasRlDuelingDqnAgent ( model_config : easyagents . core . ModelConfig ) Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1511.06581 View Source class KerasRlDuelingDqnAgent ( KerasRlDqnAgent ): \"\"\"Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1511.06581 \"\"\" def __init__ ( self , model_config: core . ModelConfig ): \"\"\" creates a new agent based on the DQN algorithm using the keras-rl implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: \"\"\" super (). __init__ ( model_config = model_config , enable_dueling_dqn = True ) Ancestors (in MRO) easyagents.backends.kerasrl.KerasRlDqnAgent easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Class variables DQNAgentWrapper DqnCallback Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . StepsTrainContext ) : assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' linear ' ) self . log_api ( f ' SequentialMemory ' , f ' (limit={dc.max_steps_in_buffer}, window_length=1) ' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f ' BoltzmannQPolicy ' , f ' () ' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f ' DQNAgent ' , f ' (nb_actions={num_actions}, ' + f ' enable_double_dqn={self._enable_double_dqn}, ' + f ' enable_dueling_network={self._enable_dueling_network}, ' + f ' nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2, ' + f ' gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f ' train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...) ' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1 e - 2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f ' agent.compile ' , f ' (Adam(lr=1e-3), metrics=[\"mae\"] ' ) self . _agent . compile ( Adam ( lr = 1 e - 3 ) , metrics = [ ' mae ' ] ) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if ' loss ' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \" loss \" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={num_steps}) ' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ] ) if not dc . training_done : self . on_train_iteration_end ( math . nan )","title":"Kerasrl"},{"location":"reference/easyagents/backends/kerasrl/#module-easyagentsbackendskerasrl","text":"This module contains the backend implementation for keras-rl (see https://github.com/keras-rl/keras-rl) View Source \"\"\"This module contains the backend implementation for keras-rl (see https://github.com/keras-rl/keras-rl)\"\"\" from abc import ABCMeta from typing import Optional , Dict , Type import math # noinspection PyUnresolvedReferences import easyagents.agents from easyagents import core from easyagents.backends import core as bcore import keras.backend as K from keras.layers import Activation , Flatten , Lambda , Dense from keras.models import Sequential , Model from keras.optimizers import Adam import rl.core from rl.agents.dqn import DQNAgent from rl.agents.cem import CEMAgent from rl.callbacks import Callback from rl.policy import BoltzmannQPolicy , EpsGreedyQPolicy , GreedyQPolicy from rl.memory import EpisodeParameterMemory , SequentialMemory import gym import gym.spaces # noinspection PyUnresolvedReferences,PyAbstractClass class KerasRlAgent ( bcore . BackendAgent , metaclass = ABCMeta ): \"\"\"Reinforcement learning agents based on keras-rl originally developed by matthias plappert https://github.com/keras-rl/keras-rl \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config , backend_name = KerasRlAgentFactory . backend_name , tensorflow_v2_eager = False ) self . _agent : Optional [ rl . core . Agent ] = None self . _play_env : Optional [ gym . Env ] = None def _create_env ( self ) -> gym . Env : \"\"\"Creates a new gym instance.\"\"\" self . log_api ( f 'gym.make' , f '(\"{self.model_config.original_env_name}\")' ) result = gym . make ( self . model_config . gym_env_name ) return result def _create_model ( self , gym_env : gym . Env , activation : str ) -> Sequential : \"\"\"Creates a model consisting of fully connected layers as given by self.model_config.fc_layers with relu as activation function. Args: gym_env: gym_env whose observation shape ofdefines the size of the input layer and whose action_space defines the size of the output layer. activation: output activation function eg 'linear' or 'softmax' Returns: Keras Sequential model according to model_config.fc_layers \"\"\" assert gym_env num_actions = gym_env . action_space . n self . log_api ( f 'Sequential' , f '()' ) result = Sequential () input_shape = ( 1 ,) + gym_env . observation_space . shape self . log_api ( f 'model.add' , f '(Flatten(input_shape={input_shape}))' ) result . add ( Flatten ( input_shape = input_shape )) for layer_size in self . model_config . fc_layers : self . log_api ( f 'model.add' , f '(Dense({layer_size}))' ) result . add ( Dense ( layer_size )) self . log_api ( f 'model.add' , f '(Activation(\"relu\"))' ) result . add ( Activation ( 'relu' )) self . log_api ( f 'model.add' , f '(Dense({num_actions}))' ) result . add ( Dense ( num_actions )) self . log_api ( f 'model.add' , f '(Activation(\"{activation}\"))' ) result . add ( Activation ( activation )) return result def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _agent , \"_agent not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break class KerasRlCemAgent ( KerasRlAgent ): \"\"\"Keras-rl implementation of the cross-entropy method algorithm. see \"https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf\" and \"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf\" \"\"\" class CemCallback ( rl . callbacks . Callback ): \"\"\"Callback registered with keras rl agents to propagate iteration and episode updates.\"\"\" def __init__ ( self , cem_agent : KerasRlAgent , cem_context : core . CemTrainContext , nb_steps : int ): \"\"\" Args: cem_agent: the agent to propagate iteration begn/end events to. cem_context: the train_context containing the iteration definitions nb_steps: value set in the keras cem agent. \"\"\" assert cem_agent assert cem_context assert nb_steps self . _cem_agent : KerasRlAgent = cem_agent self . _cem_context : core . CemTrainContext = cem_context self . _nb_steps = nb_steps super () . __init__ () def on_episode_end ( self , episode , logs = None ): \"\"\"Signals the base class the end / begin of a training iteration.\"\"\" cc : core . CemTrainContext = self . _cem_context episode = episode + 1 if episode % cc . num_episodes_per_iteration == 0 : self . _cem_agent . on_train_iteration_end ( math . nan ) if self . _cem_context . training_done : self . _cem_agent . _agent . step = self . _nb_steps else : self . _cem_agent . on_train_iteration_begin () def train_implementation ( self , train_context : core . CemTrainContext ): assert train_context cc : core . CemTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = 'softmax' ) policy_buffer_size = 5 * cc . num_episodes_per_iteration self . log_api ( f 'EpisodeParameterMemory' , f '(limit={policy_buffer_size}, window_length=1)' ) memory = EpisodeParameterMemory ( limit = policy_buffer_size , window_length = 1 ) num_actions = train_env . action_space . n self . log_api ( f 'CEMAgent' , f '(model=..., nb_actions={num_actions}, memory=..., ' + \\ f 'nb_steps_warmup={cc.num_steps_buffer_preload}, ' + \\ f 'train_interval={cc.num_episodes_per_iteration}, ' + \\ f 'batch_size={cc.num_episodes_per_iteration}, ' + \\ f 'elite_frac={cc.elite_set_fraction})' ) self . _agent = CEMAgent ( model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = cc . num_steps_buffer_preload , batch_size = cc . num_episodes_per_iteration , train_interval = cc . num_episodes_per_iteration , elite_frac = cc . elite_set_fraction ) self . log_api ( f 'agent.compile' , '()' ) self . _agent . compile () nb_steps = cc . num_iterations * cc . num_episodes_per_iteration * cc . max_steps_per_episode callback = KerasRlCemAgent . CemCallback ( self , cc , nb_steps ) self . on_train_iteration_begin () self . log_api ( f 'agent.fit' , f '(train_env, nb_steps={nb_steps})' ) self . _agent . fit ( train_env , nb_steps = nb_steps , visualize = False , verbose = 0 , callbacks = [ callback ]) if not cc . training_done : self . on_train_iteration_end ( math . nan ) class KerasRlDqnAgent ( KerasRlAgent ): \"\"\"Keras-rl implementation of the algorithm described in in Mnih (2013) and Mnih (2015). http://arxiv.org/pdf/1312.5602.pdf and http://arxiv.org/abs/1509.06461 includes implementations for the double dqn and dueling dqn variations. \"\"\" class DQNAgentWrapper ( DQNAgent ): \"\"\"Override of the KerasRl DqnAgennt instantiation due to a conflict with tensorflow 1.15. Essentially a copy of https://raw.githubusercontent.com/keras-rl/keras-rl/master/rl/agents/dqn.py \"\"\" def __init__ ( self , model , policy = None , test_policy = None , enable_double_dqn = False , enable_dueling_network = False , dueling_type = 'avg' , * args , ** kwargs ): super ( DQNAgent , self ) . __init__ ( * args , ** kwargs ) if model . output . _keras_shape != ( None , self . nb_actions ): raise ValueError ( f 'Model output \"{model.output}\" has invalid shape. Dqn expects ' + f 'a model that has one dimension for each action, in this case {self.nb_actions}.' ) self . enable_double_dqn = enable_double_dqn self . enable_dueling_network = enable_dueling_network self . dueling_type = dueling_type if self . enable_dueling_network : layer = model . layers [ - 2 ] nb_action = model . output . _keras_shape [ - 1 ] y = Dense ( nb_action + 1 , activation = 'linear' )( layer . output ) if self . dueling_type == 'avg' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :] - K . mean ( a [:, 1 :], keepdims = True ), output_shape = ( nb_action ,))( y ) elif self . dueling_type == 'max' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :] - K . max ( a [:, 1 :], keepdims = True ), output_shape = ( nb_action ,))( y ) elif self . dueling_type == 'naive' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :], output_shape = ( nb_action ,))( y ) else : assert False , \"dueling_type must be one of {'avg','max','naive'}\" model = Model ( inputs = model . input , outputs = outputlayer ) self . model = model if policy is None : policy = EpsGreedyQPolicy () if test_policy is None : test_policy = GreedyQPolicy () self . policy = policy self . test_policy = test_policy self . reset_states () class DqnCallback ( rl . callbacks . Callback ): \"\"\"Callback registered with keras rl agents to propagate iteration and episode updates.\"\"\" def __init__ ( self , agent : bcore . BackendAgent , dqn_context : core . StepsTrainContext , loss_metric_idx : Optional [ int ]): \"\"\" Args: agent: the agent to propagate iteration begn/end events to. dqn_context: the train_context containing the iteration definitions loss_metric_idx: the index of the loss in the metrics list, or None \"\"\" assert agent assert dqn_context self . _agent = agent self . _dqn_context = dqn_context self . _loss_metric_idx = loss_metric_idx super () . __init__ () def on_step_end ( self , step , logs = None ): \"\"\"Signals the base class the end / begin of a training iteration.\"\"\" steps_done = self . _dqn_context . steps_done_in_training - self . _dqn_context . num_steps_buffer_preload if steps_done > 0 and steps_done % self . _dqn_context . num_steps_per_iteration == 0 : loss = math . nan if logs and 'metrics' in logs and ( self . _loss_metric_idx is not None ): metrics = logs [ 'metrics' ] if len ( metrics ) > self . _loss_metric_idx : loss = metrics [ self . _loss_metric_idx ] self . _agent . on_train_iteration_end ( loss ) if not self . _dqn_context . training_done : self . _agent . on_train_iteration_begin () def __init__ ( self , model_config : core . ModelConfig , enable_dueling_dqn : bool = False , enable_double_dqn = False ): \"\"\" creates a new agent based on the DQN algorithm using the keras-rl implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: use the double dqn algorithm instead enable_dueling_dqn: use the dueling dqn algorithm instead \"\"\" super () . __init__ ( model_config = model_config ) self . _enable_double_dqn : bool = enable_double_dqn self . _enable_dueling_network : bool = enable_dueling_dqn def train_implementation ( self , train_context : core . StepsTrainContext ): assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = 'linear' ) self . log_api ( f 'SequentialMemory' , f '(limit={dc.max_steps_in_buffer}, window_length=1)' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f 'BoltzmannQPolicy' , f '()' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f 'DQNAgent' , f '(nb_actions={num_actions}, ' + f 'enable_double_dqn={self._enable_double_dqn}, ' + f 'enable_dueling_network={self._enable_dueling_network}, ' + f 'nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2,' + f 'gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f 'train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...)' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1e-2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f 'agent.compile' , f '(Adam(lr=1e-3), metrics=[\"mae\"]' ) self . _agent . compile ( Adam ( lr = 1e-3 ), metrics = [ 'mae' ]) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if 'loss' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \"loss\" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f 'agent.fit' , f '(train_env, nb_steps={num_steps})' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ]) if not dc . training_done : self . on_train_iteration_end ( math . nan ) class KerasRlDoubleDqnAgent ( KerasRlDqnAgent ): \"\"\"Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1509.06461 \"\"\" def __init__ ( self , model_config : core . ModelConfig ): \"\"\"Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" super () . __init__ ( model_config = model_config , enable_double_dqn = True ) class KerasRlDuelingDqnAgent ( KerasRlDqnAgent ): \"\"\"Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1511.06581 \"\"\" def __init__ ( self , model_config : core . ModelConfig ): \"\"\" creates a new agent based on the DQN algorithm using the keras-rl implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: \"\"\" super () . __init__ ( model_config = model_config , enable_dueling_dqn = True ) class CemKerasRlAgent ( KerasRlAgent ): \"\"\" creates a new agent based on the CEM algorithm using the keras-rl implementation. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) def train_implementation ( self , train_context : core . StepsTrainContext ): \"\"\" train_env = self._create_env() assert isinstance(train_env,gym.spaces.Discrete), \"Only discrete actions environment are supported.\" action_space : gym.spaces.Discrete = train_env.action_space memory = EpisodeParameterMemory(limit=train_context.max_steps_in_buffer, window_length=1) cem = CEMAgent(model=model, nb_actions=action_space.n, memory=memory, batch_size=train_context.num_steps_sampled_from_buffer, nb_steps_warmup=train_context.num_steps_buffer_preload, train_interval=50, elite_frac=0.05) cem.compile() \"\"\" class KerasRlAgentFactory ( bcore . BackendAgentFactory ): \"\"\"Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the keras-rl implementations. \"\"\" backend_name : str = 'kerasrl' tensorflow_v2_eager_compatible : bool = False def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . CemAgent : KerasRlCemAgent , easyagents . agents . DqnAgent : KerasRlDqnAgent , easyagents . agents . DoubleDqnAgent : KerasRlDoubleDqnAgent , easyagents . agents . DuelingDqnAgent : KerasRlDuelingDqnAgent , }","title":"Module easyagents.backends.kerasrl"},{"location":"reference/easyagents/backends/kerasrl/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/backends/kerasrl/#cemkerasrlagent","text":"class CemKerasRlAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the CEM algorithm using the keras-rl implementation. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class CemKerasRlAgent ( KerasRlAgent ): \"\"\" creates a new agent based on the CEM algorithm using the keras-rl implementation. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) def train_implementation ( self , train_context: core . StepsTrainContext ): \"\"\" train_env = self._create_env() assert isinstance(train_env,gym.spaces.Discrete), \" Only discrete actions environment are supported . \" action_space : gym.spaces.Discrete = train_env.action_space memory = EpisodeParameterMemory(limit=train_context.max_steps_in_buffer, window_length=1) cem = CEMAgent(model=model, nb_actions=action_space.n, memory=memory, batch_size=train_context.num_steps_sampled_from_buffer, nb_steps_warmup=train_context.num_steps_buffer_preload, train_interval=50, elite_frac=0.05) cem.compile() \"\"\"","title":"CemKerasRlAgent"},{"location":"reference/easyagents/backends/kerasrl/#ancestors-in-mro","text":"easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/kerasrl/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/backends/kerasrl/#log","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/kerasrl/#log_api","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_begin","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_end","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_end","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/kerasrl/#play","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/kerasrl/#play_implementation","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/kerasrl/#train","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/kerasrl/#train_implementation","text":"def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) train_env = self._create_env() assert isinstance(train_env,gym.spaces.Discrete), \"Only discrete actions environment are supported.\" action_space : gym.spaces.Discrete = train_env.action_space memory = EpisodeParameterMemory(limit=train_context.max_steps_in_buffer, window_length=1) cem = CEMAgent(model=model, nb_actions=action_space.n, memory=memory, batch_size=train_context.num_steps_sampled_from_buffer, nb_steps_warmup=train_context.num_steps_buffer_preload, train_interval=50, elite_frac=0.05) cem.compile() View Source def train_implementation ( self , train_context : core . StepsTrainContext ): \"\"\" train_env = self._create_env() assert isinstance(train_env,gym.spaces.Discrete), \" Only discrete actions environment are supported . \" action_space : gym.spaces.Discrete = train_env.action_space memory = EpisodeParameterMemory(limit=train_context.max_steps_in_buffer, window_length=1) cem = CEMAgent(model=model, nb_actions=action_space.n, memory=memory, batch_size=train_context.num_steps_sampled_from_buffer, nb_steps_warmup=train_context.num_steps_buffer_preload, train_interval=50, elite_frac=0.05) cem.compile() \"\"\"","title":"train_implementation"},{"location":"reference/easyagents/backends/kerasrl/#kerasrlagent","text":"class KerasRlAgent ( model_config : easyagents . core . ModelConfig ) Reinforcement learning agents based on keras-rl originally developed by matthias plappert https://github.com/keras-rl/keras-rl View Source class KerasRlAgent ( bcore . BackendAgent , metaclass = ABCMeta ) : \"\"\" Reinforcement learning agents based on keras-rl originally developed by matthias plappert https : // github . com / keras - rl / keras - rl \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config , backend_name = KerasRlAgentFactory . backend_name , tensorflow_v2_eager = False ) self . _agent : Optional [ rl . core . Agent ] = None self . _play_env : Optional [ gym . Env ] = None def _create_env ( self ) -> gym . Env : \"\"\" Creates a new gym instance. \"\"\" self . log_api ( f ' gym.make ' , f ' (\"{self.model_config.original_env_name}\") ' ) result = gym . make ( self . model_config . gym_env_name ) return result def _create_model ( self , gym_env : gym . Env , activation : str ) -> Sequential : \"\"\" Creates a model consisting of fully connected layers as given by self.model_config.fc_layers with relu as activation function . Args : gym_env : gym_env whose observation shape ofdefines the size of the input layer and whose action_space defines the size of the output layer . activation : output activation function eg ' linear ' or ' softmax ' Returns : Keras Sequential model according to model_config . fc_layers \"\"\" assert gym_env num_actions = gym_env . action_space . n self . log_api ( f ' Sequential ' , f ' () ' ) result = Sequential () input_shape = ( 1 , ) + gym_env . observation_space . shape self . log_api ( f ' model.add ' , f ' (Flatten(input_shape={input_shape})) ' ) result . add ( Flatten ( input_shape = input_shape )) for layer_size in self . model_config . fc_layers : self . log_api ( f ' model.add ' , f ' (Dense({layer_size})) ' ) result . add ( Dense ( layer_size )) self . log_api ( f ' model.add ' , f ' (Activation(\"relu\")) ' ) result . add ( Activation ( ' relu ' )) self . log_api ( f ' model.add ' , f ' (Dense({num_actions})) ' ) result . add ( Dense ( num_actions )) self . log_api ( f ' model.add ' , f ' (Activation(\"{activation}\")) ' ) result . add ( Activation ( activation )) return result def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break","title":"KerasRlAgent"},{"location":"reference/easyagents/backends/kerasrl/#ancestors-in-mro_1","text":"easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/kerasrl/#descendants","text":"easyagents.backends.kerasrl.KerasRlCemAgent easyagents.backends.kerasrl.KerasRlDqnAgent easyagents.backends.kerasrl.CemKerasRlAgent","title":"Descendants"},{"location":"reference/easyagents/backends/kerasrl/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/backends/kerasrl/#log_1","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/kerasrl/#log_api_1","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_end_1","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/kerasrl/#play_1","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/kerasrl/#play_implementation_1","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/kerasrl/#train_1","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/kerasrl/#train_implementation_1","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\"","title":"train_implementation"},{"location":"reference/easyagents/backends/kerasrl/#kerasrlagentfactory","text":"class KerasRlAgentFactory ( / , * args , ** kwargs ) Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the keras-rl implementations. View Source class KerasRlAgentFactory ( bcore . BackendAgentFactory ) : \"\"\" Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the keras - rl implementations . \"\"\" backend_name : str = ' kerasrl ' tensorflow_v2_eager_compatible : bool = False def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . CemAgent : KerasRlCemAgent , easyagents . agents . DqnAgent : KerasRlDqnAgent , easyagents . agents . DoubleDqnAgent : KerasRlDoubleDqnAgent , easyagents . agents . DuelingDqnAgent : KerasRlDuelingDqnAgent , }","title":"KerasRlAgentFactory"},{"location":"reference/easyagents/backends/kerasrl/#ancestors-in-mro_2","text":"easyagents.backends.core.BackendAgentFactory abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/kerasrl/#class-variables","text":"backend_name tensorflow_v2_eager_compatible","title":"Class variables"},{"location":"reference/easyagents/backends/kerasrl/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/backends/kerasrl/#create_agent","text":"def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result","title":"create_agent"},{"location":"reference/easyagents/backends/kerasrl/#get_algorithms","text":"def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . CemAgent : KerasRlCemAgent , easyagents . agents . DqnAgent : KerasRlDqnAgent , easyagents . agents . DoubleDqnAgent : KerasRlDoubleDqnAgent , easyagents . agents . DuelingDqnAgent : KerasRlDuelingDqnAgent , }","title":"get_algorithms"},{"location":"reference/easyagents/backends/kerasrl/#kerasrlcemagent","text":"class KerasRlCemAgent ( model_config : easyagents . core . ModelConfig ) Keras-rl implementation of the cross-entropy method algorithm. see \"https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf\" and \"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf\" View Source class KerasRlCemAgent ( KerasRlAgent ) : \"\"\" Keras-rl implementation of the cross-entropy method algorithm. see \" https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf \" and \" https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf \" \"\"\" class CemCallback ( rl . callbacks . Callback ) : \"\"\" Callback registered with keras rl agents to propagate iteration and episode updates. \"\"\" def __init__ ( self , cem_agent : KerasRlAgent , cem_context : core . CemTrainContext , nb_steps : int ) : \"\"\" Args : cem_agent : the agent to propagate iteration begn / end events to . cem_context : the train_context containing the iteration definitions nb_steps : value set in the keras cem agent . \"\"\" assert cem_agent assert cem_context assert nb_steps self . _cem_agent : KerasRlAgent = cem_agent self . _cem_context : core . CemTrainContext = cem_context self . _nb_steps = nb_steps super () . __init__ () def on_episode_end ( self , episode , logs = None ) : \"\"\" Signals the base class the end / begin of a training iteration. \"\"\" cc : core . CemTrainContext = self . _cem_context episode = episode + 1 if episode % cc . num_episodes_per_iteration == 0 : self . _cem_agent . on_train_iteration_end ( math . nan ) if self . _cem_context . training_done : self . _cem_agent . _agent . step = self . _nb_steps else : self . _cem_agent . on_train_iteration_begin () def train_implementation ( self , train_context : core . CemTrainContext ) : assert train_context cc : core . CemTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' softmax ' ) policy_buffer_size = 5 * cc . num_episodes_per_iteration self . log_api ( f ' EpisodeParameterMemory ' , f ' (limit={policy_buffer_size}, window_length=1) ' ) memory = EpisodeParameterMemory ( limit = policy_buffer_size , window_length = 1 ) num_actions = train_env . action_space . n self . log_api ( f ' CEMAgent ' , f ' (model=..., nb_actions={num_actions}, memory=..., ' + \\ f ' nb_steps_warmup={cc.num_steps_buffer_preload}, ' + \\ f ' train_interval={cc.num_episodes_per_iteration}, ' + \\ f ' batch_size={cc.num_episodes_per_iteration}, ' + \\ f ' elite_frac={cc.elite_set_fraction}) ' ) self . _agent = CEMAgent ( model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = cc . num_steps_buffer_preload , batch_size = cc . num_episodes_per_iteration , train_interval = cc . num_episodes_per_iteration , elite_frac = cc . elite_set_fraction ) self . log_api ( f ' agent.compile ' , ' () ' ) self . _agent . compile () nb_steps = cc . num_iterations * cc . num_episodes_per_iteration * cc . max_steps_per_episode callback = KerasRlCemAgent . CemCallback ( self , cc , nb_steps ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={nb_steps}) ' ) self . _agent . fit ( train_env , nb_steps = nb_steps , visualize = False , verbose = 0 , callbacks = [ callback ] ) if not cc . training_done : self . on_train_iteration_end ( math . nan )","title":"KerasRlCemAgent"},{"location":"reference/easyagents/backends/kerasrl/#ancestors-in-mro_3","text":"easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/kerasrl/#class-variables_1","text":"CemCallback","title":"Class variables"},{"location":"reference/easyagents/backends/kerasrl/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/backends/kerasrl/#log_2","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/kerasrl/#log_api_2","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_begin_2","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_end_2","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_begin_2","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_end_2","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/kerasrl/#play_2","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/kerasrl/#play_implementation_2","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/kerasrl/#train_2","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/kerasrl/#train_implementation_2","text":"def train_implementation ( self , train_context : easyagents . core . CemTrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . CemTrainContext ) : assert train_context cc : core . CemTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' softmax ' ) policy_buffer_size = 5 * cc . num_episodes_per_iteration self . log_api ( f ' EpisodeParameterMemory ' , f ' (limit={policy_buffer_size}, window_length=1) ' ) memory = EpisodeParameterMemory ( limit = policy_buffer_size , window_length = 1 ) num_actions = train_env . action_space . n self . log_api ( f ' CEMAgent ' , f ' (model=..., nb_actions={num_actions}, memory=..., ' + \\ f ' nb_steps_warmup={cc.num_steps_buffer_preload}, ' + \\ f ' train_interval={cc.num_episodes_per_iteration}, ' + \\ f ' batch_size={cc.num_episodes_per_iteration}, ' + \\ f ' elite_frac={cc.elite_set_fraction}) ' ) self . _agent = CEMAgent ( model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = cc . num_steps_buffer_preload , batch_size = cc . num_episodes_per_iteration , train_interval = cc . num_episodes_per_iteration , elite_frac = cc . elite_set_fraction ) self . log_api ( f ' agent.compile ' , ' () ' ) self . _agent . compile () nb_steps = cc . num_iterations * cc . num_episodes_per_iteration * cc . max_steps_per_episode callback = KerasRlCemAgent . CemCallback ( self , cc , nb_steps ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={nb_steps}) ' ) self . _agent . fit ( train_env , nb_steps = nb_steps , visualize = False , verbose = 0 , callbacks = [ callback ] ) if not cc . training_done : self . on_train_iteration_end ( math . nan )","title":"train_implementation"},{"location":"reference/easyagents/backends/kerasrl/#kerasrldoubledqnagent","text":"class KerasRlDoubleDqnAgent ( model_config : easyagents . core . ModelConfig ) Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1509.06461 View Source class KerasRlDoubleDqnAgent ( KerasRlDqnAgent ): \"\"\"Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1509.06461 \"\"\" def __init__ ( self , model_config: core . ModelConfig ): \"\"\"Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" super (). __init__ ( model_config = model_config , enable_double_dqn = True )","title":"KerasRlDoubleDqnAgent"},{"location":"reference/easyagents/backends/kerasrl/#ancestors-in-mro_4","text":"easyagents.backends.kerasrl.KerasRlDqnAgent easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/kerasrl/#class-variables_2","text":"DQNAgentWrapper DqnCallback","title":"Class variables"},{"location":"reference/easyagents/backends/kerasrl/#methods_4","text":"","title":"Methods"},{"location":"reference/easyagents/backends/kerasrl/#log_3","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/kerasrl/#log_api_3","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_begin_3","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_end_3","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_begin_3","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_end_3","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/kerasrl/#play_3","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/kerasrl/#play_implementation_3","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/kerasrl/#train_3","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/kerasrl/#train_implementation_3","text":"def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . StepsTrainContext ) : assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' linear ' ) self . log_api ( f ' SequentialMemory ' , f ' (limit={dc.max_steps_in_buffer}, window_length=1) ' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f ' BoltzmannQPolicy ' , f ' () ' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f ' DQNAgent ' , f ' (nb_actions={num_actions}, ' + f ' enable_double_dqn={self._enable_double_dqn}, ' + f ' enable_dueling_network={self._enable_dueling_network}, ' + f ' nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2, ' + f ' gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f ' train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...) ' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1 e - 2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f ' agent.compile ' , f ' (Adam(lr=1e-3), metrics=[\"mae\"] ' ) self . _agent . compile ( Adam ( lr = 1 e - 3 ) , metrics = [ ' mae ' ] ) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if ' loss ' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \" loss \" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={num_steps}) ' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ] ) if not dc . training_done : self . on_train_iteration_end ( math . nan )","title":"train_implementation"},{"location":"reference/easyagents/backends/kerasrl/#kerasrldqnagent","text":"class KerasRlDqnAgent ( model_config : easyagents . core . ModelConfig , enable_dueling_dqn : bool = False , enable_double_dqn = False ) Keras-rl implementation of the algorithm described in in Mnih (2013) and Mnih (2015). http://arxiv.org/pdf/1312.5602.pdf and http://arxiv.org/abs/1509.06461 includes implementations for the double dqn and dueling dqn variations. View Source class KerasRlDqnAgent ( KerasRlAgent ) : \"\"\" Keras-rl implementation of the algorithm described in in Mnih (2013) and Mnih (2015). http : // arxiv . org / pdf / 1312 . 5602 . pdf and http : // arxiv . org / abs / 1509 . 06461 includes implementations for the double dqn and dueling dqn variations . \"\"\" class DQNAgentWrapper ( DQNAgent ) : \"\"\" Override of the KerasRl DqnAgennt instantiation due to a conflict with tensorflow 1.15. Essentially a copy of https : // raw . githubusercontent . com / keras - rl / keras - rl / master / rl / agents / dqn . py \"\"\" def __init__ ( self , model , policy = None , test_policy = None , enable_double_dqn = False , enable_dueling_network = False , dueling_type = ' avg ' , * args , ** kwargs ) : super ( DQNAgent , self ) . __init__ ( * args , ** kwargs ) if model . output . _keras_shape != ( None , self . nb_actions ) : raise ValueError ( f ' Model output \"{model.output}\" has invalid shape. Dqn expects ' + f ' a model that has one dimension for each action, in this case {self.nb_actions}. ' ) self . enable_double_dqn = enable_double_dqn self . enable_dueling_network = enable_dueling_network self . dueling_type = dueling_type if self . enable_dueling_network : layer = model . layers [ - 2 ] nb_action = model . output . _keras_shape [ - 1 ] y = Dense ( nb_action + 1 , activation = ' linear ' )( layer . output ) if self . dueling_type == ' avg ' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :] - K . mean ( a [:, 1 :], keepdims = True ) , output_shape = ( nb_action , ))( y ) elif self . dueling_type == ' max ' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :] - K . max ( a [:, 1 :], keepdims = True ) , output_shape = ( nb_action , ))( y ) elif self . dueling_type == ' naive ' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [:, 0 ], - 1 ) + a [:, 1 :], output_shape = ( nb_action , ))( y ) else : assert False , \" dueling_type must be one of {'avg','max','naive'} \" model = Model ( inputs = model . input , outputs = outputlayer ) self . model = model if policy is None : policy = EpsGreedyQPolicy () if test_policy is None : test_policy = GreedyQPolicy () self . policy = policy self . test_policy = test_policy self . reset_states () class DqnCallback ( rl . callbacks . Callback ) : \"\"\" Callback registered with keras rl agents to propagate iteration and episode updates. \"\"\" def __init__ ( self , agent : bcore . BackendAgent , dqn_context : core . StepsTrainContext , loss_metric_idx : Optional [ int ] ) : \"\"\" Args : agent : the agent to propagate iteration begn / end events to . dqn_context : the train_context containing the iteration definitions loss_metric_idx : the index of the loss in the metrics list , or None \"\"\" assert agent assert dqn_context self . _agent = agent self . _dqn_context = dqn_context self . _loss_metric_idx = loss_metric_idx super () . __init__ () def on_step_end ( self , step , logs = None ) : \"\"\" Signals the base class the end / begin of a training iteration. \"\"\" steps_done = self . _dqn_context . steps_done_in_training - self . _dqn_context . num_steps_buffer_preload if steps_done > 0 and steps_done % self . _dqn_context . num_steps_per_iteration == 0 : loss = math . nan if logs and ' metrics ' in logs and ( self . _loss_metric_idx is not None ) : metrics = logs [ ' metrics ' ] if len ( metrics ) > self . _loss_metric_idx : loss = metrics [ self . _loss_metric_idx ] self . _agent . on_train_iteration_end ( loss ) if not self . _dqn_context . training_done : self . _agent . on_train_iteration_begin () def __init__ ( self , model_config : core . ModelConfig , enable_dueling_dqn : bool = False , enable_double_dqn = False ) : \"\"\" creates a new agent based on the DQN algorithm using the keras-rl implementation. Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . enable_double_dqn : use the double dqn algorithm instead enable_dueling_dqn : use the dueling dqn algorithm instead \"\"\" super () . __init__ ( model_config = model_config ) self . _enable_double_dqn : bool = enable_double_dqn self . _enable_dueling_network : bool = enable_dueling_dqn def train_implementation ( self , train_context : core . StepsTrainContext ) : assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' linear ' ) self . log_api ( f ' SequentialMemory ' , f ' (limit={dc.max_steps_in_buffer}, window_length=1) ' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f ' BoltzmannQPolicy ' , f ' () ' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f ' DQNAgent ' , f ' (nb_actions={num_actions}, ' + f ' enable_double_dqn={self._enable_double_dqn}, ' + f ' enable_dueling_network={self._enable_dueling_network}, ' + f ' nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2, ' + f ' gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f ' train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...) ' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1 e - 2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f ' agent.compile ' , f ' (Adam(lr=1e-3), metrics=[\"mae\"] ' ) self . _agent . compile ( Adam ( lr = 1 e - 3 ) , metrics = [ ' mae ' ] ) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if ' loss ' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \" loss \" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={num_steps}) ' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ] ) if not dc . training_done : self . on_train_iteration_end ( math . nan )","title":"KerasRlDqnAgent"},{"location":"reference/easyagents/backends/kerasrl/#ancestors-in-mro_5","text":"easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/kerasrl/#descendants_1","text":"easyagents.backends.kerasrl.KerasRlDoubleDqnAgent easyagents.backends.kerasrl.KerasRlDuelingDqnAgent","title":"Descendants"},{"location":"reference/easyagents/backends/kerasrl/#class-variables_3","text":"DQNAgentWrapper DqnCallback","title":"Class variables"},{"location":"reference/easyagents/backends/kerasrl/#methods_5","text":"","title":"Methods"},{"location":"reference/easyagents/backends/kerasrl/#log_4","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/kerasrl/#log_api_4","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_begin_4","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_end_4","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_begin_4","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_end_4","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/kerasrl/#play_4","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/kerasrl/#play_implementation_4","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/kerasrl/#train_4","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/kerasrl/#train_implementation_4","text":"def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . StepsTrainContext ) : assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' linear ' ) self . log_api ( f ' SequentialMemory ' , f ' (limit={dc.max_steps_in_buffer}, window_length=1) ' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f ' BoltzmannQPolicy ' , f ' () ' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f ' DQNAgent ' , f ' (nb_actions={num_actions}, ' + f ' enable_double_dqn={self._enable_double_dqn}, ' + f ' enable_dueling_network={self._enable_dueling_network}, ' + f ' nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2, ' + f ' gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f ' train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...) ' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1 e - 2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f ' agent.compile ' , f ' (Adam(lr=1e-3), metrics=[\"mae\"] ' ) self . _agent . compile ( Adam ( lr = 1 e - 3 ) , metrics = [ ' mae ' ] ) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if ' loss ' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \" loss \" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={num_steps}) ' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ] ) if not dc . training_done : self . on_train_iteration_end ( math . nan )","title":"train_implementation"},{"location":"reference/easyagents/backends/kerasrl/#kerasrlduelingdqnagent","text":"class KerasRlDuelingDqnAgent ( model_config : easyagents . core . ModelConfig ) Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1511.06581 View Source class KerasRlDuelingDqnAgent ( KerasRlDqnAgent ): \"\"\"Keras-rl implementation of the algorithm described in https://arxiv.org/abs/1511.06581 \"\"\" def __init__ ( self , model_config: core . ModelConfig ): \"\"\" creates a new agent based on the DQN algorithm using the keras-rl implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: \"\"\" super (). __init__ ( model_config = model_config , enable_dueling_dqn = True )","title":"KerasRlDuelingDqnAgent"},{"location":"reference/easyagents/backends/kerasrl/#ancestors-in-mro_6","text":"easyagents.backends.kerasrl.KerasRlDqnAgent easyagents.backends.kerasrl.KerasRlAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/kerasrl/#class-variables_4","text":"DQNAgentWrapper DqnCallback","title":"Class variables"},{"location":"reference/easyagents/backends/kerasrl/#methods_6","text":"","title":"Methods"},{"location":"reference/easyagents/backends/kerasrl/#log_5","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/kerasrl/#log_api_5","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_begin_5","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_play_episode_end_5","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_begin_5","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/kerasrl/#on_train_iteration_end_5","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/kerasrl/#play_5","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/kerasrl/#play_implementation_5","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" _agent not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : self . on_play_episode_begin ( env = self . _play_env ) observation = self . _play_env . reset () done = False while not done : action = self . _agent . forward ( observation ) observation , reward , done , info = self . _play_env . step ( action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/kerasrl/#train_5","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/kerasrl/#train_implementation_5","text":"def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . StepsTrainContext ) : assert train_context dc : core . StepsTrainContext = train_context train_env = self . _create_env () keras_model = self . _create_model ( gym_env = train_env , activation = ' linear ' ) self . log_api ( f ' SequentialMemory ' , f ' (limit={dc.max_steps_in_buffer}, window_length=1) ' ) memory = SequentialMemory ( limit = dc . max_steps_in_buffer , window_length = 1 ) self . log_api ( f ' BoltzmannQPolicy ' , f ' () ' ) policy = BoltzmannQPolicy () num_actions = train_env . action_space . n self . log_api ( f ' DQNAgent ' , f ' (nb_actions={num_actions}, ' + f ' enable_double_dqn={self._enable_double_dqn}, ' + f ' enable_dueling_network={self._enable_dueling_network}, ' + f ' nb_steps_warmup={dc.num_steps_buffer_preload}, target_model_update=1e-2, ' + f ' gamma={dc.reward_discount_gamma}, batch_size={dc.num_steps_sampled_from_buffer}, ' + f ' train_interval={dc.num_steps_per_iteration}, model=..., memory=..., policy=...) ' ) self . _agent = KerasRlDqnAgent . DQNAgentWrapper ( enable_double_dqn = self . _enable_double_dqn , enable_dueling_network = self . _enable_dueling_network , model = keras_model , nb_actions = num_actions , memory = memory , nb_steps_warmup = dc . num_steps_buffer_preload , target_model_update = 1 e - 2 , gamma = dc . reward_discount_gamma , batch_size = dc . num_steps_sampled_from_buffer , train_interval = dc . num_steps_per_iteration , policy = policy ) self . log_api ( f ' agent.compile ' , f ' (Adam(lr=1e-3), metrics=[\"mae\"] ' ) self . _agent . compile ( Adam ( lr = 1 e - 3 ) , metrics = [ ' mae ' ] ) num_steps = dc . num_iterations * dc . num_steps_per_iteration loss_metric_idx = None if ' loss ' in self . _agent . metrics_names : loss_metric_idx = self . _agent . metrics_names . index ( \" loss \" ) dqn_callback = KerasRlDqnAgent . DqnCallback ( self , dc , loss_metric_idx ) self . on_train_iteration_begin () self . log_api ( f ' agent.fit ' , f ' (train_env, nb_steps={num_steps}) ' ) self . _agent . fit ( train_env , nb_steps = num_steps , visualize = False , verbose = 0 , callbacks = [ dqn_callback ] ) if not dc . training_done : self . on_train_iteration_end ( math . nan )","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/","text":"Module easyagents.backends.tfagents This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents) View Source \"\"\"This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents)\"\"\" from abc import ABCMeta from typing import Dict , Type import math # noinspection PyUnresolvedReferences import easyagents.agents from easyagents import core from easyagents.backends import core as bcore from easyagents.backends import monitor # noinspection PyPackageRequirements import tensorflow as tf from tf_agents.agents.ddpg import critic_network from tf_agents.agents.dqn import dqn_agent from tf_agents.agents.ppo import ppo_agent from tf_agents.agents.reinforce import reinforce_agent from tf_agents.agents.sac import sac_agent from tf_agents.drivers import dynamic_step_driver from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver from tf_agents.environments import gym_wrapper , py_environment , tf_py_environment from tf_agents.networks import actor_distribution_network , normal_projection_network , q_network , value_network from tf_agents.policies import greedy_policy , tf_policy , random_tf_policy from tf_agents.replay_buffers import tf_uniform_replay_buffer from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer from tf_agents.trajectories import trajectory from tf_agents.utils import common import gym # noinspection PyUnresolvedReferences,PyAbstractClass class TfAgent ( bcore . BackendAgent , metaclass = ABCMeta ): \"\"\"Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config , backend_name = TfAgentAgentFactory . backend_name ) self . _trained_policy = None self . _play_env : Optional [ gym . Env ] = None def _create_gym_with_wrapper ( self , discount ): gym_spec = gym . spec ( self . model_config . gym_env_name ) gym_env = gym_spec . make () # simplify_box_bounds: Whether to replace bounds of Box space that are arrays # with identical values with one number and rely on broadcasting. # important, simplify_box_bounds True crashes environments with boundaries with identical values env = gym_wrapper . GymWrapper ( gym_env , discount = discount , simplify_box_bounds = False ) return env def _create_env ( self , discount : float = 1 ) -> tf_py_environment . TFPyEnvironment : \"\"\" creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment Args: discount: the reward discount factor \"\"\" assert 0 < discount <= 1 , \"discount not admissible\" self . log_api ( f 'TFPyEnvironment' , f '( suite_gym.load( ... ) )' ) # suit_gym.load crashes our environment # py_env = suite_gym.load(self.model_config.gym_env_name, discount=discount) py_env = self . _create_gym_with_wrapper ( discount ) result = tf_py_environment . TFPyEnvironment ( py_env ) return result def _get_gym_env ( self , tf_py_env : tf_py_environment . TFPyEnvironment ) -> monitor . _MonitorEnv : \"\"\" extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env\"\"\" assert isinstance ( tf_py_env , tf_py_environment . TFPyEnvironment ), \\ \"passed tf_py_env is not an instance of TFPyEnvironment\" assert isinstance ( tf_py_env . pyenv , py_environment . PyEnvironment ), \\ \"passed TFPyEnvironment.pyenv does not contain a PyEnvironment\" assert len ( tf_py_env . pyenv . envs ) == 1 , \"passed TFPyEnvironment.pyenv does not contain a unique environment\" result = tf_py_env . pyenv . envs [ 0 ] . gym assert isinstance ( result , monitor . _MonitorEnv ), \"passed TFPyEnvironment does not contain a _MonitorEnv\" return result def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break # noinspection PyUnresolvedReferences class TfDqnAgent ( TfAgent ): \"\"\" creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and DqnAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( 'QNetwork' , '()' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'DqnAgent' , '()' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( 'tf_agent.initialize' , f '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( 'RandomTFPolicy' , '()' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( 'replay_buffer.add_batch' , '(trajectory)' ) for _ in range ( dc . num_steps_buffer_preload ): self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log_api ( 'replay_buffer.as_dataset' , f '(num_parallel_calls=3, ' + f 'sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iter_dataset = iter ( dataset ) self . log_api ( 'for each iteration' ) self . log_api ( ' replay_buffer.add_batch' , '(trajectory)' ) self . log_api ( ' tf_agent.train' , '(experience=trajectory)' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ): self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return # noinspection PyUnresolvedReferences class TfPpoAgent ( TfAgent ): \"\"\" creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , '()' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ValueNetwork' , '()' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'PpoAgent' , '()' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( 'DynamicEpisodeDriver' , '()' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( '-----' , f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} -----' ) self . log_api ( 'collect_driver.run' , '()' ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , '()' ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , '(experience=...)' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}]' ) self . log_api ( 'replay_buffer.clear' , '()' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return # noinspection PyUnresolvedReferences class TfRandomAgent ( TfAgent ): \"\"\" creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) self . _set_trained_policy () def _set_trained_policy ( self ): \"\"\"Tf-Agents Random Implementation of the train loop.\"\"\" self . log ( 'Creating environment...' ) train_env = self . _create_env () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Data collection & Buffering self . log_api ( 'RandomTFPolicy' , 'create' ) self . _trained_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): self . log ( \"Training...\" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return # noinspection PyUnresolvedReferences class TfReinforceAgent ( TfAgent ): \"\"\" creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Reinforce Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( 'Creating environment...' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , 'create' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , 'create' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ReinforceAgent' , 'create' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( 'tf_agent.initialize()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , 'create' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicEpisodeDriver' , 'create' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( 'Starting training...' ) while True : self . on_train_iteration_begin () msg = f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4}' self . log_api ( 'collect_driver.run' , msg ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f}' ) self . log_api ( 'replay_buffer.clear' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return # noinspection PyUnresolvedReferences class TfSacAgent ( TfAgent ): \"\"\" creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'CriticNetwork' , f '(observation_spec, action_spec), observation_fc_layer_params=None, ' + f 'action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ), observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0.1 ): return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( 'ActorDistributionNetwork' , f 'observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f 'continuous_projection_net=...)' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self.log_api('tf.compat.v1.train.get_or_create_global_step','()') # global_step = tf.compat.v1.train.get_or_create_global_step() self . log_api ( 'SacAgent' , f '(timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f 'actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'gamma={tc.reward_discount_gamma})' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), # target_update_tau=0.005, # target_update_period=1, # td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error, gamma = tc . reward_discount_gamma ) # reward_scale_factor=1.0, # gradient_clipping=None, # train_step_counter=global_step) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=tf_agent.collect_data_spec, ' + f 'batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_buffer_preload})' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( 'initial_collect_driver.run()' ) initial_collect_driver . run () # Dataset generates trajectories with shape [Bx2x...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_per_iteration})' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # (Optional) Optimize by wrapping some of the code in a graph using TF function. tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( 'for each iteration' ) self . log_api ( ' collect_driver.run' , '()' ) self . log_api ( ' tf_agent.train' , '(experience=...)' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer. for _ in range ( tc . num_steps_per_iteration ): collect_driver . run () # Sample a batch of data from the buffer and update the agent's network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return class TfAgentAgentFactory ( bcore . BackendAgentFactory ): \"\"\"Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. \"\"\" backend_name : str = 'tfagents' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent } Classes TfAgent class TfAgent ( model_config : easyagents . core . ModelConfig ) Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents View Source class TfAgent ( bcore . BackendAgent , metaclass = ABCMeta ) : \"\"\" Reinforcement learning agents based on googles tf_agent implementations https : // github . com / tensorflow / agents \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config , backend_name = TfAgentAgentFactory . backend_name ) self . _trained_policy = None self . _play_env : Optional [ gym . Env ] = None def _create_gym_with_wrapper ( self , discount ) : gym_spec = gym . spec ( self . model_config . gym_env_name ) gym_env = gym_spec . make () # simplify_box_bounds : Whether to replace bounds of Box space that are arrays # with identical values with one number and rely on broadcasting . # important , simplify_box_bounds True crashes environments with boundaries with identical values env = gym_wrapper . GymWrapper ( gym_env , discount = discount , simplify_box_bounds = False ) return env def _create_env ( self , discount : float = 1 ) -> tf_py_environment . TFPyEnvironment : \"\"\" creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment Args : discount : the reward discount factor \"\"\" assert 0 < discount <= 1 , \" discount not admissible \" self . log_api ( f ' TFPyEnvironment ' , f ' ( suite_gym.load( ... ) ) ' ) # suit_gym . load crashes our environment # py_env = suite_gym . load ( self . model_config . gym_env_name , discount = discount ) py_env = self . _create_gym_with_wrapper ( discount ) result = tf_py_environment . TFPyEnvironment ( py_env ) return result def _get_gym_env ( self , tf_py_env : tf_py_environment . TFPyEnvironment ) -> monitor . _MonitorEnv : \"\"\" extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env \"\"\" assert isinstance ( tf_py_env , tf_py_environment . TFPyEnvironment ) , \\ \" passed tf_py_env is not an instance of TFPyEnvironment \" assert isinstance ( tf_py_env . pyenv , py_environment . PyEnvironment ) , \\ \" passed TFPyEnvironment.pyenv does not contain a PyEnvironment \" assert len ( tf_py_env . pyenv . envs ) == 1 , \" passed TFPyEnvironment.pyenv does not contain a unique environment \" result = tf_py_env . pyenv . envs [ 0 ]. gym assert isinstance ( result , monitor . _MonitorEnv ) , \" passed TFPyEnvironment does not contain a _MonitorEnv \" return result def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break Ancestors (in MRO) easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Descendants easyagents.backends.tfagents.TfDqnAgent easyagents.backends.tfagents.TfPpoAgent easyagents.backends.tfagents.TfRandomAgent easyagents.backends.tfagents.TfReinforceAgent easyagents.backends.tfagents.TfSacAgent Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\" TfAgentAgentFactory class TfAgentAgentFactory ( / , * args , ** kwargs ) Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. View Source class TfAgentAgentFactory ( bcore . BackendAgentFactory ) : \"\"\" Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations . \"\"\" backend_name : str = ' tfagents ' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent } Ancestors (in MRO) easyagents.backends.core.BackendAgentFactory abc.ABC Class variables backend_name tensorflow_v2_eager_compatible Methods create_agent def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result get_algorithms def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent } TfDqnAgent class TfDqnAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfDqnAgent ( TfAgent ) : \"\"\" creates a new agent based on the DQN algorithm using the tfagents implementation. Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ) : time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. The implementation follows https : // colab . research . google . com / github / tensorflow / agents / blob / master / tf_agents / colabs / 1 _dqn_tutorial . ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and DqnAgent self . log_api ( ' AdamOptimizer ' , ' () ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( ' QNetwork ' , ' () ' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' DqnAgent ' , ' () ' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( ' tf_agent.initialize ' , f ' () ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( ' TFUniformReplayBuffer ' , ' () ' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( ' RandomTFPolicy ' , ' () ' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( ' replay_buffer.add_batch ' , ' (trajectory) ' ) for _ in range ( dc . num_steps_buffer_preload ) : self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log_api ( ' replay_buffer.as_dataset ' , f ' (num_parallel_calls=3, ' + f ' sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3) ' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iter_dataset = iter ( dataset ) self . log_api ( ' for each iteration ' ) self . log_api ( ' replay_buffer.add_batch ' , ' (trajectory) ' ) self . log_api ( ' tf_agent.train ' , ' (experience=trajectory) ' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ) : self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods collect_step def collect_step ( self , env : tf_agents . environments . tf_py_environment . TFPyEnvironment , policy : tf_agents . policies . tf_policy . Base , replay_buffer : tf_agents . replay_buffers . tf_uniform_replay_buffer . TFUniformReplayBuffer ) View Source def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb View Source def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. The implementation follows https : // colab . research . google . com / github / tensorflow / agents / blob / master / tf_agents / colabs / 1 _dqn_tutorial . ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and DqnAgent self . log_api ( ' AdamOptimizer ' , ' () ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( ' QNetwork ' , ' () ' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' DqnAgent ' , ' () ' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( ' tf_agent.initialize ' , f ' () ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( ' TFUniformReplayBuffer ' , ' () ' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( ' RandomTFPolicy ' , ' () ' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( ' replay_buffer.add_batch ' , ' (trajectory) ' ) for _ in range ( dc . num_steps_buffer_preload ) : self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log_api ( ' replay_buffer.as_dataset ' , f ' (num_parallel_calls=3, ' + f ' sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3) ' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iter_dataset = iter ( dataset ) self . log_api ( ' for each iteration ' ) self . log_api ( ' replay_buffer.add_batch ' , ' (trajectory) ' ) self . log_api ( ' tf_agent.train ' , ' (experience=trajectory) ' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ) : self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return TfPpoAgent class TfPpoAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfPpoAgent ( TfAgent ) : \"\"\" creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor - critic algorithm using 2 neural networks . The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in ( the expected , discounted sum of future rewards when following the current actor network ) . Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( ' AdamOptimizer ' , ' () ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( ' ActorDistributionNetwork ' , ' () ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' ValueNetwork ' , ' () ' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' PpoAgent ' , ' () ' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( ' tf_agent.initialize ' , ' () ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( ' TFUniformReplayBuffer ' , ' () ' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( ' DynamicEpisodeDriver ' , ' () ' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( ' ----- ' , f ' iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} ----- ' ) self . log_api ( ' collect_driver.run ' , ' () ' ) collect_driver . run () self . log_api ( ' replay_buffer.gather_all ' , ' () ' ) trajectories = replay_buffer . gather_all () self . log_api ( ' tf_agent.train ' , ' (experience=...) ' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f ' loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}] ' ) self . log_api ( ' replay_buffer.clear ' , ' () ' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( ' AdamOptimizer ' , ' () ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( ' ActorDistributionNetwork ' , ' () ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' ValueNetwork ' , ' () ' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' PpoAgent ' , ' () ' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( ' tf_agent.initialize ' , ' () ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( ' TFUniformReplayBuffer ' , ' () ' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( ' DynamicEpisodeDriver ' , ' () ' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( ' ----- ' , f ' iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} ----- ' ) self . log_api ( ' collect_driver.run ' , ' () ' ) collect_driver . run () self . log_api ( ' replay_buffer.gather_all ' , ' () ' ) trajectories = replay_buffer . gather_all () self . log_api ( ' tf_agent.train ' , ' (experience=...) ' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f ' loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}] ' ) self . log_api ( ' replay_buffer.clear ' , ' () ' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return TfRandomAgent class TfRandomAgent ( model_config : easyagents . core . ModelConfig ) creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfRandomAgent ( TfAgent ) : \"\"\" creates a new random agent based on uniform random actions. Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) self . _set_trained_policy () def _set_trained_policy ( self ) : \"\"\" Tf-Agents Random Implementation of the train loop. \"\"\" self . log ( ' Creating environment... ' ) train_env = self . _create_env () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Data collection & Buffering self . log_api ( ' RandomTFPolicy ' , ' create ' ) self . _trained_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : self . log ( \" Training... \" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . TrainContext ) : self . log ( \" Training... \" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return TfReinforceAgent class TfReinforceAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfReinforceAgent ( TfAgent ) : \"\"\" creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions . Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Reinforce Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( ' Creating environment... ' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( ' AdamOptimizer ' , ' create ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( ' ActorDistributionNetwork ' , ' create ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' ReinforceAgent ' , ' create ' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( ' tf_agent.initialize() ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( ' TFUniformReplayBuffer ' , ' create ' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( ' DynamicEpisodeDriver ' , ' create ' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( ' Starting training... ' ) while True : self . on_train_iteration_begin () msg = f ' iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} ' self . log_api ( ' collect_driver.run ' , msg ) collect_driver . run () self . log_api ( ' replay_buffer.gather_all ' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( ' tf_agent.train ' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f ' loss={total_loss:<7.1f} ' ) self . log_api ( ' replay_buffer.clear ' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Reinforce Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Reinforce Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( ' Creating environment... ' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( ' AdamOptimizer ' , ' create ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( ' ActorDistributionNetwork ' , ' create ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' ReinforceAgent ' , ' create ' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( ' tf_agent.initialize() ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( ' TFUniformReplayBuffer ' , ' create ' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( ' DynamicEpisodeDriver ' , ' create ' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( ' Starting training... ' ) while True : self . on_train_iteration_begin () msg = f ' iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} ' self . log_api ( ' collect_driver.run ' , msg ) collect_driver . run () self . log_api ( ' replay_buffer.gather_all ' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( ' tf_agent.train ' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f ' loss={total_loss:<7.1f} ' ) self . log_api ( ' replay_buffer.clear ' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return TfSacAgent class TfSacAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfSacAgent ( TfAgent ) : \"\"\" creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https : // github . com / tensorflow / agents / blob / master / tf_agents / colabs / 7 _SAC_minitaur_tutorial . ipynb Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( ' CriticNetwork ' , f ' (observation_spec, action_spec), observation_fc_layer_params=None, ' + f ' action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers}) ' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ) , observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0 . 1 ) : return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( ' ActorDistributionNetwork ' , f ' observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f ' continuous_projection_net=...) ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self . log_api ( ' tf.compat.v1.train.get_or_create_global_step ' , ' () ' ) # global_step = tf . compat . v1 . train . get_or_create_global_step () self . log_api ( ' SacAgent ' , f ' (timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f ' actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' gamma={tc.reward_discount_gamma}) ' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , # target_update_tau = 0 . 005 , # target_update_period = 1 , # td_errors_loss_fn = tf . compat . v1 . losses . mean_squared_error , gamma = tc . reward_discount_gamma ) # reward_scale_factor = 1 . 0 , # gradient_clipping = None , # train_step_counter = global_step ) self . log_api ( ' tf_agent.initialize ' , ' () ' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( ' TFUniformReplayBuffer ' , f ' (data_spec=tf_agent.collect_data_spec, ' + f ' batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer}) ' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( ' DynamicStepDriver ' , f ' (env, collect_policy, observers=[replay_buffer.add_batch], ' + f ' num_steps={tc.num_steps_buffer_preload}) ' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( ' initial_collect_driver.run() ' ) initial_collect_driver . run () # Dataset generates trajectories with shape [ Bx2x ...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( ' DynamicStepDriver ' , f ' (env, collect_policy, observers=[replay_buffer.add_batch], ' + f ' num_steps={tc.num_steps_per_iteration}) ' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # ( Optional ) Optimize by wrapping some of the code in a graph using TF function . tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( ' for each iteration ' ) self . log_api ( ' collect_driver.run ' , ' () ' ) self . log_api ( ' tf_agent.train ' , ' (experience=...) ' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer . for _ in range ( tc . num_steps_per_iteration ) : collect_driver . run () # Sample a batch of data from the buffer and update the agent ' s network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( ' CriticNetwork ' , f ' (observation_spec, action_spec), observation_fc_layer_params=None, ' + f ' action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers}) ' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ) , observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0 . 1 ) : return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( ' ActorDistributionNetwork ' , f ' observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f ' continuous_projection_net=...) ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self . log_api ( ' tf.compat.v1.train.get_or_create_global_step ' , ' () ' ) # global_step = tf . compat . v1 . train . get_or_create_global_step () self . log_api ( ' SacAgent ' , f ' (timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f ' actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' gamma={tc.reward_discount_gamma}) ' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , # target_update_tau = 0 . 005 , # target_update_period = 1 , # td_errors_loss_fn = tf . compat . v1 . losses . mean_squared_error , gamma = tc . reward_discount_gamma ) # reward_scale_factor = 1 . 0 , # gradient_clipping = None , # train_step_counter = global_step ) self . log_api ( ' tf_agent.initialize ' , ' () ' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( ' TFUniformReplayBuffer ' , f ' (data_spec=tf_agent.collect_data_spec, ' + f ' batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer}) ' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( ' DynamicStepDriver ' , f ' (env, collect_policy, observers=[replay_buffer.add_batch], ' + f ' num_steps={tc.num_steps_buffer_preload}) ' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( ' initial_collect_driver.run() ' ) initial_collect_driver . run () # Dataset generates trajectories with shape [ Bx2x ...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( ' DynamicStepDriver ' , f ' (env, collect_policy, observers=[replay_buffer.add_batch], ' + f ' num_steps={tc.num_steps_per_iteration}) ' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # ( Optional ) Optimize by wrapping some of the code in a graph using TF function . tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( ' for each iteration ' ) self . log_api ( ' collect_driver.run ' , ' () ' ) self . log_api ( ' tf_agent.train ' , ' (experience=...) ' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer . for _ in range ( tc . num_steps_per_iteration ) : collect_driver . run () # Sample a batch of data from the buffer and update the agent ' s network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return","title":"Tfagents"},{"location":"reference/easyagents/backends/tfagents/#module-easyagentsbackendstfagents","text":"This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents) View Source \"\"\"This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents)\"\"\" from abc import ABCMeta from typing import Dict , Type import math # noinspection PyUnresolvedReferences import easyagents.agents from easyagents import core from easyagents.backends import core as bcore from easyagents.backends import monitor # noinspection PyPackageRequirements import tensorflow as tf from tf_agents.agents.ddpg import critic_network from tf_agents.agents.dqn import dqn_agent from tf_agents.agents.ppo import ppo_agent from tf_agents.agents.reinforce import reinforce_agent from tf_agents.agents.sac import sac_agent from tf_agents.drivers import dynamic_step_driver from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver from tf_agents.environments import gym_wrapper , py_environment , tf_py_environment from tf_agents.networks import actor_distribution_network , normal_projection_network , q_network , value_network from tf_agents.policies import greedy_policy , tf_policy , random_tf_policy from tf_agents.replay_buffers import tf_uniform_replay_buffer from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer from tf_agents.trajectories import trajectory from tf_agents.utils import common import gym # noinspection PyUnresolvedReferences,PyAbstractClass class TfAgent ( bcore . BackendAgent , metaclass = ABCMeta ): \"\"\"Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config , backend_name = TfAgentAgentFactory . backend_name ) self . _trained_policy = None self . _play_env : Optional [ gym . Env ] = None def _create_gym_with_wrapper ( self , discount ): gym_spec = gym . spec ( self . model_config . gym_env_name ) gym_env = gym_spec . make () # simplify_box_bounds: Whether to replace bounds of Box space that are arrays # with identical values with one number and rely on broadcasting. # important, simplify_box_bounds True crashes environments with boundaries with identical values env = gym_wrapper . GymWrapper ( gym_env , discount = discount , simplify_box_bounds = False ) return env def _create_env ( self , discount : float = 1 ) -> tf_py_environment . TFPyEnvironment : \"\"\" creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment Args: discount: the reward discount factor \"\"\" assert 0 < discount <= 1 , \"discount not admissible\" self . log_api ( f 'TFPyEnvironment' , f '( suite_gym.load( ... ) )' ) # suit_gym.load crashes our environment # py_env = suite_gym.load(self.model_config.gym_env_name, discount=discount) py_env = self . _create_gym_with_wrapper ( discount ) result = tf_py_environment . TFPyEnvironment ( py_env ) return result def _get_gym_env ( self , tf_py_env : tf_py_environment . TFPyEnvironment ) -> monitor . _MonitorEnv : \"\"\" extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env\"\"\" assert isinstance ( tf_py_env , tf_py_environment . TFPyEnvironment ), \\ \"passed tf_py_env is not an instance of TFPyEnvironment\" assert isinstance ( tf_py_env . pyenv , py_environment . PyEnvironment ), \\ \"passed TFPyEnvironment.pyenv does not contain a PyEnvironment\" assert len ( tf_py_env . pyenv . envs ) == 1 , \"passed TFPyEnvironment.pyenv does not contain a unique environment\" result = tf_py_env . pyenv . envs [ 0 ] . gym assert isinstance ( result , monitor . _MonitorEnv ), \"passed TFPyEnvironment does not contain a _MonitorEnv\" return result def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break # noinspection PyUnresolvedReferences class TfDqnAgent ( TfAgent ): \"\"\" creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and DqnAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( 'QNetwork' , '()' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'DqnAgent' , '()' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( 'tf_agent.initialize' , f '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( 'RandomTFPolicy' , '()' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( 'replay_buffer.add_batch' , '(trajectory)' ) for _ in range ( dc . num_steps_buffer_preload ): self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log_api ( 'replay_buffer.as_dataset' , f '(num_parallel_calls=3, ' + f 'sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iter_dataset = iter ( dataset ) self . log_api ( 'for each iteration' ) self . log_api ( ' replay_buffer.add_batch' , '(trajectory)' ) self . log_api ( ' tf_agent.train' , '(experience=trajectory)' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ): self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return # noinspection PyUnresolvedReferences class TfPpoAgent ( TfAgent ): \"\"\" creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , '()' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ValueNetwork' , '()' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'PpoAgent' , '()' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( 'DynamicEpisodeDriver' , '()' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( '-----' , f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} -----' ) self . log_api ( 'collect_driver.run' , '()' ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , '()' ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , '(experience=...)' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}]' ) self . log_api ( 'replay_buffer.clear' , '()' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return # noinspection PyUnresolvedReferences class TfRandomAgent ( TfAgent ): \"\"\" creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) self . _set_trained_policy () def _set_trained_policy ( self ): \"\"\"Tf-Agents Random Implementation of the train loop.\"\"\" self . log ( 'Creating environment...' ) train_env = self . _create_env () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Data collection & Buffering self . log_api ( 'RandomTFPolicy' , 'create' ) self . _trained_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): self . log ( \"Training...\" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return # noinspection PyUnresolvedReferences class TfReinforceAgent ( TfAgent ): \"\"\" creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Reinforce Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( 'Creating environment...' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , 'create' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , 'create' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ReinforceAgent' , 'create' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( 'tf_agent.initialize()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , 'create' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicEpisodeDriver' , 'create' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( 'Starting training...' ) while True : self . on_train_iteration_begin () msg = f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4}' self . log_api ( 'collect_driver.run' , msg ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f}' ) self . log_api ( 'replay_buffer.clear' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return # noinspection PyUnresolvedReferences class TfSacAgent ( TfAgent ): \"\"\" creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'CriticNetwork' , f '(observation_spec, action_spec), observation_fc_layer_params=None, ' + f 'action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ), observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0.1 ): return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( 'ActorDistributionNetwork' , f 'observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f 'continuous_projection_net=...)' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self.log_api('tf.compat.v1.train.get_or_create_global_step','()') # global_step = tf.compat.v1.train.get_or_create_global_step() self . log_api ( 'SacAgent' , f '(timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f 'actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'gamma={tc.reward_discount_gamma})' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), # target_update_tau=0.005, # target_update_period=1, # td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error, gamma = tc . reward_discount_gamma ) # reward_scale_factor=1.0, # gradient_clipping=None, # train_step_counter=global_step) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=tf_agent.collect_data_spec, ' + f 'batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_buffer_preload})' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( 'initial_collect_driver.run()' ) initial_collect_driver . run () # Dataset generates trajectories with shape [Bx2x...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_per_iteration})' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # (Optional) Optimize by wrapping some of the code in a graph using TF function. tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( 'for each iteration' ) self . log_api ( ' collect_driver.run' , '()' ) self . log_api ( ' tf_agent.train' , '(experience=...)' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer. for _ in range ( tc . num_steps_per_iteration ): collect_driver . run () # Sample a batch of data from the buffer and update the agent's network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return class TfAgentAgentFactory ( bcore . BackendAgentFactory ): \"\"\"Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. \"\"\" backend_name : str = 'tfagents' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent }","title":"Module easyagents.backends.tfagents"},{"location":"reference/easyagents/backends/tfagents/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/backends/tfagents/#tfagent","text":"class TfAgent ( model_config : easyagents . core . ModelConfig ) Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents View Source class TfAgent ( bcore . BackendAgent , metaclass = ABCMeta ) : \"\"\" Reinforcement learning agents based on googles tf_agent implementations https : // github . com / tensorflow / agents \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config , backend_name = TfAgentAgentFactory . backend_name ) self . _trained_policy = None self . _play_env : Optional [ gym . Env ] = None def _create_gym_with_wrapper ( self , discount ) : gym_spec = gym . spec ( self . model_config . gym_env_name ) gym_env = gym_spec . make () # simplify_box_bounds : Whether to replace bounds of Box space that are arrays # with identical values with one number and rely on broadcasting . # important , simplify_box_bounds True crashes environments with boundaries with identical values env = gym_wrapper . GymWrapper ( gym_env , discount = discount , simplify_box_bounds = False ) return env def _create_env ( self , discount : float = 1 ) -> tf_py_environment . TFPyEnvironment : \"\"\" creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment Args : discount : the reward discount factor \"\"\" assert 0 < discount <= 1 , \" discount not admissible \" self . log_api ( f ' TFPyEnvironment ' , f ' ( suite_gym.load( ... ) ) ' ) # suit_gym . load crashes our environment # py_env = suite_gym . load ( self . model_config . gym_env_name , discount = discount ) py_env = self . _create_gym_with_wrapper ( discount ) result = tf_py_environment . TFPyEnvironment ( py_env ) return result def _get_gym_env ( self , tf_py_env : tf_py_environment . TFPyEnvironment ) -> monitor . _MonitorEnv : \"\"\" extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env \"\"\" assert isinstance ( tf_py_env , tf_py_environment . TFPyEnvironment ) , \\ \" passed tf_py_env is not an instance of TFPyEnvironment \" assert isinstance ( tf_py_env . pyenv , py_environment . PyEnvironment ) , \\ \" passed TFPyEnvironment.pyenv does not contain a PyEnvironment \" assert len ( tf_py_env . pyenv . envs ) == 1 , \" passed TFPyEnvironment.pyenv does not contain a unique environment \" result = tf_py_env . pyenv . envs [ 0 ]. gym assert isinstance ( result , monitor . _MonitorEnv ) , \" passed TFPyEnvironment does not contain a _MonitorEnv \" return result def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"TfAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro","text":"easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#descendants","text":"easyagents.backends.tfagents.TfDqnAgent easyagents.backends.tfagents.TfPpoAgent easyagents.backends.tfagents.TfRandomAgent easyagents.backends.tfagents.TfReinforceAgent easyagents.backends.tfagents.TfSacAgent","title":"Descendants"},{"location":"reference/easyagents/backends/tfagents/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#log","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#train","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\"","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfagentagentfactory","text":"class TfAgentAgentFactory ( / , * args , ** kwargs ) Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. View Source class TfAgentAgentFactory ( bcore . BackendAgentFactory ) : \"\"\" Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations . \"\"\" backend_name : str = ' tfagents ' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent }","title":"TfAgentAgentFactory"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_1","text":"easyagents.backends.core.BackendAgentFactory abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#class-variables","text":"backend_name tensorflow_v2_eager_compatible","title":"Class variables"},{"location":"reference/easyagents/backends/tfagents/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#create_agent","text":"def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result","title":"create_agent"},{"location":"reference/easyagents/backends/tfagents/#get_algorithms","text":"def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent }","title":"get_algorithms"},{"location":"reference/easyagents/backends/tfagents/#tfdqnagent","text":"class TfDqnAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfDqnAgent ( TfAgent ) : \"\"\" creates a new agent based on the DQN algorithm using the tfagents implementation. Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ) : time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. The implementation follows https : // colab . research . google . com / github / tensorflow / agents / blob / master / tf_agents / colabs / 1 _dqn_tutorial . ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and DqnAgent self . log_api ( ' AdamOptimizer ' , ' () ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( ' QNetwork ' , ' () ' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' DqnAgent ' , ' () ' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( ' tf_agent.initialize ' , f ' () ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( ' TFUniformReplayBuffer ' , ' () ' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( ' RandomTFPolicy ' , ' () ' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( ' replay_buffer.add_batch ' , ' (trajectory) ' ) for _ in range ( dc . num_steps_buffer_preload ) : self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log_api ( ' replay_buffer.as_dataset ' , f ' (num_parallel_calls=3, ' + f ' sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3) ' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iter_dataset = iter ( dataset ) self . log_api ( ' for each iteration ' ) self . log_api ( ' replay_buffer.add_batch ' , ' (trajectory) ' ) self . log_api ( ' tf_agent.train ' , ' (experience=trajectory) ' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ) : self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return","title":"TfDqnAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_2","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#collect_step","text":"def collect_step ( self , env : tf_agents . environments . tf_py_environment . TFPyEnvironment , policy : tf_agents . policies . tf_policy . Base , replay_buffer : tf_agents . replay_buffers . tf_uniform_replay_buffer . TFUniformReplayBuffer ) View Source def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj )","title":"collect_step"},{"location":"reference/easyagents/backends/tfagents/#log_1","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_1","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_1","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_1","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_1","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_1","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_1","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb View Source def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. The implementation follows https : // colab . research . google . com / github / tensorflow / agents / blob / master / tf_agents / colabs / 1 _dqn_tutorial . ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and DqnAgent self . log_api ( ' AdamOptimizer ' , ' () ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( ' QNetwork ' , ' () ' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' DqnAgent ' , ' () ' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( ' tf_agent.initialize ' , f ' () ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( ' TFUniformReplayBuffer ' , ' () ' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( ' RandomTFPolicy ' , ' () ' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( ' replay_buffer.add_batch ' , ' (trajectory) ' ) for _ in range ( dc . num_steps_buffer_preload ) : self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log_api ( ' replay_buffer.as_dataset ' , f ' (num_parallel_calls=3, ' + f ' sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3) ' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iter_dataset = iter ( dataset ) self . log_api ( ' for each iteration ' ) self . log_api ( ' replay_buffer.add_batch ' , ' (trajectory) ' ) self . log_api ( ' tf_agent.train ' , ' (experience=trajectory) ' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ) : self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfppoagent","text":"class TfPpoAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfPpoAgent ( TfAgent ) : \"\"\" creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor - critic algorithm using 2 neural networks . The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in ( the expected , discounted sum of future rewards when following the current actor network ) . Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( ' AdamOptimizer ' , ' () ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( ' ActorDistributionNetwork ' , ' () ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' ValueNetwork ' , ' () ' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' PpoAgent ' , ' () ' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( ' tf_agent.initialize ' , ' () ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( ' TFUniformReplayBuffer ' , ' () ' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( ' DynamicEpisodeDriver ' , ' () ' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( ' ----- ' , f ' iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} ----- ' ) self . log_api ( ' collect_driver.run ' , ' () ' ) collect_driver . run () self . log_api ( ' replay_buffer.gather_all ' , ' () ' ) trajectories = replay_buffer . gather_all () self . log_api ( ' tf_agent.train ' , ' (experience=...) ' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f ' loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}] ' ) self . log_api ( ' replay_buffer.clear ' , ' () ' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return","title":"TfPpoAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_3","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#log_2","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_2","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_2","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_2","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_2","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_2","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_2","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_2","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_2","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_2","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( ' AdamOptimizer ' , ' () ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( ' ActorDistributionNetwork ' , ' () ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' ValueNetwork ' , ' () ' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' PpoAgent ' , ' () ' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( ' tf_agent.initialize ' , ' () ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( ' TFUniformReplayBuffer ' , ' () ' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( ' DynamicEpisodeDriver ' , ' () ' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( ' ----- ' , f ' iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} ----- ' ) self . log_api ( ' collect_driver.run ' , ' () ' ) collect_driver . run () self . log_api ( ' replay_buffer.gather_all ' , ' () ' ) trajectories = replay_buffer . gather_all () self . log_api ( ' tf_agent.train ' , ' (experience=...) ' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f ' loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}] ' ) self . log_api ( ' replay_buffer.clear ' , ' () ' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfrandomagent","text":"class TfRandomAgent ( model_config : easyagents . core . ModelConfig ) creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfRandomAgent ( TfAgent ) : \"\"\" creates a new random agent based on uniform random actions. Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) self . _set_trained_policy () def _set_trained_policy ( self ) : \"\"\" Tf-Agents Random Implementation of the train loop. \"\"\" self . log ( ' Creating environment... ' ) train_env = self . _create_env () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Data collection & Buffering self . log_api ( ' RandomTFPolicy ' , ' create ' ) self . _trained_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : self . log ( \" Training... \" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return","title":"TfRandomAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_4","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_4","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#log_3","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_3","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_3","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_3","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_3","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_3","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_3","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_3","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_3","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_3","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . TrainContext ) : self . log ( \" Training... \" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfreinforceagent","text":"class TfReinforceAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfReinforceAgent ( TfAgent ) : \"\"\" creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions . Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Reinforce Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( ' Creating environment... ' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( ' AdamOptimizer ' , ' create ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( ' ActorDistributionNetwork ' , ' create ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' ReinforceAgent ' , ' create ' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( ' tf_agent.initialize() ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( ' TFUniformReplayBuffer ' , ' create ' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( ' DynamicEpisodeDriver ' , ' create ' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( ' Starting training... ' ) while True : self . on_train_iteration_begin () msg = f ' iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} ' self . log_api ( ' collect_driver.run ' , msg ) collect_driver . run () self . log_api ( ' replay_buffer.gather_all ' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( ' tf_agent.train ' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f ' loss={total_loss:<7.1f} ' ) self . log_api ( ' replay_buffer.clear ' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return","title":"TfReinforceAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_5","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_5","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#log_4","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_4","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_4","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_4","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_4","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_4","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_4","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_4","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_4","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_4","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Reinforce Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Reinforce Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( ' Creating environment... ' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( ' AdamOptimizer ' , ' create ' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( ' ActorDistributionNetwork ' , ' create ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( ' ReinforceAgent ' , ' create ' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( ' tf_agent.initialize() ' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( ' TFUniformReplayBuffer ' , ' create ' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( ' DynamicEpisodeDriver ' , ' create ' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( ' Starting training... ' ) while True : self . on_train_iteration_begin () msg = f ' iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} ' self . log_api ( ' collect_driver.run ' , msg ) collect_driver . run () self . log_api ( ' replay_buffer.gather_all ' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( ' tf_agent.train ' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f ' loss={total_loss:<7.1f} ' ) self . log_api ( ' replay_buffer.clear ' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfsacagent","text":"class TfSacAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfSacAgent ( TfAgent ) : \"\"\" creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https : // github . com / tensorflow / agents / blob / master / tf_agents / colabs / 7 _SAC_minitaur_tutorial . ipynb Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" def __init__ ( self , model_config : core . ModelConfig ) : super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( ' CriticNetwork ' , f ' (observation_spec, action_spec), observation_fc_layer_params=None, ' + f ' action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers}) ' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ) , observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0 . 1 ) : return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( ' ActorDistributionNetwork ' , f ' observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f ' continuous_projection_net=...) ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self . log_api ( ' tf.compat.v1.train.get_or_create_global_step ' , ' () ' ) # global_step = tf . compat . v1 . train . get_or_create_global_step () self . log_api ( ' SacAgent ' , f ' (timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f ' actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' gamma={tc.reward_discount_gamma}) ' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , # target_update_tau = 0 . 005 , # target_update_period = 1 , # td_errors_loss_fn = tf . compat . v1 . losses . mean_squared_error , gamma = tc . reward_discount_gamma ) # reward_scale_factor = 1 . 0 , # gradient_clipping = None , # train_step_counter = global_step ) self . log_api ( ' tf_agent.initialize ' , ' () ' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( ' TFUniformReplayBuffer ' , f ' (data_spec=tf_agent.collect_data_spec, ' + f ' batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer}) ' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( ' DynamicStepDriver ' , f ' (env, collect_policy, observers=[replay_buffer.add_batch], ' + f ' num_steps={tc.num_steps_buffer_preload}) ' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( ' initial_collect_driver.run() ' ) initial_collect_driver . run () # Dataset generates trajectories with shape [ Bx2x ...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( ' DynamicStepDriver ' , f ' (env, collect_policy, observers=[replay_buffer.add_batch], ' + f ' num_steps={tc.num_steps_per_iteration}) ' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # ( Optional ) Optimize by wrapping some of the code in a graph using TF function . tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( ' for each iteration ' ) self . log_api ( ' collect_driver.run ' , ' () ' ) self . log_api ( ' tf_agent.train ' , ' (experience=...) ' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer . for _ in range ( tc . num_steps_per_iteration ) : collect_driver . run () # Sample a batch of data from the buffer and update the agent ' s network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return","title":"TfSacAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_6","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_6","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#log_5","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_5","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_5","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_5","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_5","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_5","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_5","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_5","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episodes with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _trained_policy , \" trained_policy not set. call train() first. \" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last () : action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_5","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_5","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Tf-Agents Ppo Implementation of the train loop. \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( ' CriticNetwork ' , f ' (observation_spec, action_spec), observation_fc_layer_params=None, ' + f ' action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers}) ' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ) , observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0 . 1 ) : return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( ' ActorDistributionNetwork ' , f ' observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f ' continuous_projection_net=...) ' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self . log_api ( ' tf.compat.v1.train.get_or_create_global_step ' , ' () ' ) # global_step = tf . compat . v1 . train . get_or_create_global_step () self . log_api ( ' SacAgent ' , f ' (timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f ' actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f ' gamma={tc.reward_discount_gamma}) ' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) , # target_update_tau = 0 . 005 , # target_update_period = 1 , # td_errors_loss_fn = tf . compat . v1 . losses . mean_squared_error , gamma = tc . reward_discount_gamma ) # reward_scale_factor = 1 . 0 , # gradient_clipping = None , # train_step_counter = global_step ) self . log_api ( ' tf_agent.initialize ' , ' () ' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( ' TFUniformReplayBuffer ' , f ' (data_spec=tf_agent.collect_data_spec, ' + f ' batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer}) ' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( ' DynamicStepDriver ' , f ' (env, collect_policy, observers=[replay_buffer.add_batch], ' + f ' num_steps={tc.num_steps_buffer_preload}) ' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( ' initial_collect_driver.run() ' ) initial_collect_driver . run () # Dataset generates trajectories with shape [ Bx2x ...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( ' DynamicStepDriver ' , f ' (env, collect_policy, observers=[replay_buffer.add_batch], ' + f ' num_steps={tc.num_steps_per_iteration}) ' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # ( Optional ) Optimize by wrapping some of the code in a graph using TF function . tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( ' for each iteration ' ) self . log_api ( ' collect_driver.run ' , ' () ' ) self . log_api ( ' tf_agent.train ' , ' (experience=...) ' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer . for _ in range ( tc . num_steps_per_iteration ) : collect_driver . run () # Sample a batch of data from the buffer and update the agent ' s network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tforce/","text":"Module easyagents.backends.tforce This module contains the backend implementation for tensorforce see https://github.com/tensorforce/tensorforce View Source \"\"\"This module contains the backend implementation for tensorforce see https://github.com/tensorforce/tensorforce \"\"\" from abc import ABCMeta from typing import List , Dict , Optional , Type import math import os import shutil import tempfile import datetime import gym import easyagents.backends.core from tensorforce.agents import Agent from tensorforce.environments import Environment from tensorforce.execution import Runner class TforceAgent ( easyagents . backends . core . BackendAgent , metaclass = ABCMeta ): \"\"\" Base class for agents based on the tensorforce implementation.\"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig ): \"\"\" Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" super () . __init__ ( model_config = model_config , backend_name = TensorforceAgentFactory . backend_name ) self . _agent : Optional [ Agent ] = None self . _play_env : Optional [ Environment ] = None def _create_env ( self ) -> Environment : \"\"\"Creates a tensorforce Environment encapsulating the underlying gym environment given in self.model_config\"\"\" self . log_api ( 'Environment.create' , f '(environment=\"gym\", level={self.model_config.original_env_name})' ) result = Environment . create ( environment = 'gym' , level = self . model_config . gym_env_name ) return result def _create_network_specification ( self ): \"\"\"Creates a tensorforce network specification based on the layer specification given in self.model_config\"\"\" result : List [ Dict ] = [] layer_sizes = self . model_config . fc_layers for layer_size in layer_sizes : result . append ( dict ( type = 'dense' , size = layer_size , activation = 'relu' )) return result def _get_temp_path ( self ): result = os . path . join ( tempfile . gettempdir (), tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f '-{n.year % 100:2}{n.month:02}{n.day:02}-{n.hour:02}{n.minute:02}{n.second:02}-' + \\ f '{n.microsecond:06}' return result def _train_with_runner ( self , train_env : Environment , train_context : easyagents . core . TrainContext ): \"\"\"Trains the self._agent using a tensorforce runner. Args: train_env: the tensorforce environment to use for the training train_context: context containing the training parameters \"\"\" assert train_context assert train_env assert self . _agent def step_callback ( _ : Runner ) -> bool : result = not train_context . training_done if isinstance ( train_context , easyagents . core . StepsTrainContext ): dc : easyagents . core . StepsTrainContext = train_context while result and \\ dc . steps_done_in_training > dc . iterations_done_in_training * dc . num_steps_per_iteration : self . on_train_iteration_end ( loss = math . nan ) result = not train_context . training_done if result : self . on_train_iteration_begin () return result def eval_callback ( tforce_runner : Runner ) -> bool : if isinstance ( train_context , easyagents . core . StepsTrainContext ): result = step_callback ( tforce_runner ) else : self . on_train_iteration_end ( loss = math . nan , actor_loss = math . nan , critic_loss = math . nan ) result = not train_context . training_done if result : self . on_train_iteration_begin () return result # Initialize the runner self . log_api ( 'Runner.create' , \"(agent=..., environment=...)\" ) runner = Runner ( agent = self . _agent , environment = train_env ) # Start the runner self . log_api ( 'runner.run' , f '(num_episodes=None, ' + f 'max_episode_timesteps={train_context.max_steps_per_episode})' ) self . on_train_iteration_begin () runner . run ( num_episodes = None , max_episode_timesteps = train_context . max_steps_per_episode , use_tqdm = False , callback = step_callback , callback_timestep_frequency = 1 , evaluation_callback = eval_callback , evaluation_frequency = None , evaluation = False , num_evaluation_iterations = 0 ) if not train_context . training_done : self . on_train_iteration_end ( loss = math . nan , actor_loss = math . nan , critic_loss = math . nan ) runner . close () def play_implementation ( self , play_context : easyagents . core . PlayContext ): \"\"\"Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _agent , \"agent not set. call train first.\" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ): done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return class TforceDqnAgent ( TforceAgent ): \"\"\" Agent based on the DQN algorithm using the tensorforce implementation.\"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig , enable_dueling_dqn : bool = False ): \"\"\" Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" super () . __init__ ( model_config = model_config ) def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ): \"\"\"Tensorforce Dqn Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () agent_type = 'dqn' self . log_api ( 'Agent.create' , f '(agent=\"{agent_type}\", ' + f 'network={network}, ' + f 'memory={tc.max_steps_in_buffer}, ' + f 'start_updating={tc.num_steps_buffer_preload},' f 'learning_rate={tc.learning_rate}, ' + f 'batch_size={tc.num_steps_sampled_from_buffer}, ' + f 'update_frequeny={tc.num_steps_per_iteration}, ' + f 'discount={tc.reward_discount_gamma})' ) self . _agent = Agent . create ( agent = agent_type , environment = train_env , network = network , memory = tc . max_steps_in_buffer , start_updating = tc . num_steps_buffer_preload , learning_rate = tc . learning_rate , batch_size = tc . num_steps_sampled_from_buffer , update_frequency = tc . num_steps_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) class TforceDuelingDqnAgent ( TforceDqnAgent ): \"\"\" Agent based on the DQN algorithm using the tensorforce implementation.\"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig ): \"\"\" Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: \"\"\" super () . __init__ ( model_config = model_config , enable_dueling_dqn = True ) class TforcePpoAgent ( TforceAgent ): \"\"\" Agent based on the PPO algorithm using the tensorforce implementation.\"\"\" def train_implementation ( self , train_context : easyagents . core . PpoTrainContext ): \"\"\"Tensorforce Ppo Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( 'Agent.create' , f '(agent=\"ppo\", environment=..., ' + f 'network={network}' + f 'learning_rate={tc.learning_rate}, ' + f 'batch_size={tc.num_episodes_per_iteration}, ' + f 'optimization_steps={tc.num_epochs_per_iteration}, ' + f 'discount={tc.reward_discount_gamma})' ) self . _agent = Agent . create ( agent = 'ppo' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , optimization_steps = tc . num_epochs_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) class TforceRandomAgent ( TforceAgent ): \"\"\" Random agent using the tensorforce implementation.\"\"\" def train_implementation ( self , train_context : easyagents . core . TrainContext ): assert isinstance ( train_context , easyagents . core . TrainContext ) train_env = self . _create_env () self . log_api ( 'Agent.create' , f '(agent=\"random\", environment=...)' ) self . _agent = Agent . create ( agent = 'random' , environment = train_env ) if not self . _agent . model . is_initialized : self . _agent . initialize () while not train_context . training_done : self . on_train_iteration_begin () state = train_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = train_env . execute ( actions = action ) if isinstance ( terminal , bool ): done = terminal else : done = terminal > 0 self . on_train_iteration_end ( math . nan ) class TforceReinforceAgent ( TforceAgent ): \"\"\" Agent based on the REINFORCE algorithm using the tensorforce implementation.\"\"\" def train_implementation ( self , train_context : easyagents . core . EpisodesTrainContext ): \"\"\"Tensorforce REINFORCE Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py \"\"\" assert isinstance ( train_context , easyagents . core . EpisodesTrainContext ) tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( 'Agent.create' , f '(agent=\"vpg\", environment=..., ' + f 'network={network}' + f 'learning_rate={tc.learning_rate}, ' + f 'batch_size={tc.num_episodes_per_iteration}, ' + f 'discount={tc.reward_discount_gamma})' ) self . _agent = Agent . create ( agent = 'vpg' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) class TensorforceAgentFactory ( easyagents . backends . core . BackendAgentFactory ): \"\"\"Backend for Tensorforce. Serves as a factory to create algorithm specific wrappers for the tensorforce implementations. \"\"\" backend_name : str = 'tensorforce' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent : TforceDqnAgent , easyagents . agents . PpoAgent : TforcePpoAgent , easyagents . agents . RandomAgent : TforceRandomAgent , easyagents . agents . ReinforceAgent : TforceReinforceAgent } Classes TensorforceAgentFactory class TensorforceAgentFactory ( / , * args , ** kwargs ) Backend for Tensorforce. Serves as a factory to create algorithm specific wrappers for the tensorforce implementations. View Source class TensorforceAgentFactory ( easyagents . backends . core . BackendAgentFactory ) : \"\"\" Backend for Tensorforce. Serves as a factory to create algorithm specific wrappers for the tensorforce implementations . \"\"\" backend_name : str = ' tensorforce ' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . DqnAgent : TforceDqnAgent , easyagents . agents . PpoAgent : TforcePpoAgent , easyagents . agents . RandomAgent : TforceRandomAgent , easyagents . agents . ReinforceAgent : TforceReinforceAgent } Ancestors (in MRO) easyagents.backends.core.BackendAgentFactory abc.ABC Class variables backend_name tensorflow_v2_eager_compatible Methods create_agent def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result get_algorithms def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . DqnAgent : TforceDqnAgent , easyagents . agents . PpoAgent : TforcePpoAgent , easyagents . agents . RandomAgent : TforceRandomAgent , easyagents . agents . ReinforceAgent : TforceReinforceAgent } TforceAgent class TforceAgent ( model_config : easyagents . core . ModelConfig ) Base class for agents based on the tensorforce implementation. View Source class TforceAgent ( easyagents . backends . core . BackendAgent , metaclass = ABCMeta ) : \"\"\" Base class for agents based on the tensorforce implementation. \"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig ) : \"\"\" Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" super () . __init__ ( model_config = model_config , backend_name = TensorforceAgentFactory . backend_name ) self . _agent : Optional [ Agent ] = None self . _play_env : Optional [ Environment ] = None def _create_env ( self ) -> Environment : \"\"\" Creates a tensorforce Environment encapsulating the underlying gym environment given in self.model_config \"\"\" self . log_api ( ' Environment.create ' , f ' (environment=\"gym\", level={self.model_config.original_env_name}) ' ) result = Environment . create ( environment = ' gym ' , level = self . model_config . gym_env_name ) return result def _create_network_specification ( self ) : \"\"\" Creates a tensorforce network specification based on the layer specification given in self.model_config \"\"\" result : List [ Dict ] = [] layer_sizes = self . model_config . fc_layers for layer_size in layer_sizes : result . append ( dict ( type = ' dense ' , size = layer_size , activation = ' relu ' )) return result def _get_temp_path ( self ) : result = os . path . join ( tempfile . gettempdir () , tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f ' -{n.year % 100:2}{n.month:02}{n.day:02}-{n.hour:02}{n.minute:02}{n.second:02}- ' + \\ f ' {n.microsecond:06} ' return result def _train_with_runner ( self , train_env : Environment , train_context : easyagents . core . TrainContext ) : \"\"\" Trains the self._agent using a tensorforce runner. Args : train_env : the tensorforce environment to use for the training train_context : context containing the training parameters \"\"\" assert train_context assert train_env assert self . _agent def step_callback ( _ : Runner ) -> bool : result = not train_context . training_done if isinstance ( train_context , easyagents . core . StepsTrainContext ) : dc : easyagents . core . StepsTrainContext = train_context while result and \\ dc . steps_done_in_training > dc . iterations_done_in_training * dc . num_steps_per_iteration : self . on_train_iteration_end ( loss = math . nan ) result = not train_context . training_done if result : self . on_train_iteration_begin () return result def eval_callback ( tforce_runner : Runner ) -> bool : if isinstance ( train_context , easyagents . core . StepsTrainContext ) : result = step_callback ( tforce_runner ) else : self . on_train_iteration_end ( loss = math . nan , actor_loss = math . nan , critic_loss = math . nan ) result = not train_context . training_done if result : self . on_train_iteration_begin () return result # Initialize the runner self . log_api ( ' Runner.create ' , \" (agent=..., environment=...) \" ) runner = Runner ( agent = self . _agent , environment = train_env ) # Start the runner self . log_api ( ' runner.run ' , f ' (num_episodes=None, ' + f ' max_episode_timesteps={train_context.max_steps_per_episode}) ' ) self . on_train_iteration_begin () runner . run ( num_episodes = None , max_episode_timesteps = train_context . max_steps_per_episode , use_tqdm = False , callback = step_callback , callback_timestep_frequency = 1 , evaluation_callback = eval_callback , evaluation_frequency = None , evaluation = False , num_evaluation_iterations = 0 ) if not train_context . training_done : self . on_train_iteration_end ( loss = math . nan , actor_loss = math . nan , critic_loss = math . nan ) runner . close () def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return Ancestors (in MRO) easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Descendants easyagents.backends.tforce.TforceDqnAgent easyagents.backends.tforce.TforcePpoAgent easyagents.backends.tforce.TforceRandomAgent easyagents.backends.tforce.TforceReinforceAgent Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\" TforceDqnAgent class TforceDqnAgent ( model_config : easyagents . core . ModelConfig , enable_dueling_dqn : bool = False ) Agent based on the DQN algorithm using the tensorforce implementation. View Source class TforceDqnAgent ( TforceAgent ) : \"\"\" Agent based on the DQN algorithm using the tensorforce implementation. \"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig , enable_dueling_dqn : bool = False ) : \"\"\" Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" super () . __init__ ( model_config = model_config ) def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) : \"\"\" Tensorforce Dqn Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () agent_type = ' dqn ' self . log_api ( ' Agent.create ' , f ' (agent=\"{agent_type}\", ' + f ' network={network}, ' + f ' memory={tc.max_steps_in_buffer}, ' + f ' start_updating={tc.num_steps_buffer_preload}, ' f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_steps_sampled_from_buffer}, ' + f ' update_frequeny={tc.num_steps_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = agent_type , environment = train_env , network = network , memory = tc . max_steps_in_buffer , start_updating = tc . num_steps_buffer_preload , learning_rate = tc . learning_rate , batch_size = tc . num_steps_sampled_from_buffer , update_frequency = tc . num_steps_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) Ancestors (in MRO) easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Descendants easyagents.backends.tforce.TforceDuelingDqnAgent Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Tensorforce Dqn Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py View Source def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) : \"\"\" Tensorforce Dqn Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () agent_type = ' dqn ' self . log_api ( ' Agent.create ' , f ' (agent=\"{agent_type}\", ' + f ' network={network}, ' + f ' memory={tc.max_steps_in_buffer}, ' + f ' start_updating={tc.num_steps_buffer_preload}, ' f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_steps_sampled_from_buffer}, ' + f ' update_frequeny={tc.num_steps_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = agent_type , environment = train_env , network = network , memory = tc . max_steps_in_buffer , start_updating = tc . num_steps_buffer_preload , learning_rate = tc . learning_rate , batch_size = tc . num_steps_sampled_from_buffer , update_frequency = tc . num_steps_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) TforceDuelingDqnAgent class TforceDuelingDqnAgent ( model_config : easyagents . core . ModelConfig ) Agent based on the DQN algorithm using the tensorforce implementation. View Source class TforceDuelingDqnAgent ( TforceDqnAgent ): \"\"\" Agent based on the DQN algorithm using the tensorforce implementation.\"\"\" def __init__ ( self , model_config: easyagents . core . ModelConfig ): \"\"\" Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: \"\"\" super (). __init__ ( model_config = model_config , enable_dueling_dqn = True ) Ancestors (in MRO) easyagents.backends.tforce.TforceDqnAgent easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Tensorforce Dqn Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py View Source def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) : \"\"\" Tensorforce Dqn Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () agent_type = ' dqn ' self . log_api ( ' Agent.create ' , f ' (agent=\"{agent_type}\", ' + f ' network={network}, ' + f ' memory={tc.max_steps_in_buffer}, ' + f ' start_updating={tc.num_steps_buffer_preload}, ' f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_steps_sampled_from_buffer}, ' + f ' update_frequeny={tc.num_steps_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = agent_type , environment = train_env , network = network , memory = tc . max_steps_in_buffer , start_updating = tc . num_steps_buffer_preload , learning_rate = tc . learning_rate , batch_size = tc . num_steps_sampled_from_buffer , update_frequency = tc . num_steps_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) TforcePpoAgent class TforcePpoAgent ( model_config : easyagents . core . ModelConfig ) Agent based on the PPO algorithm using the tensorforce implementation. View Source class TforcePpoAgent ( TforceAgent ) : \"\"\" Agent based on the PPO algorithm using the tensorforce implementation. \"\"\" def train_implementation ( self , train_context : easyagents . core . PpoTrainContext ) : \"\"\" Tensorforce Ppo Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( ' Agent.create ' , f ' (agent=\"ppo\", environment=..., ' + f ' network={network} ' + f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_episodes_per_iteration}, ' + f ' optimization_steps={tc.num_epochs_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = ' ppo ' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , optimization_steps = tc . num_epochs_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) Ancestors (in MRO) easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . PpoTrainContext ) Tensorforce Ppo Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py View Source def train_implementation ( self , train_context : easyagents . core . PpoTrainContext ) : \"\"\" Tensorforce Ppo Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( ' Agent.create ' , f ' (agent=\"ppo\", environment=..., ' + f ' network={network} ' + f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_episodes_per_iteration}, ' + f ' optimization_steps={tc.num_epochs_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = ' ppo ' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , optimization_steps = tc . num_epochs_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) TforceRandomAgent class TforceRandomAgent ( model_config : easyagents . core . ModelConfig ) Random agent using the tensorforce implementation. View Source class TforceRandomAgent ( TforceAgent ) : \"\"\" Random agent using the tensorforce implementation. \"\"\" def train_implementation ( self , train_context : easyagents . core . TrainContext ) : assert isinstance ( train_context , easyagents . core . TrainContext ) train_env = self . _create_env () self . log_api ( ' Agent.create ' , f ' (agent=\"random\", environment=...) ' ) self . _agent = Agent . create ( agent = ' random ' , environment = train_env ) if not self . _agent . model . is_initialized : self . _agent . initialize () while not train_context . training_done : self . on_train_iteration_begin () state = train_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = train_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_train_iteration_end ( math . nan ) Ancestors (in MRO) easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : easyagents . core . TrainContext ) : assert isinstance ( train_context , easyagents . core . TrainContext ) train_env = self . _create_env () self . log_api ( ' Agent.create ' , f ' (agent=\"random\", environment=...) ' ) self . _agent = Agent . create ( agent = ' random ' , environment = train_env ) if not self . _agent . model . is_initialized : self . _agent . initialize () while not train_context . training_done : self . on_train_iteration_begin () state = train_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = train_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_train_iteration_end ( math . nan ) TforceReinforceAgent class TforceReinforceAgent ( model_config : easyagents . core . ModelConfig ) Agent based on the REINFORCE algorithm using the tensorforce implementation. View Source class TforceReinforceAgent ( TforceAgent ) : \"\"\" Agent based on the REINFORCE algorithm using the tensorforce implementation. \"\"\" def train_implementation ( self , train_context : easyagents . core . EpisodesTrainContext ) : \"\"\" Tensorforce REINFORCE Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" assert isinstance ( train_context , easyagents . core . EpisodesTrainContext ) tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( ' Agent.create ' , f ' (agent=\"vpg\", environment=..., ' + f ' network={network} ' + f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_episodes_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = ' vpg ' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) Ancestors (in MRO) easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . EpisodesTrainContext ) Tensorforce REINFORCE Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py View Source def train_implementation ( self , train_context : easyagents . core . EpisodesTrainContext ) : \"\"\" Tensorforce REINFORCE Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" assert isinstance ( train_context , easyagents . core . EpisodesTrainContext ) tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( ' Agent.create ' , f ' (agent=\"vpg\", environment=..., ' + f ' network={network} ' + f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_episodes_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = ' vpg ' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc )","title":"Tforce"},{"location":"reference/easyagents/backends/tforce/#module-easyagentsbackendstforce","text":"This module contains the backend implementation for tensorforce see https://github.com/tensorforce/tensorforce View Source \"\"\"This module contains the backend implementation for tensorforce see https://github.com/tensorforce/tensorforce \"\"\" from abc import ABCMeta from typing import List , Dict , Optional , Type import math import os import shutil import tempfile import datetime import gym import easyagents.backends.core from tensorforce.agents import Agent from tensorforce.environments import Environment from tensorforce.execution import Runner class TforceAgent ( easyagents . backends . core . BackendAgent , metaclass = ABCMeta ): \"\"\" Base class for agents based on the tensorforce implementation.\"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig ): \"\"\" Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" super () . __init__ ( model_config = model_config , backend_name = TensorforceAgentFactory . backend_name ) self . _agent : Optional [ Agent ] = None self . _play_env : Optional [ Environment ] = None def _create_env ( self ) -> Environment : \"\"\"Creates a tensorforce Environment encapsulating the underlying gym environment given in self.model_config\"\"\" self . log_api ( 'Environment.create' , f '(environment=\"gym\", level={self.model_config.original_env_name})' ) result = Environment . create ( environment = 'gym' , level = self . model_config . gym_env_name ) return result def _create_network_specification ( self ): \"\"\"Creates a tensorforce network specification based on the layer specification given in self.model_config\"\"\" result : List [ Dict ] = [] layer_sizes = self . model_config . fc_layers for layer_size in layer_sizes : result . append ( dict ( type = 'dense' , size = layer_size , activation = 'relu' )) return result def _get_temp_path ( self ): result = os . path . join ( tempfile . gettempdir (), tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f '-{n.year % 100:2}{n.month:02}{n.day:02}-{n.hour:02}{n.minute:02}{n.second:02}-' + \\ f '{n.microsecond:06}' return result def _train_with_runner ( self , train_env : Environment , train_context : easyagents . core . TrainContext ): \"\"\"Trains the self._agent using a tensorforce runner. Args: train_env: the tensorforce environment to use for the training train_context: context containing the training parameters \"\"\" assert train_context assert train_env assert self . _agent def step_callback ( _ : Runner ) -> bool : result = not train_context . training_done if isinstance ( train_context , easyagents . core . StepsTrainContext ): dc : easyagents . core . StepsTrainContext = train_context while result and \\ dc . steps_done_in_training > dc . iterations_done_in_training * dc . num_steps_per_iteration : self . on_train_iteration_end ( loss = math . nan ) result = not train_context . training_done if result : self . on_train_iteration_begin () return result def eval_callback ( tforce_runner : Runner ) -> bool : if isinstance ( train_context , easyagents . core . StepsTrainContext ): result = step_callback ( tforce_runner ) else : self . on_train_iteration_end ( loss = math . nan , actor_loss = math . nan , critic_loss = math . nan ) result = not train_context . training_done if result : self . on_train_iteration_begin () return result # Initialize the runner self . log_api ( 'Runner.create' , \"(agent=..., environment=...)\" ) runner = Runner ( agent = self . _agent , environment = train_env ) # Start the runner self . log_api ( 'runner.run' , f '(num_episodes=None, ' + f 'max_episode_timesteps={train_context.max_steps_per_episode})' ) self . on_train_iteration_begin () runner . run ( num_episodes = None , max_episode_timesteps = train_context . max_steps_per_episode , use_tqdm = False , callback = step_callback , callback_timestep_frequency = 1 , evaluation_callback = eval_callback , evaluation_frequency = None , evaluation = False , num_evaluation_iterations = 0 ) if not train_context . training_done : self . on_train_iteration_end ( loss = math . nan , actor_loss = math . nan , critic_loss = math . nan ) runner . close () def play_implementation ( self , play_context : easyagents . core . PlayContext ): \"\"\"Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _agent , \"agent not set. call train first.\" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ): done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return class TforceDqnAgent ( TforceAgent ): \"\"\" Agent based on the DQN algorithm using the tensorforce implementation.\"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig , enable_dueling_dqn : bool = False ): \"\"\" Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" super () . __init__ ( model_config = model_config ) def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ): \"\"\"Tensorforce Dqn Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () agent_type = 'dqn' self . log_api ( 'Agent.create' , f '(agent=\"{agent_type}\", ' + f 'network={network}, ' + f 'memory={tc.max_steps_in_buffer}, ' + f 'start_updating={tc.num_steps_buffer_preload},' f 'learning_rate={tc.learning_rate}, ' + f 'batch_size={tc.num_steps_sampled_from_buffer}, ' + f 'update_frequeny={tc.num_steps_per_iteration}, ' + f 'discount={tc.reward_discount_gamma})' ) self . _agent = Agent . create ( agent = agent_type , environment = train_env , network = network , memory = tc . max_steps_in_buffer , start_updating = tc . num_steps_buffer_preload , learning_rate = tc . learning_rate , batch_size = tc . num_steps_sampled_from_buffer , update_frequency = tc . num_steps_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) class TforceDuelingDqnAgent ( TforceDqnAgent ): \"\"\" Agent based on the DQN algorithm using the tensorforce implementation.\"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig ): \"\"\" Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: \"\"\" super () . __init__ ( model_config = model_config , enable_dueling_dqn = True ) class TforcePpoAgent ( TforceAgent ): \"\"\" Agent based on the PPO algorithm using the tensorforce implementation.\"\"\" def train_implementation ( self , train_context : easyagents . core . PpoTrainContext ): \"\"\"Tensorforce Ppo Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( 'Agent.create' , f '(agent=\"ppo\", environment=..., ' + f 'network={network}' + f 'learning_rate={tc.learning_rate}, ' + f 'batch_size={tc.num_episodes_per_iteration}, ' + f 'optimization_steps={tc.num_epochs_per_iteration}, ' + f 'discount={tc.reward_discount_gamma})' ) self . _agent = Agent . create ( agent = 'ppo' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , optimization_steps = tc . num_epochs_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) class TforceRandomAgent ( TforceAgent ): \"\"\" Random agent using the tensorforce implementation.\"\"\" def train_implementation ( self , train_context : easyagents . core . TrainContext ): assert isinstance ( train_context , easyagents . core . TrainContext ) train_env = self . _create_env () self . log_api ( 'Agent.create' , f '(agent=\"random\", environment=...)' ) self . _agent = Agent . create ( agent = 'random' , environment = train_env ) if not self . _agent . model . is_initialized : self . _agent . initialize () while not train_context . training_done : self . on_train_iteration_begin () state = train_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = train_env . execute ( actions = action ) if isinstance ( terminal , bool ): done = terminal else : done = terminal > 0 self . on_train_iteration_end ( math . nan ) class TforceReinforceAgent ( TforceAgent ): \"\"\" Agent based on the REINFORCE algorithm using the tensorforce implementation.\"\"\" def train_implementation ( self , train_context : easyagents . core . EpisodesTrainContext ): \"\"\"Tensorforce REINFORCE Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py \"\"\" assert isinstance ( train_context , easyagents . core . EpisodesTrainContext ) tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( 'Agent.create' , f '(agent=\"vpg\", environment=..., ' + f 'network={network}' + f 'learning_rate={tc.learning_rate}, ' + f 'batch_size={tc.num_episodes_per_iteration}, ' + f 'discount={tc.reward_discount_gamma})' ) self . _agent = Agent . create ( agent = 'vpg' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc ) class TensorforceAgentFactory ( easyagents . backends . core . BackendAgentFactory ): \"\"\"Backend for Tensorforce. Serves as a factory to create algorithm specific wrappers for the tensorforce implementations. \"\"\" backend_name : str = 'tensorforce' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent : TforceDqnAgent , easyagents . agents . PpoAgent : TforcePpoAgent , easyagents . agents . RandomAgent : TforceRandomAgent , easyagents . agents . ReinforceAgent : TforceReinforceAgent }","title":"Module easyagents.backends.tforce"},{"location":"reference/easyagents/backends/tforce/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/backends/tforce/#tensorforceagentfactory","text":"class TensorforceAgentFactory ( / , * args , ** kwargs ) Backend for Tensorforce. Serves as a factory to create algorithm specific wrappers for the tensorforce implementations. View Source class TensorforceAgentFactory ( easyagents . backends . core . BackendAgentFactory ) : \"\"\" Backend for Tensorforce. Serves as a factory to create algorithm specific wrappers for the tensorforce implementations . \"\"\" backend_name : str = ' tensorforce ' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . DqnAgent : TforceDqnAgent , easyagents . agents . PpoAgent : TforcePpoAgent , easyagents . agents . RandomAgent : TforceRandomAgent , easyagents . agents . ReinforceAgent : TforceReinforceAgent }","title":"TensorforceAgentFactory"},{"location":"reference/easyagents/backends/tforce/#ancestors-in-mro","text":"easyagents.backends.core.BackendAgentFactory abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tforce/#class-variables","text":"backend_name tensorflow_v2_eager_compatible","title":"Class variables"},{"location":"reference/easyagents/backends/tforce/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tforce/#create_agent","text":"def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\" Creates a backend agent instance implementing the algorithm given by agent_type. Args : easyagent_type : the EasyAgent derived type for which an implementing backend instance will be created model_config : the model_config passed to the constructor of the backend instance . Returns : instance of the agent or None if not implemented by this backend . \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result","title":"create_agent"},{"location":"reference/easyagents/backends/tforce/#get_algorithms","text":"def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\" Yields a mapping of EasyAgent types to the implementations provided by this backend. \"\"\" return { easyagents . agents . DqnAgent : TforceDqnAgent , easyagents . agents . PpoAgent : TforcePpoAgent , easyagents . agents . RandomAgent : TforceRandomAgent , easyagents . agents . ReinforceAgent : TforceReinforceAgent }","title":"get_algorithms"},{"location":"reference/easyagents/backends/tforce/#tforceagent","text":"class TforceAgent ( model_config : easyagents . core . ModelConfig ) Base class for agents based on the tensorforce implementation. View Source class TforceAgent ( easyagents . backends . core . BackendAgent , metaclass = ABCMeta ) : \"\"\" Base class for agents based on the tensorforce implementation. \"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig ) : \"\"\" Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" super () . __init__ ( model_config = model_config , backend_name = TensorforceAgentFactory . backend_name ) self . _agent : Optional [ Agent ] = None self . _play_env : Optional [ Environment ] = None def _create_env ( self ) -> Environment : \"\"\" Creates a tensorforce Environment encapsulating the underlying gym environment given in self.model_config \"\"\" self . log_api ( ' Environment.create ' , f ' (environment=\"gym\", level={self.model_config.original_env_name}) ' ) result = Environment . create ( environment = ' gym ' , level = self . model_config . gym_env_name ) return result def _create_network_specification ( self ) : \"\"\" Creates a tensorforce network specification based on the layer specification given in self.model_config \"\"\" result : List [ Dict ] = [] layer_sizes = self . model_config . fc_layers for layer_size in layer_sizes : result . append ( dict ( type = ' dense ' , size = layer_size , activation = ' relu ' )) return result def _get_temp_path ( self ) : result = os . path . join ( tempfile . gettempdir () , tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f ' -{n.year % 100:2}{n.month:02}{n.day:02}-{n.hour:02}{n.minute:02}{n.second:02}- ' + \\ f ' {n.microsecond:06} ' return result def _train_with_runner ( self , train_env : Environment , train_context : easyagents . core . TrainContext ) : \"\"\" Trains the self._agent using a tensorforce runner. Args : train_env : the tensorforce environment to use for the training train_context : context containing the training parameters \"\"\" assert train_context assert train_env assert self . _agent def step_callback ( _ : Runner ) -> bool : result = not train_context . training_done if isinstance ( train_context , easyagents . core . StepsTrainContext ) : dc : easyagents . core . StepsTrainContext = train_context while result and \\ dc . steps_done_in_training > dc . iterations_done_in_training * dc . num_steps_per_iteration : self . on_train_iteration_end ( loss = math . nan ) result = not train_context . training_done if result : self . on_train_iteration_begin () return result def eval_callback ( tforce_runner : Runner ) -> bool : if isinstance ( train_context , easyagents . core . StepsTrainContext ) : result = step_callback ( tforce_runner ) else : self . on_train_iteration_end ( loss = math . nan , actor_loss = math . nan , critic_loss = math . nan ) result = not train_context . training_done if result : self . on_train_iteration_begin () return result # Initialize the runner self . log_api ( ' Runner.create ' , \" (agent=..., environment=...) \" ) runner = Runner ( agent = self . _agent , environment = train_env ) # Start the runner self . log_api ( ' runner.run ' , f ' (num_episodes=None, ' + f ' max_episode_timesteps={train_context.max_steps_per_episode}) ' ) self . on_train_iteration_begin () runner . run ( num_episodes = None , max_episode_timesteps = train_context . max_steps_per_episode , use_tqdm = False , callback = step_callback , callback_timestep_frequency = 1 , evaluation_callback = eval_callback , evaluation_frequency = None , evaluation = False , num_evaluation_iterations = 0 ) if not train_context . training_done : self . on_train_iteration_end ( loss = math . nan , actor_loss = math . nan , critic_loss = math . nan ) runner . close () def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return","title":"TforceAgent"},{"location":"reference/easyagents/backends/tforce/#ancestors-in-mro_1","text":"easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tforce/#descendants","text":"easyagents.backends.tforce.TforceDqnAgent easyagents.backends.tforce.TforcePpoAgent easyagents.backends.tforce.TforceRandomAgent easyagents.backends.tforce.TforceReinforceAgent","title":"Descendants"},{"location":"reference/easyagents/backends/tforce/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tforce/#log","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tforce/#log_api","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_begin","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_end","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_end","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tforce/#play","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tforce/#play_implementation","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return","title":"play_implementation"},{"location":"reference/easyagents/backends/tforce/#train","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tforce/#train_implementation","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @ abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\" Agent specific implementation of the train loop. The implementation should have the form : while True : on_iteration_begin for e in num_episodes_per_iterations play episode and record steps ( while steps_in_episode < max_steps_per_episode and ) train policy for num_epochs_per_iteration epochs on_iteration_end ( loss ) if training_done break Args : train_context : context configuring the train loop Hints : o the subclasses training loss is passed through to BackendAgent by on_iteration_end . Thus the subclass must not add the experienced loss to the TrainContext . \"\"\"","title":"train_implementation"},{"location":"reference/easyagents/backends/tforce/#tforcedqnagent","text":"class TforceDqnAgent ( model_config : easyagents . core . ModelConfig , enable_dueling_dqn : bool = False ) Agent based on the DQN algorithm using the tensorforce implementation. View Source class TforceDqnAgent ( TforceAgent ) : \"\"\" Agent based on the DQN algorithm using the tensorforce implementation. \"\"\" def __init__ ( self , model_config : easyagents . core . ModelConfig , enable_dueling_dqn : bool = False ) : \"\"\" Args : model_config : the model configuration including the name of the target gym environment as well as the neural network architecture . \"\"\" super () . __init__ ( model_config = model_config ) def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) : \"\"\" Tensorforce Dqn Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () agent_type = ' dqn ' self . log_api ( ' Agent.create ' , f ' (agent=\"{agent_type}\", ' + f ' network={network}, ' + f ' memory={tc.max_steps_in_buffer}, ' + f ' start_updating={tc.num_steps_buffer_preload}, ' f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_steps_sampled_from_buffer}, ' + f ' update_frequeny={tc.num_steps_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = agent_type , environment = train_env , network = network , memory = tc . max_steps_in_buffer , start_updating = tc . num_steps_buffer_preload , learning_rate = tc . learning_rate , batch_size = tc . num_steps_sampled_from_buffer , update_frequency = tc . num_steps_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc )","title":"TforceDqnAgent"},{"location":"reference/easyagents/backends/tforce/#ancestors-in-mro_2","text":"easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tforce/#descendants_1","text":"easyagents.backends.tforce.TforceDuelingDqnAgent","title":"Descendants"},{"location":"reference/easyagents/backends/tforce/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tforce/#log_1","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tforce/#log_api_1","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_end_1","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tforce/#play_1","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tforce/#play_implementation_1","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return","title":"play_implementation"},{"location":"reference/easyagents/backends/tforce/#train_1","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tforce/#train_implementation_1","text":"def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Tensorforce Dqn Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py View Source def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) : \"\"\" Tensorforce Dqn Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () agent_type = ' dqn ' self . log_api ( ' Agent.create ' , f ' (agent=\"{agent_type}\", ' + f ' network={network}, ' + f ' memory={tc.max_steps_in_buffer}, ' + f ' start_updating={tc.num_steps_buffer_preload}, ' f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_steps_sampled_from_buffer}, ' + f ' update_frequeny={tc.num_steps_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = agent_type , environment = train_env , network = network , memory = tc . max_steps_in_buffer , start_updating = tc . num_steps_buffer_preload , learning_rate = tc . learning_rate , batch_size = tc . num_steps_sampled_from_buffer , update_frequency = tc . num_steps_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc )","title":"train_implementation"},{"location":"reference/easyagents/backends/tforce/#tforceduelingdqnagent","text":"class TforceDuelingDqnAgent ( model_config : easyagents . core . ModelConfig ) Agent based on the DQN algorithm using the tensorforce implementation. View Source class TforceDuelingDqnAgent ( TforceDqnAgent ): \"\"\" Agent based on the DQN algorithm using the tensorforce implementation.\"\"\" def __init__ ( self , model_config: easyagents . core . ModelConfig ): \"\"\" Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. enable_double_dqn: \"\"\" super (). __init__ ( model_config = model_config , enable_dueling_dqn = True )","title":"TforceDuelingDqnAgent"},{"location":"reference/easyagents/backends/tforce/#ancestors-in-mro_3","text":"easyagents.backends.tforce.TforceDqnAgent easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tforce/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tforce/#log_2","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tforce/#log_api_2","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_begin_2","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_end_2","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_begin_2","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_end_2","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tforce/#play_2","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tforce/#play_implementation_2","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return","title":"play_implementation"},{"location":"reference/easyagents/backends/tforce/#train_2","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tforce/#train_implementation_2","text":"def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) Tensorforce Dqn Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py View Source def train_implementation ( self , train_context : easyagents . core . StepsTrainContext ) : \"\"\" Tensorforce Dqn Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () agent_type = ' dqn ' self . log_api ( ' Agent.create ' , f ' (agent=\"{agent_type}\", ' + f ' network={network}, ' + f ' memory={tc.max_steps_in_buffer}, ' + f ' start_updating={tc.num_steps_buffer_preload}, ' f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_steps_sampled_from_buffer}, ' + f ' update_frequeny={tc.num_steps_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = agent_type , environment = train_env , network = network , memory = tc . max_steps_in_buffer , start_updating = tc . num_steps_buffer_preload , learning_rate = tc . learning_rate , batch_size = tc . num_steps_sampled_from_buffer , update_frequency = tc . num_steps_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc )","title":"train_implementation"},{"location":"reference/easyagents/backends/tforce/#tforceppoagent","text":"class TforcePpoAgent ( model_config : easyagents . core . ModelConfig ) Agent based on the PPO algorithm using the tensorforce implementation. View Source class TforcePpoAgent ( TforceAgent ) : \"\"\" Agent based on the PPO algorithm using the tensorforce implementation. \"\"\" def train_implementation ( self , train_context : easyagents . core . PpoTrainContext ) : \"\"\" Tensorforce Ppo Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( ' Agent.create ' , f ' (agent=\"ppo\", environment=..., ' + f ' network={network} ' + f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_episodes_per_iteration}, ' + f ' optimization_steps={tc.num_epochs_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = ' ppo ' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , optimization_steps = tc . num_epochs_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc )","title":"TforcePpoAgent"},{"location":"reference/easyagents/backends/tforce/#ancestors-in-mro_4","text":"easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tforce/#methods_4","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tforce/#log_3","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tforce/#log_api_3","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_begin_3","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_end_3","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_begin_3","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_end_3","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tforce/#play_3","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tforce/#play_implementation_3","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return","title":"play_implementation"},{"location":"reference/easyagents/backends/tforce/#train_3","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tforce/#train_implementation_3","text":"def train_implementation ( self , train_context : easyagents . core . PpoTrainContext ) Tensorforce Ppo Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py View Source def train_implementation ( self , train_context : easyagents . core . PpoTrainContext ) : \"\"\" Tensorforce Ppo Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( ' Agent.create ' , f ' (agent=\"ppo\", environment=..., ' + f ' network={network} ' + f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_episodes_per_iteration}, ' + f ' optimization_steps={tc.num_epochs_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = ' ppo ' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , optimization_steps = tc . num_epochs_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc )","title":"train_implementation"},{"location":"reference/easyagents/backends/tforce/#tforcerandomagent","text":"class TforceRandomAgent ( model_config : easyagents . core . ModelConfig ) Random agent using the tensorforce implementation. View Source class TforceRandomAgent ( TforceAgent ) : \"\"\" Random agent using the tensorforce implementation. \"\"\" def train_implementation ( self , train_context : easyagents . core . TrainContext ) : assert isinstance ( train_context , easyagents . core . TrainContext ) train_env = self . _create_env () self . log_api ( ' Agent.create ' , f ' (agent=\"random\", environment=...) ' ) self . _agent = Agent . create ( agent = ' random ' , environment = train_env ) if not self . _agent . model . is_initialized : self . _agent . initialize () while not train_context . training_done : self . on_train_iteration_begin () state = train_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = train_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_train_iteration_end ( math . nan )","title":"TforceRandomAgent"},{"location":"reference/easyagents/backends/tforce/#ancestors-in-mro_5","text":"easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tforce/#methods_5","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tforce/#log_4","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tforce/#log_api_4","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_begin_4","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_end_4","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_begin_4","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_end_4","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tforce/#play_4","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tforce/#play_implementation_4","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return","title":"play_implementation"},{"location":"reference/easyagents/backends/tforce/#train_4","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tforce/#train_implementation_4","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : easyagents . core . TrainContext ) : assert isinstance ( train_context , easyagents . core . TrainContext ) train_env = self . _create_env () self . log_api ( ' Agent.create ' , f ' (agent=\"random\", environment=...) ' ) self . _agent = Agent . create ( agent = ' random ' , environment = train_env ) if not self . _agent . model . is_initialized : self . _agent . initialize () while not train_context . training_done : self . on_train_iteration_begin () state = train_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = train_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_train_iteration_end ( math . nan )","title":"train_implementation"},{"location":"reference/easyagents/backends/tforce/#tforcereinforceagent","text":"class TforceReinforceAgent ( model_config : easyagents . core . ModelConfig ) Agent based on the REINFORCE algorithm using the tensorforce implementation. View Source class TforceReinforceAgent ( TforceAgent ) : \"\"\" Agent based on the REINFORCE algorithm using the tensorforce implementation. \"\"\" def train_implementation ( self , train_context : easyagents . core . EpisodesTrainContext ) : \"\"\" Tensorforce REINFORCE Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" assert isinstance ( train_context , easyagents . core . EpisodesTrainContext ) tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( ' Agent.create ' , f ' (agent=\"vpg\", environment=..., ' + f ' network={network} ' + f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_episodes_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = ' vpg ' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc )","title":"TforceReinforceAgent"},{"location":"reference/easyagents/backends/tforce/#ancestors-in-mro_6","text":"easyagents.backends.tforce.TforceAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tforce/#methods_6","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tforce/#log_5","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ) : \"\"\" Logs msg. \"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tforce/#log_api_5","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\" Logs a call to api_target with additional log_msg. \"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_begin_5","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ) : \"\"\" Must be called by play_implementation at the beginning of a new episode Args : env : the gym environment used to play the episode . \"\"\" assert env , \" env not set. \" assert isinstance ( env , gym . core . Env ) , \" env not an an instance of gym.Env. \" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tforce/#on_play_episode_end_5","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ) : \"\"\" Must be called by play_implementation at the end of an episode \"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_begin_5","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ) : \"\"\" Must be called by train_implementation at the begining of a new iteration \"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tforce/#on_train_iteration_end_5","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\" Must be called by train_implementation at the end of an iteration Evaluates the current policy . Use kwargs to set additional dict values in train context . Eg for an ActorCriticTrainContext the losses may be set like this : on_train_iteration ( loss = 123 , actor_loss = 456 , critic_loss = 789 ) Args : loss : loss after the training of the model in this iteration or math . nan if the loss is not available ** kwargs : if a keyword matches a dict property of the TrainContext instance , then the dict [ episodes_done_in_training ] is set to the arg . \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tforce/#play_5","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tforce/#play_implementation_5","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episode with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : easyagents . core . PlayContext ) : \"\"\" Agent specific implementation of playing a single episode with the current policy. Args : play_context : play configuration to be used \"\"\" assert play_context , \" play_context not set. \" assert self . _agent , \" agent not set. call train first. \" if self . _play_env is None : self . _play_env = self . _create_env () while True : # noinspection PyUnresolvedReferences gym_env : gym . Env = self . _play_env . environment self . on_play_episode_begin ( env = gym_env ) state = self . _play_env . reset () done = False while not done : action = self . _agent . act ( state , evaluation = True ) state , terminal , reward = self . _play_env . execute ( actions = action ) if isinstance ( terminal , bool ) : done = terminal else : done = terminal > 0 self . on_play_episode_end () if play_context . play_done : break return","title":"play_implementation"},{"location":"reference/easyagents/backends/tforce/#train_5","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tforce/#train_implementation_5","text":"def train_implementation ( self , train_context : easyagents . core . EpisodesTrainContext ) Tensorforce REINFORCE Implementation of the train loop. The implementation follows https://github.com/tensorforce/tensorforce/blob/master/examples/quickstart.py View Source def train_implementation ( self , train_context : easyagents . core . EpisodesTrainContext ) : \"\"\" Tensorforce REINFORCE Implementation of the train loop. The implementation follows https : // github . com / tensorforce / tensorforce / blob / master / examples / quickstart . py \"\"\" assert isinstance ( train_context , easyagents . core . EpisodesTrainContext ) tc = train_context train_env = self . _create_env () network = self . _create_network_specification () self . log_api ( ' Agent.create ' , f ' (agent=\"vpg\", environment=..., ' + f ' network={network} ' + f ' learning_rate={tc.learning_rate}, ' + f ' batch_size={tc.num_episodes_per_iteration}, ' + f ' discount={tc.reward_discount_gamma}) ' ) self . _agent = Agent . create ( agent = ' vpg ' , environment = train_env , network = network , learning_rate = tc . learning_rate , batch_size = tc . num_episodes_per_iteration , discount = tc . reward_discount_gamma , ) self . _train_with_runner ( train_env , tc )","title":"train_implementation"},{"location":"reference/easyagents/callbacks/duration/","text":"Module easyagents.callbacks.duration View Source import math import easyagents.core as core class Fast ( core . AgentCallback ): \"\"\"Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks.\"\"\" def __init__ ( self , num_iterations = None ): self . _num_iterations = num_iterations self . _num_episodes_per_iteration = 3 self . _max_steps_per_episode = 50 def on_play_begin ( self , agent_context : core . AgentContext ): agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ): agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration def on_train_begin ( self , agent_context : core . AgentContext ): tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ): dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode class _SingleEpisode ( Fast ): \"\"\"Train / Play only for 1 episode (no evaluation in training, max. 10 steps).\"\"\" def __init__ ( self ): super () . __init__ ( num_iterations = 1 ) self . _num_episodes_per_iteration = 1 self . _max_steps_per_episode = 10 def on_train_begin ( self , agent_context : core . AgentContext ): super () . on_train_begin ( agent_context ) tc = agent_context . train if isinstance ( tc , core . StepsTrainContext ): tc . num_iterations = self . _max_steps_per_episode tc . num_iterations_between_eval = 0 tc . num_episodes_per_eval = 0 class _SingleIteration ( Fast ): \"\"\"Train / play for a single iteration with 3 episodes, and evaluation of 2 episodes\"\"\" def __init__ ( self ): super () . __init__ ( num_iterations = 1 ) Classes Fast class Fast ( num_iterations = None ) Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks. View Source class Fast ( core . AgentCallback ) : \"\"\" Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks. \"\"\" def __init__ ( self , num_iterations = None ) : self . _num_iterations = num_iterations self . _num_episodes_per_iteration = 3 self . _max_steps_per_episode = 50 def on_play_begin ( self , agent_context : core . AgentContext ) : agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ) : agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration def on_train_begin ( self , agent_context : core . AgentContext ) : tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ) : ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ) : dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode Ancestors (in MRO) easyagents.core.AgentCallback abc.ABC Descendants easyagents.callbacks.duration._SingleEpisode easyagents.callbacks.duration._SingleIteration Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ) : agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ) : ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ) : dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"Duration"},{"location":"reference/easyagents/callbacks/duration/#module-easyagentscallbacksduration","text":"View Source import math import easyagents.core as core class Fast ( core . AgentCallback ): \"\"\"Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks.\"\"\" def __init__ ( self , num_iterations = None ): self . _num_iterations = num_iterations self . _num_episodes_per_iteration = 3 self . _max_steps_per_episode = 50 def on_play_begin ( self , agent_context : core . AgentContext ): agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ): agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration def on_train_begin ( self , agent_context : core . AgentContext ): tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ): dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode class _SingleEpisode ( Fast ): \"\"\"Train / Play only for 1 episode (no evaluation in training, max. 10 steps).\"\"\" def __init__ ( self ): super () . __init__ ( num_iterations = 1 ) self . _num_episodes_per_iteration = 1 self . _max_steps_per_episode = 10 def on_train_begin ( self , agent_context : core . AgentContext ): super () . on_train_begin ( agent_context ) tc = agent_context . train if isinstance ( tc , core . StepsTrainContext ): tc . num_iterations = self . _max_steps_per_episode tc . num_iterations_between_eval = 0 tc . num_episodes_per_eval = 0 class _SingleIteration ( Fast ): \"\"\"Train / play for a single iteration with 3 episodes, and evaluation of 2 episodes\"\"\" def __init__ ( self ): super () . __init__ ( num_iterations = 1 )","title":"Module easyagents.callbacks.duration"},{"location":"reference/easyagents/callbacks/duration/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/callbacks/duration/#fast","text":"class Fast ( num_iterations = None ) Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks. View Source class Fast ( core . AgentCallback ) : \"\"\" Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks. \"\"\" def __init__ ( self , num_iterations = None ) : self . _num_iterations = num_iterations self . _num_episodes_per_iteration = 3 self . _max_steps_per_episode = 50 def on_play_begin ( self , agent_context : core . AgentContext ) : agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ) : agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration def on_train_begin ( self , agent_context : core . AgentContext ) : tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ) : ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ) : dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode","title":"Fast"},{"location":"reference/easyagents/callbacks/duration/#ancestors-in-mro","text":"easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/duration/#descendants","text":"easyagents.callbacks.duration._SingleEpisode easyagents.callbacks.duration._SingleIteration","title":"Descendants"},{"location":"reference/easyagents/callbacks/duration/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/duration/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/duration/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/duration/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/duration/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/duration/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/duration/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/duration/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/duration/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/duration/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ) : agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/duration/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/duration/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/duration/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/duration/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/duration/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/duration/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ) : ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ) : dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/duration/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/duration/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/duration/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/log/","text":"Module easyagents.callbacks.log View Source from typing import Tuple import logging import math from easyagents import core class _LogCallbackBase ( core . AgentCallback ): \"\"\"Base class for Callback loggers\"\"\" def __init__ ( self , logger : logging . Logger = None , prefix : str = None ): \"\"\"Writes all calls to logger with the given prefix. Args: logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _logger = logger if self . _logger is None : self . _logger = logging . getLogger () self . _prefix = prefix if self . _prefix is None : self . _prefix = '' def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) class _Callbacks ( _LogCallbackBase ): \"\"\"Logs all AgentCallback calls to a Logger\"\"\" def __init__ ( self , logger : logging . Logger = None , prefix : str = None ): \"\"\"Writes all calls to a callback function to logger with the given prefix. Args: logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" super () . __init__ ( logger = logger , prefix = prefix ) def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): msg = f '{api_target:<30}' if log_msg : msg += ' ' + log_msg self . log ( 'on_api_log' , msg ) def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( 'on_log' , log_msg ) def on_gym_init_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_gym_init_begin' , agent_context ) def on_gym_init_end ( self , agent_context : core . AgentContext ): self . log ( 'on_gym_init_end' , agent_context ) def on_gym_reset_begin ( self , agent_context : core . AgentContext , ** kwargs ): self . log ( 'on_gym_reset_begin' , agent_context ) def on_gym_reset_end ( self , agent_context : core . AgentContext , reset_result : Tuple , ** kwargs ): self . log ( 'on_gym_reset_end' , agent_context ) def on_gym_step_begin ( self , agent_context : core . AgentContext , action ): self . log ( 'on_gym_step_begin' , agent_context ) def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . log ( 'on_gym_step_end' , agent_context ) def on_play_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_play_begin' , agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): self . log ( 'on_play_end' , agent_context ) def on_play_episode_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_play_episode_begin' , agent_context ) def on_play_episode_end ( self , agent_context : core . AgentContext ): self . log ( 'on_play_episode_end' , agent_context ) def on_play_step_begin ( self , agent_context : core . AgentContext , action ): self . log ( 'on_play_step_begin' , agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . log ( 'on_play_step_end' , agent_context ) def on_train_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_train_begin' , agent_context ) def on_train_end ( self , agent_context : core . AgentContext ): self . log ( 'on_train_end' , agent_context ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_train_iteration_begin' , agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log ( 'on_train_iteration_end' , agent_context ) class _AgentContext ( _LogCallbackBase ): \"\"\"Logs the agent context and its subcontexts after every training iteration / episode played \"\"\" def on_play_episode_end ( self , agent_context : core . AgentContext ): self . log ( str ( agent_context )) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log ( str ( agent_context )) class _CallbackCounts ( core . AgentCallback ): def __init__ ( self ): self . gym_init_begin_count = 0 self . gym_init_end_count = 0 self . gym_reset_begin_count = 0 self . gym_reset_end_count = 0 self . gym_step_begin_count = 0 self . gym_step_end_count = 0 self . api_log_count = 0 self . log_count = 0 self . train_begin_count = 0 self . train_end_count = 0 self . train_iteration_begin_count = 0 self . train_iteration_end_count = 0 def __str__ ( self ): return f 'gym_init={self.gym_init_begin_count}:{self.gym_init_end_count} ' + \\ f 'gym_reset={self.gym_reset_begin_count}:{self.gym_reset_end_count} ' + \\ f 'gym_step={self.gym_step_begin_count}:{self.gym_step_end_count}' + \\ f 'train={self.train_begin_count}:{self.train_end_count} ' + \\ f 'train_iteration={self.train_iteration_begin_count}:{self.train_iteration_end_count}' + \\ f 'api_log={self.api_log_count} log={self.log_count} ' def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . api_log_count += 1 def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log_count += 1 def on_gym_init_begin ( self , agent_context : core . AgentContext ): self . gym_init_begin_count += 1 def on_gym_init_end ( self , agent_context : core . AgentContext ): self . gym_init_end_count += 1 def on_gym_reset_begin ( self , agent_context : core . AgentContext , ** kwargs ): self . gym_reset_begin_count += 1 def on_gym_reset_end ( self , agent_context : core . AgentContext , reset_result : Tuple , ** kwargs ): self . gym_reset_end_count += 1 def on_gym_step_begin ( self , agent_context : core . AgentContext , action ): self . gym_step_begin_count += 1 def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . gym_step_end_count += 1 def on_train_begin ( self , agent_context : core . AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" self . train_begin_count += 1 def on_train_end ( self , agent_context : core . AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" self . train_end_count += 1 def on_train_iteration_begin ( self , agent_context : core . AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" self . train_iteration_begin_count += 1 def on_train_iteration_end ( self , agent_context : core . AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" self . train_iteration_end_count += 1 class Agent ( _LogCallbackBase ): \"\"\"Logs agent activities to a python logger.\"\"\" def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . log ( api_target , log_msg ) def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( log_msg ) class Duration ( _LogCallbackBase ): \"\"\"Logs training / play duration definition summary to a logger.\"\"\" def log_duration ( self , agent_context : core . AgentContext ): tc = agent_context . train msg = f '#iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc msg = msg + f '#episodes_per_iteration={ec.num_episodes_per_iteration} ' msg = msg + f '#max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f '#iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f '#iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f '#episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f '{\"duration\":<25}{msg}' ) def on_train_begin ( self , agent_context : core . AgentContext ): self . log_duration ( agent_context ) class Iteration ( _LogCallbackBase ): \"\"\"Logs training iteration summaries to a python logger.\"\"\" def __init__ ( self , eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ): \"\"\"Logs the completeion of each training iteration. On iteration with policy evaluation the current average reward/episode and steps/episode is logged as well. Args: eval_only: if set a log is only created if the policy was re-evaluated in the current iteration. logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _eval_only : bool = eval_only super () . __init__ ( logger = logger , prefix = prefix ) def log_iteration ( self , agent_context : core . AgentContext ): tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ): msg = f 'episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )): msg = msg + f 'loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ): msg = msg + f '[actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f 'critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f 'rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f 'steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f 'iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f '{prefix:<25}{msg}' ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): tc : core . TrainContext = agent_context . train # log the results of a pre-train evaluation (if existing) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ): self . log_iteration ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log_iteration ( agent_context ) class Step ( _LogCallbackBase ): \"\"\"Logs each environment step to a python logger.\"\"\" def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f '[{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}:' + \\ f '{monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f 'train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4}' pc : core . PlayContext = agent_context . play if pc : prefix += f 'play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f 'sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f}' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg}' self . log ( f '{prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg}' ) Classes Agent class Agent ( logger : logging . Logger = None , prefix : str = None ) Logs agent activities to a python logger. View Source class Agent ( _LogCallbackBase ): \"\"\"Logs agent activities to a python logger.\"\"\" def on_api_log ( self , agent_context: core . AgentContext , api_target: str , log_msg: str ): self . log ( api_target , log_msg ) def on_log ( self , agent_context: core . AgentContext , log_msg: str ): self . log ( log_msg ) Ancestors (in MRO) easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC Methods log def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ) : msg = self . _prefix + f ' {msg_id:<25} ' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . log ( api_target , log_msg ) on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( log_msg ) on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" Duration class Duration ( logger : logging . Logger = None , prefix : str = None ) Logs training / play duration definition summary to a logger. View Source class Duration ( _LogCallbackBase ) : \"\"\" Logs training / play duration definition summary to a logger. \"\"\" def log_duration ( self , agent_context : core . AgentContext ) : tc = agent_context . train msg = f ' #iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ) : ec : core . EpisodesTrainContext = tc msg = msg + f ' #episodes_per_iteration={ec.num_episodes_per_iteration} ' msg = msg + f ' #max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f ' #iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f ' #iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f ' #episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f ' {\"duration\":<25}{msg} ' ) def on_train_begin ( self , agent_context : core . AgentContext ) : self . log_duration ( agent_context ) Ancestors (in MRO) easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC Methods log def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ) : msg = self . _prefix + f ' {msg_id:<25} ' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) log_duration def log_duration ( self , agent_context : easyagents . core . AgentContext ) View Source def log_duration ( self , agent_context : core . AgentContext ) : tc = agent_context . train msg = f ' #iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ) : ec : core . EpisodesTrainContext = tc msg = msg + f ' #episodes_per_iteration={ec.num_episodes_per_iteration} ' msg = msg + f ' #max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f ' #iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f ' #iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f ' #episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f ' {\"duration\":<25}{msg} ' ) on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): self . log_duration ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" Iteration class Iteration ( eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ) Logs training iteration summaries to a python logger. View Source class Iteration ( _LogCallbackBase ) : \"\"\" Logs training iteration summaries to a python logger. \"\"\" def __init__ ( self , eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ) : \"\"\" Logs the completeion of each training iteration. On iteration with policy evaluation the current average reward / episode and steps / episode is logged as well . Args : eval_only : if set a log is only created if the policy was re - evaluated in the current iteration . logger : the logger to log ( if None a new logger with level debug is created ) prefix : a string written in front of each log msg \"\"\" self . _eval_only : bool = eval_only super () . __init__ ( logger = logger , prefix = prefix ) def log_iteration ( self , agent_context : core . AgentContext ) : tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : msg = f ' episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )) : msg = msg + f ' loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ) : msg = msg + f ' [actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f ' critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f ' rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f ' steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f ' iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f ' {prefix:<25}{msg} ' ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ) : tc : core . TrainContext = agent_context . train # log the results of a pre - train evaluation ( if existing ) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ) : self . log_iteration ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ) : self . log_iteration ( agent_context ) Ancestors (in MRO) easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC Methods log def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ) : msg = self . _prefix + f ' {msg_id:<25} ' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) log_iteration def log_iteration ( self , agent_context : easyagents . core . AgentContext ) View Source def log_iteration ( self , agent_context : core . AgentContext ) : tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : msg = f ' episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )) : msg = msg + f ' loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ) : msg = msg + f ' [actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f ' critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f ' rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f ' steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f ' iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f ' {prefix:<25}{msg} ' ) on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : core . AgentContext ) : tc : core . TrainContext = agent_context . train # log the results of a pre - train evaluation ( if existing ) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ) : self . log_iteration ( agent_context ) on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log_iteration ( agent_context ) Step class Step ( logger : logging . Logger = None , prefix : str = None ) Logs each environment step to a python logger. View Source class Step ( _LogCallbackBase ) : \"\"\" Logs each environment step to a python logger. \"\"\" def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ) : prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f ' [{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}: ' + \\ f ' {monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f ' train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4} ' pc : core . PlayContext = agent_context . play if pc : prefix += f ' play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f ' sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f} ' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg} ' self . log ( f ' {prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg} ' ) Ancestors (in MRO) easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC Methods log def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ) : msg = self . _prefix + f ' {msg_id:<25} ' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ) : prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f ' [{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}: ' + \\ f ' {monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f ' train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4} ' pc : core . PlayContext = agent_context . play if pc : prefix += f ' play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f ' sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f} ' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg} ' self . log ( f ' {prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg} ' ) on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"Log"},{"location":"reference/easyagents/callbacks/log/#module-easyagentscallbackslog","text":"View Source from typing import Tuple import logging import math from easyagents import core class _LogCallbackBase ( core . AgentCallback ): \"\"\"Base class for Callback loggers\"\"\" def __init__ ( self , logger : logging . Logger = None , prefix : str = None ): \"\"\"Writes all calls to logger with the given prefix. Args: logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _logger = logger if self . _logger is None : self . _logger = logging . getLogger () self . _prefix = prefix if self . _prefix is None : self . _prefix = '' def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) class _Callbacks ( _LogCallbackBase ): \"\"\"Logs all AgentCallback calls to a Logger\"\"\" def __init__ ( self , logger : logging . Logger = None , prefix : str = None ): \"\"\"Writes all calls to a callback function to logger with the given prefix. Args: logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" super () . __init__ ( logger = logger , prefix = prefix ) def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): msg = f '{api_target:<30}' if log_msg : msg += ' ' + log_msg self . log ( 'on_api_log' , msg ) def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( 'on_log' , log_msg ) def on_gym_init_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_gym_init_begin' , agent_context ) def on_gym_init_end ( self , agent_context : core . AgentContext ): self . log ( 'on_gym_init_end' , agent_context ) def on_gym_reset_begin ( self , agent_context : core . AgentContext , ** kwargs ): self . log ( 'on_gym_reset_begin' , agent_context ) def on_gym_reset_end ( self , agent_context : core . AgentContext , reset_result : Tuple , ** kwargs ): self . log ( 'on_gym_reset_end' , agent_context ) def on_gym_step_begin ( self , agent_context : core . AgentContext , action ): self . log ( 'on_gym_step_begin' , agent_context ) def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . log ( 'on_gym_step_end' , agent_context ) def on_play_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_play_begin' , agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): self . log ( 'on_play_end' , agent_context ) def on_play_episode_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_play_episode_begin' , agent_context ) def on_play_episode_end ( self , agent_context : core . AgentContext ): self . log ( 'on_play_episode_end' , agent_context ) def on_play_step_begin ( self , agent_context : core . AgentContext , action ): self . log ( 'on_play_step_begin' , agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . log ( 'on_play_step_end' , agent_context ) def on_train_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_train_begin' , agent_context ) def on_train_end ( self , agent_context : core . AgentContext ): self . log ( 'on_train_end' , agent_context ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_train_iteration_begin' , agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log ( 'on_train_iteration_end' , agent_context ) class _AgentContext ( _LogCallbackBase ): \"\"\"Logs the agent context and its subcontexts after every training iteration / episode played \"\"\" def on_play_episode_end ( self , agent_context : core . AgentContext ): self . log ( str ( agent_context )) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log ( str ( agent_context )) class _CallbackCounts ( core . AgentCallback ): def __init__ ( self ): self . gym_init_begin_count = 0 self . gym_init_end_count = 0 self . gym_reset_begin_count = 0 self . gym_reset_end_count = 0 self . gym_step_begin_count = 0 self . gym_step_end_count = 0 self . api_log_count = 0 self . log_count = 0 self . train_begin_count = 0 self . train_end_count = 0 self . train_iteration_begin_count = 0 self . train_iteration_end_count = 0 def __str__ ( self ): return f 'gym_init={self.gym_init_begin_count}:{self.gym_init_end_count} ' + \\ f 'gym_reset={self.gym_reset_begin_count}:{self.gym_reset_end_count} ' + \\ f 'gym_step={self.gym_step_begin_count}:{self.gym_step_end_count}' + \\ f 'train={self.train_begin_count}:{self.train_end_count} ' + \\ f 'train_iteration={self.train_iteration_begin_count}:{self.train_iteration_end_count}' + \\ f 'api_log={self.api_log_count} log={self.log_count} ' def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . api_log_count += 1 def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log_count += 1 def on_gym_init_begin ( self , agent_context : core . AgentContext ): self . gym_init_begin_count += 1 def on_gym_init_end ( self , agent_context : core . AgentContext ): self . gym_init_end_count += 1 def on_gym_reset_begin ( self , agent_context : core . AgentContext , ** kwargs ): self . gym_reset_begin_count += 1 def on_gym_reset_end ( self , agent_context : core . AgentContext , reset_result : Tuple , ** kwargs ): self . gym_reset_end_count += 1 def on_gym_step_begin ( self , agent_context : core . AgentContext , action ): self . gym_step_begin_count += 1 def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . gym_step_end_count += 1 def on_train_begin ( self , agent_context : core . AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" self . train_begin_count += 1 def on_train_end ( self , agent_context : core . AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" self . train_end_count += 1 def on_train_iteration_begin ( self , agent_context : core . AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" self . train_iteration_begin_count += 1 def on_train_iteration_end ( self , agent_context : core . AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" self . train_iteration_end_count += 1 class Agent ( _LogCallbackBase ): \"\"\"Logs agent activities to a python logger.\"\"\" def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . log ( api_target , log_msg ) def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( log_msg ) class Duration ( _LogCallbackBase ): \"\"\"Logs training / play duration definition summary to a logger.\"\"\" def log_duration ( self , agent_context : core . AgentContext ): tc = agent_context . train msg = f '#iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc msg = msg + f '#episodes_per_iteration={ec.num_episodes_per_iteration} ' msg = msg + f '#max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f '#iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f '#iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f '#episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f '{\"duration\":<25}{msg}' ) def on_train_begin ( self , agent_context : core . AgentContext ): self . log_duration ( agent_context ) class Iteration ( _LogCallbackBase ): \"\"\"Logs training iteration summaries to a python logger.\"\"\" def __init__ ( self , eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ): \"\"\"Logs the completeion of each training iteration. On iteration with policy evaluation the current average reward/episode and steps/episode is logged as well. Args: eval_only: if set a log is only created if the policy was re-evaluated in the current iteration. logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _eval_only : bool = eval_only super () . __init__ ( logger = logger , prefix = prefix ) def log_iteration ( self , agent_context : core . AgentContext ): tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ): msg = f 'episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )): msg = msg + f 'loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ): msg = msg + f '[actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f 'critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f 'rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f 'steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f 'iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f '{prefix:<25}{msg}' ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): tc : core . TrainContext = agent_context . train # log the results of a pre-train evaluation (if existing) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ): self . log_iteration ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log_iteration ( agent_context ) class Step ( _LogCallbackBase ): \"\"\"Logs each environment step to a python logger.\"\"\" def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f '[{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}:' + \\ f '{monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f 'train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4}' pc : core . PlayContext = agent_context . play if pc : prefix += f 'play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f 'sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f}' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg}' self . log ( f '{prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg}' )","title":"Module easyagents.callbacks.log"},{"location":"reference/easyagents/callbacks/log/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/callbacks/log/#agent","text":"class Agent ( logger : logging . Logger = None , prefix : str = None ) Logs agent activities to a python logger. View Source class Agent ( _LogCallbackBase ): \"\"\"Logs agent activities to a python logger.\"\"\" def on_api_log ( self , agent_context: core . AgentContext , api_target: str , log_msg: str ): self . log ( api_target , log_msg ) def on_log ( self , agent_context: core . AgentContext , log_msg: str ): self . log ( log_msg )","title":"Agent"},{"location":"reference/easyagents/callbacks/log/#ancestors-in-mro","text":"easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/log/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/log/#log","text":"def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ) : msg = self . _prefix + f ' {msg_id:<25} ' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg )","title":"log"},{"location":"reference/easyagents/callbacks/log/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . log ( api_target , log_msg )","title":"on_api_log"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/log/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( log_msg )","title":"on_log"},{"location":"reference/easyagents/callbacks/log/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/log/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/log/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/log/#duration","text":"class Duration ( logger : logging . Logger = None , prefix : str = None ) Logs training / play duration definition summary to a logger. View Source class Duration ( _LogCallbackBase ) : \"\"\" Logs training / play duration definition summary to a logger. \"\"\" def log_duration ( self , agent_context : core . AgentContext ) : tc = agent_context . train msg = f ' #iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ) : ec : core . EpisodesTrainContext = tc msg = msg + f ' #episodes_per_iteration={ec.num_episodes_per_iteration} ' msg = msg + f ' #max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f ' #iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f ' #iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f ' #episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f ' {\"duration\":<25}{msg} ' ) def on_train_begin ( self , agent_context : core . AgentContext ) : self . log_duration ( agent_context )","title":"Duration"},{"location":"reference/easyagents/callbacks/log/#ancestors-in-mro_1","text":"easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/log/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/log/#log_1","text":"def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ) : msg = self . _prefix + f ' {msg_id:<25} ' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg )","title":"log"},{"location":"reference/easyagents/callbacks/log/#log_duration","text":"def log_duration ( self , agent_context : easyagents . core . AgentContext ) View Source def log_duration ( self , agent_context : core . AgentContext ) : tc = agent_context . train msg = f ' #iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ) : ec : core . EpisodesTrainContext = tc msg = msg + f ' #episodes_per_iteration={ec.num_episodes_per_iteration} ' msg = msg + f ' #max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f ' #iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f ' #iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f ' #episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f ' {\"duration\":<25}{msg} ' )","title":"log_duration"},{"location":"reference/easyagents/callbacks/log/#on_api_log_1","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_begin_1","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_end_1","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_begin_1","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_end_1","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_begin_1","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_end_1","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/log/#on_log_1","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/log/#on_play_begin_1","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_end_1","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_end_1","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/log/#on_play_step_begin_1","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_step_end_1","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/log/#on_train_begin_1","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): self . log_duration ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_end_1","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/log/#iteration","text":"class Iteration ( eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ) Logs training iteration summaries to a python logger. View Source class Iteration ( _LogCallbackBase ) : \"\"\" Logs training iteration summaries to a python logger. \"\"\" def __init__ ( self , eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ) : \"\"\" Logs the completeion of each training iteration. On iteration with policy evaluation the current average reward / episode and steps / episode is logged as well . Args : eval_only : if set a log is only created if the policy was re - evaluated in the current iteration . logger : the logger to log ( if None a new logger with level debug is created ) prefix : a string written in front of each log msg \"\"\" self . _eval_only : bool = eval_only super () . __init__ ( logger = logger , prefix = prefix ) def log_iteration ( self , agent_context : core . AgentContext ) : tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : msg = f ' episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )) : msg = msg + f ' loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ) : msg = msg + f ' [actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f ' critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f ' rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f ' steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f ' iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f ' {prefix:<25}{msg} ' ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ) : tc : core . TrainContext = agent_context . train # log the results of a pre - train evaluation ( if existing ) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ) : self . log_iteration ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ) : self . log_iteration ( agent_context )","title":"Iteration"},{"location":"reference/easyagents/callbacks/log/#ancestors-in-mro_2","text":"easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/log/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/log/#log_2","text":"def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ) : msg = self . _prefix + f ' {msg_id:<25} ' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg )","title":"log"},{"location":"reference/easyagents/callbacks/log/#log_iteration","text":"def log_iteration ( self , agent_context : easyagents . core . AgentContext ) View Source def log_iteration ( self , agent_context : core . AgentContext ) : tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : msg = f ' episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )) : msg = msg + f ' loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ) : msg = msg + f ' [actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f ' critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f ' rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f ' steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f ' iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f ' {prefix:<25}{msg} ' )","title":"log_iteration"},{"location":"reference/easyagents/callbacks/log/#on_api_log_2","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_begin_2","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_end_2","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_begin_2","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_end_2","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_begin_2","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_end_2","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/log/#on_log_2","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/log/#on_play_begin_2","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_end_2","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_begin_2","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_end_2","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/log/#on_play_step_begin_2","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_step_end_2","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/log/#on_train_begin_2","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_end_2","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_begin_2","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : core . AgentContext ) : tc : core . TrainContext = agent_context . train # log the results of a pre - train evaluation ( if existing ) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ) : self . log_iteration ( agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_end_2","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log_iteration ( agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/log/#step","text":"class Step ( logger : logging . Logger = None , prefix : str = None ) Logs each environment step to a python logger. View Source class Step ( _LogCallbackBase ) : \"\"\" Logs each environment step to a python logger. \"\"\" def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ) : prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f ' [{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}: ' + \\ f ' {monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f ' train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4} ' pc : core . PlayContext = agent_context . play if pc : prefix += f ' play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f ' sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f} ' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg} ' self . log ( f ' {prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg} ' )","title":"Step"},{"location":"reference/easyagents/callbacks/log/#ancestors-in-mro_3","text":"easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/log/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/log/#log_3","text":"def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ) : msg = self . _prefix + f ' {msg_id:<25} ' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg )","title":"log"},{"location":"reference/easyagents/callbacks/log/#on_api_log_3","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_begin_3","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_end_3","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_begin_3","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_end_3","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_begin_3","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_end_3","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ) : prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f ' [{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}: ' + \\ f ' {monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f ' train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4} ' pc : core . PlayContext = agent_context . play if pc : prefix += f ' play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f ' sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f} ' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg} ' self . log ( f ' {prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg} ' )","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/log/#on_log_3","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/log/#on_play_begin_3","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_end_3","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_begin_3","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_end_3","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/log/#on_play_step_begin_3","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_step_end_3","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/log/#on_train_begin_3","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_end_3","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_begin_3","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_end_3","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/","text":"Module easyagents.callbacks.plot View Source from typing import Optional , List , Tuple , Union , Dict import easyagents.core as core import base64 import matplotlib.pyplot as plt import numpy as np import imageio import math import gym import tempfile import os.path import datetime # download mp4 rendering imageio . plugins . ffmpeg . download () # avoid \"double rendering\" of the final jupyter output on_play_end_clear_jupyter_display : bool = False on_train_end_clear_jupyter_display : bool = True # check if we are running in Jupyter, if so interactive plotting must be handled differently # (in order to get plot updates during training) _is_jupyter_active = False try : # noinspection PyUnresolvedReferences from IPython import get_ipython # noinspection PyUnresolvedReferences from IPython.display import display , clear_output # noinspection PyUnresolvedReferences from IPython.display import HTML shell = get_ipython () . __class__ . __name__ if shell == 'ZMQInteractiveShell' : _is_jupyter_active = True else : # noinspection PyPackageRequirements import google.colab _is_jupyter_active = True except ImportError : pass class _PreProcess ( core . _PreProcessCallback ): \"\"\"Initializes the matplotlib agent_context.pyplot.figure\"\"\" def _setup ( self , agent_context : core . AgentContext ): # create figure / remove all existing axes from previous calls to train/play pyc = agent_context . pyplot pyc . is_jupyter_active = _is_jupyter_active if pyc . figure is None : pyc . figure = plt . figure ( \"_EasyAgents\" , figsize = pyc . figsize ) for ax in pyc . figure . axes : pyc . figure . delaxes ( ax ) def on_play_begin ( self , agent_context : core . AgentContext ): # play_begin is also called at the start of a policy evaluation if agent_context . is_play : self . _setup ( agent_context = agent_context ) def on_train_begin ( self , agent_context : core . AgentContext ): self . _setup ( agent_context = agent_context ) class _PostProcess ( core . _PostProcessCallback ): \"\"\"Redraws the plots int the matplotlib agent_context.figure. In jupyter the plots are only refreshed once per second. \"\"\" def __init__ ( self ): self . _call_jupyter_display : bool self . _reset () def _clear_jupyter_plots ( self , agent_context : core . AgentContext , wait = True ): \"\"\"Clears the content in the current jupyter output cell. NoOp if not in jupyter or no plot output. Args: wait: Wait to clear the output until new output is available to replace it. \"\"\" # don't clear the jupyter output if no plot is present, may clear the log output otherwise if agent_context . pyplot . is_jupyter_active and self . _plot_exists ( agent_context ): clear_output ( wait = wait ) def _display_plots ( self , agent_context : core . AgentContext ): \"\"\"Fixes the layout of multiple subplots and refreshs the display.\"\"\" pyc = agent_context . pyplot if self . _plot_exists ( agent_context ): count = len ( pyc . figure . axes ) rows = math . ceil ( count / pyc . max_columns ) columns = math . ceil ( count / rows ) for i in range ( count ): pyc . figure . axes [ i ] . change_geometry ( rows , columns , i + 1 ) pyc . figure . tight_layout () if pyc . is_jupyter_active : self . _clear_jupyter_plots ( agent_context ) if self . _call_jupyter_display : # noinspection PyTypeChecker display ( pyc . figure ) self . _call_jupyter_display = True else : plt . pause ( 0.01 ) def _plot_exists ( self , agent_context : core . AgentContext ): \"\"\"Yields true if at least 1 jupyter plot exists.\"\"\" pyc = agent_context . pyplot count = len ( pyc . figure . axes ) result = count > 0 return result def _reset ( self ): self . _call_jupyter_display = False def on_play_begin ( self , agent_context : core . AgentContext ): if agent_context . is_play : self . _reset () def on_train_begin ( self , agent_context : core . AgentContext ): self . _reset () def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _display_plots ( agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _display_plots ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_EVAL ): self . _display_plots ( agent_context ) if agent_context . is_play : self . _display_plots ( agent_context ) if on_play_end_clear_jupyter_display : self . _clear_jupyter_plots ( agent_context , wait = False ) def on_train_end ( self , agent_context : core . AgentContext ): self . _display_plots ( agent_context ) if on_train_end_clear_jupyter_display : self . _clear_jupyter_plots ( agent_context , wait = False ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): # display initial evaluation before training starts. if agent_context . train . iterations_done_in_training == 0 and \\ agent_context . _is_plot_ready ( core . PlotType . TRAIN_EVAL ): self . _display_plots ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _display_plots ( agent_context ) # noinspection DuplicatedCode class _PlotCallback ( core . AgentCallback ): \"\"\"Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto \"\"\" def __init__ ( self , plot_type : core . PlotType ): \"\"\"Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Args: plot_type: point in time when the plot is updated \"\"\" self . axes = None self . axes_color : str = 'grey' self . _plot_type : core . PlotType = plot_type def _create_subplot ( self , agent_context : core . AgentContext ): if self . axes is None : pyc = agent_context . pyplot pyc . _created_subplots = pyc . _created_subplots | self . _plot_type count = len ( pyc . figure . axes ) + 1 rows = math . ceil ( count / pyc . max_columns ) columns = math . ceil ( count / rows ) self . axes = pyc . figure . add_subplot ( rows , columns , count ) self . plot_axes ( xlim = ( 0 , 1 ), ylabel = '' , xlabel = '' ) def _is_nan ( self , values : Optional [ List [ float ]]): \"\"\"yields true if all values are equal to nan. yields false if values is None or empty.\"\"\" result = False if values and all ( isinstance ( v , float ) for v in values ): result = all ( math . isnan ( v ) for v in values ) return result def _refresh_subplot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): \"\"\"Sets this axes active and calls plot if this plot callback is registered on at least 1 plot out of plot_type.\"\"\" assert self . axes is not None plot_type = plot_type & self . _plot_type if agent_context . _is_plot_ready ( plot_type ): pyc = agent_context . pyplot if not pyc . is_jupyter_active : plt . figure ( pyc . figure . number ) if plt . gcf () is pyc . figure : plt . sca ( self . axes ) self . plot ( agent_context , plot_type ) def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): \"\"\"Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. \"\"\" pass def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot (axes, labels, colors) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ] . set_visible ( False ) self . axes . spines [ 'right' ] . set_visible ( False ) self . axes . spines [ 'bottom' ] . set_color ( axes_color ) self . axes . spines [ 'left' ] . set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) def plot_text ( self , text : str ): if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ([]) ax . get_yaxis () . set_ticks ([]) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = 'blue' ): \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ): \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [(min,y,max),...) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ): ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) # noinspection DuplicatedCode class Actions ( _PlotCallback ): def __init__ ( self , num_steps_between_plot = 100 ): \"\"\"Plots a histogram of the actions taken during play or during the last evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _actions : List [ float ] = [] self . _num_steps_between_plot = num_steps_between_plot def _reset ( self ): self . _actions : List [ float ] = [] def on_play_begin ( self , agent_context : core . AgentContext ): super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ): super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): try : pc = agent_context . play is_plot = True xlabel = 'actions' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = 'actions taken during last evaluation period' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = 'count' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f 'Failed to create the actions histogram. \\n ' ) class Clear ( core . AgentCallback ): \"\"\"Configures the clearing of plots in the jupyter output cell after calls to train or play.\"\"\" def __init__ ( self , on_play : bool = True , on_train : bool = True ): \"\"\"Define the cell clearing behaviour after agent.train and agent.play. Args: on_play: if set the output cell is cleared after agent.play if a plot exists on_train: if set the output cell is cleared after agent.train if a plot exists \"\"\" self . _on_play : bool = on_play self . _on_train : bool = on_train on_play_end_clear_jupyter_display = self . _on_play on_train_end_clear_jupyter_display = self . _on_train def on_play_begin ( self , agent_context : core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play def on_train_begin ( self , agent_context : core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train class Loss ( _PlotCallback ): def __init__ ( self , yscale : str = 'symlog' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the loss resulting from each iterations policy training. Hints: o for actro-critic agents the loss from training the actor- and critic-networks are plotted along with the total loss. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( plot_type = core . PlotType . TRAIN_ITERATION ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ): self . plot_text ( 'no loss data available' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ), xlabel = 'episodes trained' , ylim = self . ylim , ylabel = 'loss' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ): acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()), color = 'g' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()), color = 'b' ) self . axes . legend (( 'total' , 'actor' , 'critic' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' ) class Rewards ( _PlotCallback ): def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the sum of rewards observed during policy evaluation. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'sum of rewards' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = '\u00d8 sum of rewards' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = 'green' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) class State ( _PlotCallback ): \"\"\"Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. \"\"\" def __init__ ( self , mode = 'rgb_array' ): \"\"\" Args: mode: the render mode passed to gym.render(), yielding an rgb_array \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . TRAIN_EVAL ) self . _render_mode = mode def _plot_rgb_array ( self , agent_context : core . AgentContext , rgb_array : np . ndarray ): \"\"\"Renders rgb_array to the current subplot.\"\"\" assert rgb_array is not None ax = self . axes xlabel = '' if agent_context . is_eval : xlabel = \"'done state' of last evaluation episode\" ax . imshow ( rgb_array ) ax . set_xlabel ( xlabel ) ax . get_xaxis () . set_ticks ([]) ax . get_yaxis () . set_ticks ([]) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) # noinspection PyArgumentList,DuplicatedCode def _render_to_rgb_array ( self , gym_env : gym . Env , mode : str ) -> np . ndarray : \"\"\" calls gym_env.render(mode) and validates the return value to be a numpy rgb array throws an exception if not an rgb array Returns: numpy rgb array \"\"\" result = gym_env . render ( mode = mode ) assert result is not None , f 'gym_env.render(mode={mode}) yielded None' assert isinstance ( result , np . ndarray ), f 'gym_env.render(mode={mode}) did not yield a numpy.ndarray.' assert result . min () >= 0 , f 'gym_env.render(mode={mode}) contains negative values => not an rgb array' assert result . max () <= 255 , f 'gym_env.render(mode={mode}) contains values > 255 => not an rgb array' assert len ( result . shape ) == 3 , f 'gym_env.render(mode={mode}) shape is not of the form (x,y,n)' assert result . shape [ 2 ] == 3 or result . shape [ 2 ] == 4 , \\ f 'gym_env.render(mode={mode}) shape is not of the form (x,y,3|4)' return result def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\"{self._render_mode}\") failed: \\n ' ) class Steps ( _PlotCallback ): def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the step counts observed during policy evaluation. Args: yscale: scale of the y-axes ('linear','log') ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'steps' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = '\u00d8 steps' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len ( pc . actions [ episode ]) for episode in pc . actions . keys ()] self . plot_subplot ( agent_context , color = 'blue' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) class StepRewards ( _PlotCallback ): def __init__ ( self , num_steps_between_plot = 100 ): \"\"\"Plots the sum of rewards up to the current step during play or at the end of an evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super () . __init__ ( core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _xy_values : Dict [ int , Tuple [ List [ int ], List [ float ]]] = dict () self . _xmax : int = 0 self . _num_steps_between_plot = num_steps_between_plot def _replot ( self , agent_context : core . AgentContext ): if self . _xmax >= 1 : self . clear_plot ( agent_context ) xlabel = 'steps played' if agent_context . is_eval : xlabel = 'steps taken during last evaluation period' self . plot_axes ( xlim = ( 1 , self . _xmax ), ylabel = 'sum of rewards' , xlabel = xlabel ) xy_values = list ( self . _xy_values . values ()) xlast , _ = xy_values [ - 1 ] for xvalues , yvalues in xy_values : pause = ( xvalues == xlast ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = None , marker = '' , pause = pause ) def _reset ( self ): self . _xy_values = dict () self . _xmax = 0 def on_play_begin ( self , agent_context : core . AgentContext ): super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ): super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ([], []) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ]) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ([ len ( step_rewards ) for step_rewards in pc . rewards . values ()]) for episode in pc . rewards . keys (): step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ - 1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context ) class ToMovie ( core . _PostProcessCallback ): \"\"\"Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. \"\"\" def __init__ ( self , fps : Optional [ int ] = None , filepath : str = None ): \"\"\"Writes the ploted graphs and images to the mp4 / gif file given by filepath. if filepath ends in '.gif' an animated gif is created. Args: fps: frames per second filepath: the filepath of the mp4 or gif file file. If None the file is written to a temp file. \"\"\" super () . __init__ () self . fps = fps self . _is_filepath_set = filepath is not None self . filepath = filepath if not self . _is_filepath_set : self . filepath = self . _get_temp_path () if ( not self . _is_animated_gif ()) and ( not self . filepath . lower () . endswith ( '.mp4' )): self . filepath = self . filepath + '.mp4' self . _video = imageio . get_writer ( self . filepath , fps = fps ) if fps else imageio . get_writer ( self . filepath ) def _close ( self , agent_context : core . AgentContext ): \"\"\"closes the mp4 file and displays it in jupyter cell (if in a jupyter notebook)\"\"\" self . _video . close () self . _video = None if agent_context . pyplot . is_jupyter_active : with open ( self . filepath , 'rb' ) as f : video = f . read () b64 = base64 . b64encode ( video ) if not self . _is_filepath_set : os . remove ( self . filepath ) width = 640 height = 480 if self . _is_animated_gif (): result = ''' <img src=\"data:image/gif;base64,{2}\" alt=\"easyagents.plot\" width={0}/> ''' . format ( width , height , b64 . decode ()) else : result = ''' <video width=\"{0}\" height=\"{1}\" controls> <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\"> Your browser does not support the video tag. </video>''' . format ( width , height , b64 . decode ()) result = HTML ( result ) # noinspection PyTypeChecker clear_output ( wait = True ) # noinspection PyTypeChecker display ( result ) def _get_rgb_array ( self , agent_context : core . AgentContext ) -> np . ndarray : \"\"\"Yields an rgb array representing the current content of the subplots.\"\"\" pyc = agent_context . pyplot pyc . figure . canvas . draw () result = np . frombuffer ( pyc . figure . canvas . tostring_rgb (), dtype = 'uint8' ) result = result . reshape ( pyc . figure . canvas . get_width_height ()[:: - 1 ] + ( 3 ,)) return result def _get_temp_path ( self ): result = os . path . join ( tempfile . gettempdir (), tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f '-{n.year % 100:2}{n.month:02}{n.day:02}-{n.hour:02}{n.minute:02}{n.second:02}-' + \\ f '{n.microsecond:06}' return result def _is_animated_gif ( self ): return self . filepath . lower () . endswith ( '.gif' ) def _write_figure_to_video ( self , agent_context : core . AgentContext ): \"\"\"Appends the current pyplot figure to the video. if an exception occures no frame is added. \"\"\" try : rgb_array = self . _get_rgb_array ( agent_context ) self . _video . append_data ( rgb_array ) except : pass def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ): self . _write_figure_to_video ( agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _write_figure_to_video ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _write_figure_to_video ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _close ( agent_context ) def on_train_end ( self , agent_context : core . AgentContext ): self . _close ( agent_context ) Variables on_play_end_clear_jupyter_display on_train_end_clear_jupyter_display shell Classes Actions class Actions ( num_steps_between_plot = 100 ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Actions ( _PlotCallback ) : def __init__ ( self , num_steps_between_plot = 100 ) : \"\"\" Plots a histogram of the actions taken during play or during the last evaluation period. Args : num_steps_between_plot : num of steps to play before plot is updated . \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _actions : List [ float ] = [] self . _num_steps_between_plot = num_steps_between_plot def _reset ( self ) : self . _actions : List [ float ] = [] def on_play_begin ( self , agent_context : core . AgentContext ) : super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ) : super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : try : pc = agent_context . play is_plot = True xlabel = ' actions ' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = ' actions taken during last evaluation period ' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = ' count ' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f ' Failed to create the actions histogram. \\n ' ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): super (). on_play_begin ( agent_context ) self . _reset () on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): super (). on_play_end ( agent_context ) self . _reset () on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : try : pc = agent_context . play is_plot = True xlabel = ' actions ' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = ' actions taken during last evaluation period ' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = ' count ' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f ' Failed to create the actions histogram. \\n ' ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 ) Clear class Clear ( on_play : bool = True , on_train : bool = True ) Configures the clearing of plots in the jupyter output cell after calls to train or play. View Source class Clear ( core . AgentCallback ) : \"\"\" Configures the clearing of plots in the jupyter output cell after calls to train or play. \"\"\" def __init__ ( self , on_play : bool = True , on_train : bool = True ) : \"\"\" Define the cell clearing behaviour after agent.train and agent.play. Args : on_play : if set the output cell is cleared after agent . play if a plot exists on_train : if set the output cell is cleared after agent . train if a plot exists \"\"\" self . _on_play : bool = on_play self . _on_train : bool = on_train on_play_end_clear_jupyter_display = self . _on_play on_train_end_clear_jupyter_display = self . _on_train def on_play_begin ( self , agent_context : core . AgentContext ) : global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play def on_train_begin ( self , agent_context : core . AgentContext ) : global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train Ancestors (in MRO) easyagents.core.AgentCallback abc.ABC Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" Loss class Loss ( yscale : str = 'symlog' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Loss ( _PlotCallback ) : def __init__ ( self , yscale : str = ' symlog ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Plots the loss resulting from each iterations policy training. Hints : o for actro - critic agents the loss from training the actor - and critic - networks are plotted along with the total loss . Args : yscale : scale of the y - axes ( ' linear ' , ' symlog ' ,... ) ylim : ( min , max ) for the y - axes \"\"\" super () . __init__ ( plot_type = core . PlotType . TRAIN_ITERATION ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ) : self . plot_text ( ' no loss data available ' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ) , xlabel = ' episodes trained ' , ylim = self . ylim , ylabel = ' loss ' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ) : acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = ' indigo ' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()) , color = ' g ' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()) , color = ' b ' ) self . axes . legend (( ' total ' , ' actor ' , ' critic ' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = ' indigo ' ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ) : self . plot_text ( ' no loss data available ' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ) , xlabel = ' episodes trained ' , ylim = self . ylim , ylabel = ' loss ' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ) : acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = ' indigo ' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()) , color = ' g ' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()) , color = ' b ' ) self . axes . legend (( ' total ' , ' actor ' , ' critic ' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = ' indigo ' ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 ) Rewards class Rewards ( yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Rewards ( _PlotCallback ) : def __init__ ( self , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Plots the sum of rewards observed during policy evaluation. Args : yscale : scale of the y - axes ( ' linear ' , ' symlog ' ,... ) ylim : ( min , max ) for the y - axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = ' sum of rewards ' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = ' \u00d8 sum of rewards ' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = ' green ' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = ' sum of rewards ' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = ' \u00d8 sum of rewards ' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = ' green ' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 ) State class State ( mode = 'rgb_array' ) Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. View Source class State ( _PlotCallback ) : \"\"\" Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted . During play all states are plotted . \"\"\" def __init__ ( self , mode = ' rgb_array ' ) : \"\"\" Args : mode : the render mode passed to gym . render () , yielding an rgb_array \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . TRAIN_EVAL ) self . _render_mode = mode def _plot_rgb_array ( self , agent_context : core . AgentContext , rgb_array : np . ndarray ) : \"\"\" Renders rgb_array to the current subplot. \"\"\" assert rgb_array is not None ax = self . axes xlabel = '' if agent_context . is_eval : xlabel = \" 'done state' of last evaluation episode \" ax . imshow ( rgb_array ) ax . set_xlabel ( xlabel ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color ) # noinspection PyArgumentList , DuplicatedCode def _render_to_rgb_array ( self , gym_env : gym . Env , mode : str ) -> np . ndarray : \"\"\" calls gym_env.render(mode) and validates the return value to be a numpy rgb array throws an exception if not an rgb array Returns : numpy rgb array \"\"\" result = gym_env . render ( mode = mode ) assert result is not None , f ' gym_env.render(mode={mode}) yielded None ' assert isinstance ( result , np . ndarray ) , f ' gym_env.render(mode={mode}) did not yield a numpy.ndarray. ' assert result . min () >= 0 , f ' gym_env.render(mode={mode}) contains negative values => not an rgb array ' assert result . max () <= 255 , f ' gym_env.render(mode={mode}) contains values > 255 => not an rgb array ' assert len ( result . shape ) == 3 , f ' gym_env.render(mode={mode}) shape is not of the form (x,y,n) ' assert result . shape [ 2 ] == 3 or result . shape [ 2 ] == 4 , \\ f ' gym_env.render(mode={mode}) shape is not of the form (x,y,3|4) ' return result def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f ' gym.Env.render(mode=\"{self._render_mode}\") failed: \\n ' ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\"{self._render_mode}\") failed:\\n' ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 ) StepRewards class StepRewards ( num_steps_between_plot = 100 ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class StepRewards ( _PlotCallback ) : def __init__ ( self , num_steps_between_plot = 100 ) : \"\"\" Plots the sum of rewards up to the current step during play or at the end of an evaluation period . Args : num_steps_between_plot : num of steps to play before plot is updated . \"\"\" super () . __init__ ( core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _xy_values : Dict [ int , Tuple [ List [ int ], List [ float ]]] = dict () self . _xmax : int = 0 self . _num_steps_between_plot = num_steps_between_plot def _replot ( self , agent_context : core . AgentContext ) : if self . _xmax >= 1 : self . clear_plot ( agent_context ) xlabel = ' steps played ' if agent_context . is_eval : xlabel = ' steps taken during last evaluation period ' self . plot_axes ( xlim = ( 1 , self . _xmax ) , ylabel = ' sum of rewards ' , xlabel = xlabel ) xy_values = list ( self . _xy_values . values ()) xlast , _ = xy_values [ - 1 ] for xvalues , yvalues in xy_values : pause = ( xvalues == xlast ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = None , marker = '' , pause = pause ) def _reset ( self ) : self . _xy_values = dict () self . _xmax = 0 def on_play_begin ( self , agent_context : core . AgentContext ) : super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ) : super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ( [], [] ) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ] ) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ( [ len ( step_rewards ) for step_rewards in pc . rewards . values () ] ) for episode in pc . rewards . keys () : step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ - 1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): super (). on_play_begin ( agent_context ) self . _reset () on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): super (). on_play_end ( agent_context ) self . _reset () on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ( [], [] ) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ] ) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ( [ len ( step_rewards ) for step_rewards in pc . rewards . values () ] ) for episode in pc . rewards . keys () : step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ - 1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 ) Steps class Steps ( yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Steps ( _PlotCallback ) : def __init__ ( self , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Plots the step counts observed during policy evaluation. Args : yscale : scale of the y - axes ( ' linear ' , ' log ' ) ylim : ( min , max ) for the y - axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = ' steps ' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = ' \u00d8 steps ' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len ( pc . actions [ episode ] ) for episode in pc . actions . keys () ] self . plot_subplot ( agent_context , color = ' blue ' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = ' steps ' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = ' \u00d8 steps ' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len ( pc . actions [ episode ] ) for episode in pc . actions . keys () ] self . plot_subplot ( agent_context , color = ' blue ' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 ) ToMovie class ToMovie ( fps : Union [ int , NoneType ] = None , filepath : str = None ) Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. View Source class ToMovie ( core . _ PostProcessCallback ) : \"\"\"Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. \"\"\" def __ init__ ( self , fps : Optional [ int ] = None , filepath : str = None ) : \"\"\"Writes the ploted graphs and images to the mp4 / gif file given by filepath. if filepath ends in '.gif' an animated gif is created. Args: fps: frames per second filepath: the filepath of the mp4 or gif file file. If None the file is written to a temp file. \"\"\" super (). __ init__ () self . fps = fps self . _ is_filepath_set = filepath is not None self . filepath = filepath if not self . _ is_filepath_set: self . filepath = self . _ get_temp_path () if ( not self . _ is_animated_gif ()) and ( not self . filepath . lower (). endswith ( '.mp4' )) : self . filepath = self . filepath + '.mp4' self . _ video = imageio . get_writer ( self . filepath , fps = fps ) if fps else imageio . get_writer ( self . filepath ) def _ close ( self , agent_context: core . AgentContext ) : \"\"\"closes the mp4 file and displays it in jupyter cell (if in a jupyter notebook)\"\"\" self . _ video . close () self . _ video = None if agent_context . pyplot . is_jupyter_active: with open ( self . filepath , 'rb' ) as f : video = f . read () b64 = base64 . b64encode ( video ) if not self . _ is_filepath_set: os . remove ( self . filepath ) width = 640 height = 480 if self . _ is_animated_gif () : result = ''' <img src=\"data:image/gif;base64,{2}\" alt=\"easyagents.plot\" width={0}/> ''' . format ( width , height , b64 . decode ()) else : result = ''' <video width=\"{0}\" height=\"{1}\" controls> <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\"> Your browser does not support the video tag. </video>''' . format ( width , height , b64 . decode ()) result = HTML ( result ) # noinspection PyTypeChecker clear_output ( wait = True ) # noinspection PyTypeChecker display ( result ) def _ get_rgb_array ( self , agent_context: core . AgentContext ) -> np . ndarray : \"\"\"Yields an rgb array representing the current content of the subplots.\"\"\" pyc = agent_context . pyplot pyc . figure . canvas . draw () result = np . frombuffer ( pyc . figure . canvas . tostring_rgb (), dtype='uint8' ) result = result . reshape ( pyc . figure . canvas . get_width_height ()[ ::- 1 ] + ( 3 ,)) return result def _ get_temp_path ( self ) : result = os . path . join ( tempfile . gettempdir (), tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f'-{n.year % 100:2}{n.month:02}{n.day:02}-{n.hour:02}{n.minute:02}{n.second:02}-' + \\ f'{n.microsecond:06}' return result def _ is_animated_gif ( self ) : return self . filepath . lower (). endswith ( '.gif' ) def _ write_figure_to_video ( self , agent_context: core . AgentContext ) : \"\"\"Appends the current pyplot figure to the video. if an exception occures no frame is added. \"\"\" try : rgb_array = self . _ get_rgb_array ( agent_context ) self . _ video . append_data ( rgb_array ) except : pass def on_play_episode_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) : self . _ write_figure_to_video ( agent_context ) def on_play_step_end ( self , agent_context: core . AgentContext , action , step_result: Tuple ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_STEP ) : self . _ write_figure_to_video ( agent_context ) def on_train_iteration_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . TRAIN_ITERATION ) : self . _ write_figure_to_video ( agent_context ) def on_play_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_EPISODE ) : self . _ close ( agent_context ) def on_train_end ( self , agent_context: core . AgentContext ) : self . _ close ( agent_context ) Ancestors (in MRO) easyagents.core._PostProcessCallback easyagents.core.AgentCallback abc.ABC Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ) : if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ) : self . _close ( agent_context ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ) : if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) : self . _write_figure_to_video ( agent_context ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ) : if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ) : self . _write_figure_to_video ( agent_context ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : core . AgentContext ): self . _close ( agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ) : if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ) : self . _write_figure_to_video ( agent_context )","title":"Plot"},{"location":"reference/easyagents/callbacks/plot/#module-easyagentscallbacksplot","text":"View Source from typing import Optional , List , Tuple , Union , Dict import easyagents.core as core import base64 import matplotlib.pyplot as plt import numpy as np import imageio import math import gym import tempfile import os.path import datetime # download mp4 rendering imageio . plugins . ffmpeg . download () # avoid \"double rendering\" of the final jupyter output on_play_end_clear_jupyter_display : bool = False on_train_end_clear_jupyter_display : bool = True # check if we are running in Jupyter, if so interactive plotting must be handled differently # (in order to get plot updates during training) _is_jupyter_active = False try : # noinspection PyUnresolvedReferences from IPython import get_ipython # noinspection PyUnresolvedReferences from IPython.display import display , clear_output # noinspection PyUnresolvedReferences from IPython.display import HTML shell = get_ipython () . __class__ . __name__ if shell == 'ZMQInteractiveShell' : _is_jupyter_active = True else : # noinspection PyPackageRequirements import google.colab _is_jupyter_active = True except ImportError : pass class _PreProcess ( core . _PreProcessCallback ): \"\"\"Initializes the matplotlib agent_context.pyplot.figure\"\"\" def _setup ( self , agent_context : core . AgentContext ): # create figure / remove all existing axes from previous calls to train/play pyc = agent_context . pyplot pyc . is_jupyter_active = _is_jupyter_active if pyc . figure is None : pyc . figure = plt . figure ( \"_EasyAgents\" , figsize = pyc . figsize ) for ax in pyc . figure . axes : pyc . figure . delaxes ( ax ) def on_play_begin ( self , agent_context : core . AgentContext ): # play_begin is also called at the start of a policy evaluation if agent_context . is_play : self . _setup ( agent_context = agent_context ) def on_train_begin ( self , agent_context : core . AgentContext ): self . _setup ( agent_context = agent_context ) class _PostProcess ( core . _PostProcessCallback ): \"\"\"Redraws the plots int the matplotlib agent_context.figure. In jupyter the plots are only refreshed once per second. \"\"\" def __init__ ( self ): self . _call_jupyter_display : bool self . _reset () def _clear_jupyter_plots ( self , agent_context : core . AgentContext , wait = True ): \"\"\"Clears the content in the current jupyter output cell. NoOp if not in jupyter or no plot output. Args: wait: Wait to clear the output until new output is available to replace it. \"\"\" # don't clear the jupyter output if no plot is present, may clear the log output otherwise if agent_context . pyplot . is_jupyter_active and self . _plot_exists ( agent_context ): clear_output ( wait = wait ) def _display_plots ( self , agent_context : core . AgentContext ): \"\"\"Fixes the layout of multiple subplots and refreshs the display.\"\"\" pyc = agent_context . pyplot if self . _plot_exists ( agent_context ): count = len ( pyc . figure . axes ) rows = math . ceil ( count / pyc . max_columns ) columns = math . ceil ( count / rows ) for i in range ( count ): pyc . figure . axes [ i ] . change_geometry ( rows , columns , i + 1 ) pyc . figure . tight_layout () if pyc . is_jupyter_active : self . _clear_jupyter_plots ( agent_context ) if self . _call_jupyter_display : # noinspection PyTypeChecker display ( pyc . figure ) self . _call_jupyter_display = True else : plt . pause ( 0.01 ) def _plot_exists ( self , agent_context : core . AgentContext ): \"\"\"Yields true if at least 1 jupyter plot exists.\"\"\" pyc = agent_context . pyplot count = len ( pyc . figure . axes ) result = count > 0 return result def _reset ( self ): self . _call_jupyter_display = False def on_play_begin ( self , agent_context : core . AgentContext ): if agent_context . is_play : self . _reset () def on_train_begin ( self , agent_context : core . AgentContext ): self . _reset () def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _display_plots ( agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _display_plots ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_EVAL ): self . _display_plots ( agent_context ) if agent_context . is_play : self . _display_plots ( agent_context ) if on_play_end_clear_jupyter_display : self . _clear_jupyter_plots ( agent_context , wait = False ) def on_train_end ( self , agent_context : core . AgentContext ): self . _display_plots ( agent_context ) if on_train_end_clear_jupyter_display : self . _clear_jupyter_plots ( agent_context , wait = False ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): # display initial evaluation before training starts. if agent_context . train . iterations_done_in_training == 0 and \\ agent_context . _is_plot_ready ( core . PlotType . TRAIN_EVAL ): self . _display_plots ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _display_plots ( agent_context ) # noinspection DuplicatedCode class _PlotCallback ( core . AgentCallback ): \"\"\"Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto \"\"\" def __init__ ( self , plot_type : core . PlotType ): \"\"\"Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Args: plot_type: point in time when the plot is updated \"\"\" self . axes = None self . axes_color : str = 'grey' self . _plot_type : core . PlotType = plot_type def _create_subplot ( self , agent_context : core . AgentContext ): if self . axes is None : pyc = agent_context . pyplot pyc . _created_subplots = pyc . _created_subplots | self . _plot_type count = len ( pyc . figure . axes ) + 1 rows = math . ceil ( count / pyc . max_columns ) columns = math . ceil ( count / rows ) self . axes = pyc . figure . add_subplot ( rows , columns , count ) self . plot_axes ( xlim = ( 0 , 1 ), ylabel = '' , xlabel = '' ) def _is_nan ( self , values : Optional [ List [ float ]]): \"\"\"yields true if all values are equal to nan. yields false if values is None or empty.\"\"\" result = False if values and all ( isinstance ( v , float ) for v in values ): result = all ( math . isnan ( v ) for v in values ) return result def _refresh_subplot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): \"\"\"Sets this axes active and calls plot if this plot callback is registered on at least 1 plot out of plot_type.\"\"\" assert self . axes is not None plot_type = plot_type & self . _plot_type if agent_context . _is_plot_ready ( plot_type ): pyc = agent_context . pyplot if not pyc . is_jupyter_active : plt . figure ( pyc . figure . number ) if plt . gcf () is pyc . figure : plt . sca ( self . axes ) self . plot ( agent_context , plot_type ) def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): \"\"\"Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. \"\"\" pass def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot (axes, labels, colors) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ] . set_visible ( False ) self . axes . spines [ 'right' ] . set_visible ( False ) self . axes . spines [ 'bottom' ] . set_color ( axes_color ) self . axes . spines [ 'left' ] . set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) def plot_text ( self , text : str ): if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ([]) ax . get_yaxis () . set_ticks ([]) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = 'blue' ): \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ): \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [(min,y,max),...) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ): ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) # noinspection DuplicatedCode class Actions ( _PlotCallback ): def __init__ ( self , num_steps_between_plot = 100 ): \"\"\"Plots a histogram of the actions taken during play or during the last evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _actions : List [ float ] = [] self . _num_steps_between_plot = num_steps_between_plot def _reset ( self ): self . _actions : List [ float ] = [] def on_play_begin ( self , agent_context : core . AgentContext ): super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ): super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): try : pc = agent_context . play is_plot = True xlabel = 'actions' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = 'actions taken during last evaluation period' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = 'count' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f 'Failed to create the actions histogram. \\n ' ) class Clear ( core . AgentCallback ): \"\"\"Configures the clearing of plots in the jupyter output cell after calls to train or play.\"\"\" def __init__ ( self , on_play : bool = True , on_train : bool = True ): \"\"\"Define the cell clearing behaviour after agent.train and agent.play. Args: on_play: if set the output cell is cleared after agent.play if a plot exists on_train: if set the output cell is cleared after agent.train if a plot exists \"\"\" self . _on_play : bool = on_play self . _on_train : bool = on_train on_play_end_clear_jupyter_display = self . _on_play on_train_end_clear_jupyter_display = self . _on_train def on_play_begin ( self , agent_context : core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play def on_train_begin ( self , agent_context : core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train class Loss ( _PlotCallback ): def __init__ ( self , yscale : str = 'symlog' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the loss resulting from each iterations policy training. Hints: o for actro-critic agents the loss from training the actor- and critic-networks are plotted along with the total loss. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( plot_type = core . PlotType . TRAIN_ITERATION ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ): self . plot_text ( 'no loss data available' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ), xlabel = 'episodes trained' , ylim = self . ylim , ylabel = 'loss' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ): acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()), color = 'g' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()), color = 'b' ) self . axes . legend (( 'total' , 'actor' , 'critic' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' ) class Rewards ( _PlotCallback ): def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the sum of rewards observed during policy evaluation. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'sum of rewards' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = '\u00d8 sum of rewards' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = 'green' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) class State ( _PlotCallback ): \"\"\"Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. \"\"\" def __init__ ( self , mode = 'rgb_array' ): \"\"\" Args: mode: the render mode passed to gym.render(), yielding an rgb_array \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . TRAIN_EVAL ) self . _render_mode = mode def _plot_rgb_array ( self , agent_context : core . AgentContext , rgb_array : np . ndarray ): \"\"\"Renders rgb_array to the current subplot.\"\"\" assert rgb_array is not None ax = self . axes xlabel = '' if agent_context . is_eval : xlabel = \"'done state' of last evaluation episode\" ax . imshow ( rgb_array ) ax . set_xlabel ( xlabel ) ax . get_xaxis () . set_ticks ([]) ax . get_yaxis () . set_ticks ([]) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) # noinspection PyArgumentList,DuplicatedCode def _render_to_rgb_array ( self , gym_env : gym . Env , mode : str ) -> np . ndarray : \"\"\" calls gym_env.render(mode) and validates the return value to be a numpy rgb array throws an exception if not an rgb array Returns: numpy rgb array \"\"\" result = gym_env . render ( mode = mode ) assert result is not None , f 'gym_env.render(mode={mode}) yielded None' assert isinstance ( result , np . ndarray ), f 'gym_env.render(mode={mode}) did not yield a numpy.ndarray.' assert result . min () >= 0 , f 'gym_env.render(mode={mode}) contains negative values => not an rgb array' assert result . max () <= 255 , f 'gym_env.render(mode={mode}) contains values > 255 => not an rgb array' assert len ( result . shape ) == 3 , f 'gym_env.render(mode={mode}) shape is not of the form (x,y,n)' assert result . shape [ 2 ] == 3 or result . shape [ 2 ] == 4 , \\ f 'gym_env.render(mode={mode}) shape is not of the form (x,y,3|4)' return result def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\"{self._render_mode}\") failed: \\n ' ) class Steps ( _PlotCallback ): def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the step counts observed during policy evaluation. Args: yscale: scale of the y-axes ('linear','log') ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'steps' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = '\u00d8 steps' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len ( pc . actions [ episode ]) for episode in pc . actions . keys ()] self . plot_subplot ( agent_context , color = 'blue' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) class StepRewards ( _PlotCallback ): def __init__ ( self , num_steps_between_plot = 100 ): \"\"\"Plots the sum of rewards up to the current step during play or at the end of an evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super () . __init__ ( core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _xy_values : Dict [ int , Tuple [ List [ int ], List [ float ]]] = dict () self . _xmax : int = 0 self . _num_steps_between_plot = num_steps_between_plot def _replot ( self , agent_context : core . AgentContext ): if self . _xmax >= 1 : self . clear_plot ( agent_context ) xlabel = 'steps played' if agent_context . is_eval : xlabel = 'steps taken during last evaluation period' self . plot_axes ( xlim = ( 1 , self . _xmax ), ylabel = 'sum of rewards' , xlabel = xlabel ) xy_values = list ( self . _xy_values . values ()) xlast , _ = xy_values [ - 1 ] for xvalues , yvalues in xy_values : pause = ( xvalues == xlast ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = None , marker = '' , pause = pause ) def _reset ( self ): self . _xy_values = dict () self . _xmax = 0 def on_play_begin ( self , agent_context : core . AgentContext ): super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ): super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ([], []) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ]) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ([ len ( step_rewards ) for step_rewards in pc . rewards . values ()]) for episode in pc . rewards . keys (): step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ - 1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context ) class ToMovie ( core . _PostProcessCallback ): \"\"\"Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. \"\"\" def __init__ ( self , fps : Optional [ int ] = None , filepath : str = None ): \"\"\"Writes the ploted graphs and images to the mp4 / gif file given by filepath. if filepath ends in '.gif' an animated gif is created. Args: fps: frames per second filepath: the filepath of the mp4 or gif file file. If None the file is written to a temp file. \"\"\" super () . __init__ () self . fps = fps self . _is_filepath_set = filepath is not None self . filepath = filepath if not self . _is_filepath_set : self . filepath = self . _get_temp_path () if ( not self . _is_animated_gif ()) and ( not self . filepath . lower () . endswith ( '.mp4' )): self . filepath = self . filepath + '.mp4' self . _video = imageio . get_writer ( self . filepath , fps = fps ) if fps else imageio . get_writer ( self . filepath ) def _close ( self , agent_context : core . AgentContext ): \"\"\"closes the mp4 file and displays it in jupyter cell (if in a jupyter notebook)\"\"\" self . _video . close () self . _video = None if agent_context . pyplot . is_jupyter_active : with open ( self . filepath , 'rb' ) as f : video = f . read () b64 = base64 . b64encode ( video ) if not self . _is_filepath_set : os . remove ( self . filepath ) width = 640 height = 480 if self . _is_animated_gif (): result = ''' <img src=\"data:image/gif;base64,{2}\" alt=\"easyagents.plot\" width={0}/> ''' . format ( width , height , b64 . decode ()) else : result = ''' <video width=\"{0}\" height=\"{1}\" controls> <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\"> Your browser does not support the video tag. </video>''' . format ( width , height , b64 . decode ()) result = HTML ( result ) # noinspection PyTypeChecker clear_output ( wait = True ) # noinspection PyTypeChecker display ( result ) def _get_rgb_array ( self , agent_context : core . AgentContext ) -> np . ndarray : \"\"\"Yields an rgb array representing the current content of the subplots.\"\"\" pyc = agent_context . pyplot pyc . figure . canvas . draw () result = np . frombuffer ( pyc . figure . canvas . tostring_rgb (), dtype = 'uint8' ) result = result . reshape ( pyc . figure . canvas . get_width_height ()[:: - 1 ] + ( 3 ,)) return result def _get_temp_path ( self ): result = os . path . join ( tempfile . gettempdir (), tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f '-{n.year % 100:2}{n.month:02}{n.day:02}-{n.hour:02}{n.minute:02}{n.second:02}-' + \\ f '{n.microsecond:06}' return result def _is_animated_gif ( self ): return self . filepath . lower () . endswith ( '.gif' ) def _write_figure_to_video ( self , agent_context : core . AgentContext ): \"\"\"Appends the current pyplot figure to the video. if an exception occures no frame is added. \"\"\" try : rgb_array = self . _get_rgb_array ( agent_context ) self . _video . append_data ( rgb_array ) except : pass def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ): self . _write_figure_to_video ( agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _write_figure_to_video ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _write_figure_to_video ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _close ( agent_context ) def on_train_end ( self , agent_context : core . AgentContext ): self . _close ( agent_context )","title":"Module easyagents.callbacks.plot"},{"location":"reference/easyagents/callbacks/plot/#variables","text":"on_play_end_clear_jupyter_display on_train_end_clear_jupyter_display shell","title":"Variables"},{"location":"reference/easyagents/callbacks/plot/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/callbacks/plot/#actions","text":"class Actions ( num_steps_between_plot = 100 ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Actions ( _PlotCallback ) : def __init__ ( self , num_steps_between_plot = 100 ) : \"\"\" Plots a histogram of the actions taken during play or during the last evaluation period. Args : num_steps_between_plot : num of steps to play before plot is updated . \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _actions : List [ float ] = [] self . _num_steps_between_plot = num_steps_between_plot def _reset ( self ) : self . _actions : List [ float ] = [] def on_play_begin ( self , agent_context : core . AgentContext ) : super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ) : super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : try : pc = agent_context . play is_plot = True xlabel = ' actions ' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = ' actions taken during last evaluation period ' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = ' count ' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f ' Failed to create the actions histogram. \\n ' )","title":"Actions"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): super (). on_play_begin ( agent_context ) self . _reset ()","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): super (). on_play_end ( agent_context ) self . _reset ()","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : try : pc = agent_context . play is_plot = True xlabel = ' actions ' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = ' actions taken during last evaluation period ' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = ' count ' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f ' Failed to create the actions histogram. \\n ' )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#clear","text":"class Clear ( on_play : bool = True , on_train : bool = True ) Configures the clearing of plots in the jupyter output cell after calls to train or play. View Source class Clear ( core . AgentCallback ) : \"\"\" Configures the clearing of plots in the jupyter output cell after calls to train or play. \"\"\" def __init__ ( self , on_play : bool = True , on_train : bool = True ) : \"\"\" Define the cell clearing behaviour after agent.train and agent.play. Args : on_play : if set the output cell is cleared after agent . play if a plot exists on_train : if set the output cell is cleared after agent . train if a plot exists \"\"\" self . _on_play : bool = on_play self . _on_train : bool = on_train on_play_end_clear_jupyter_display = self . _on_play on_train_end_clear_jupyter_display = self . _on_train def on_play_begin ( self , agent_context : core . AgentContext ) : global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play def on_train_begin ( self , agent_context : core . AgentContext ) : global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train","title":"Clear"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_1","text":"easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_1","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_1","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_1","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_1","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_1","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_1","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_1","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_1","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_1","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_1","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_1","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_1","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_1","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_1","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_1","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#loss","text":"class Loss ( yscale : str = 'symlog' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Loss ( _PlotCallback ) : def __init__ ( self , yscale : str = ' symlog ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Plots the loss resulting from each iterations policy training. Hints : o for actro - critic agents the loss from training the actor - and critic - networks are plotted along with the total loss . Args : yscale : scale of the y - axes ( ' linear ' , ' symlog ' ,... ) ylim : ( min , max ) for the y - axes \"\"\" super () . __init__ ( plot_type = core . PlotType . TRAIN_ITERATION ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ) : self . plot_text ( ' no loss data available ' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ) , xlabel = ' episodes trained ' , ylim = self . ylim , ylabel = ' loss ' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ) : acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = ' indigo ' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()) , color = ' g ' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()) , color = ' b ' ) self . axes . legend (( ' total ' , ' actor ' , ' critic ' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = ' indigo ' )","title":"Loss"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_2","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_1","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_2","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_2","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_2","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_2","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_2","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_2","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_2","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_2","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_2","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_2","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_2","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_2","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_2","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_2","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_2","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_2","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_2","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_2","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_1","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ) : self . plot_text ( ' no loss data available ' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ) , xlabel = ' episodes trained ' , ylim = self . ylim , ylabel = ' loss ' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ) : acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = ' indigo ' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()) , color = ' g ' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()) , color = ' b ' ) self . axes . legend (( ' total ' , ' actor ' , ' critic ' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = ' indigo ' )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_1","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_1","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_1","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_1","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#rewards","text":"class Rewards ( yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Rewards ( _PlotCallback ) : def __init__ ( self , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Plots the sum of rewards observed during policy evaluation. Args : yscale : scale of the y - axes ( ' linear ' , ' symlog ' ,... ) ylim : ( min , max ) for the y - axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = ' sum of rewards ' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = ' \u00d8 sum of rewards ' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = ' green ' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel )","title":"Rewards"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_3","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_2","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_3","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_3","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_3","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_3","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_3","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_3","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_3","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_3","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_3","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_3","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_3","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_3","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_3","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_3","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_3","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_3","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_3","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_3","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_2","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = ' sum of rewards ' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = ' \u00d8 sum of rewards ' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = ' green ' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_2","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_2","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_2","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_2","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#state","text":"class State ( mode = 'rgb_array' ) Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. View Source class State ( _PlotCallback ) : \"\"\" Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted . During play all states are plotted . \"\"\" def __init__ ( self , mode = ' rgb_array ' ) : \"\"\" Args : mode : the render mode passed to gym . render () , yielding an rgb_array \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . TRAIN_EVAL ) self . _render_mode = mode def _plot_rgb_array ( self , agent_context : core . AgentContext , rgb_array : np . ndarray ) : \"\"\" Renders rgb_array to the current subplot. \"\"\" assert rgb_array is not None ax = self . axes xlabel = '' if agent_context . is_eval : xlabel = \" 'done state' of last evaluation episode \" ax . imshow ( rgb_array ) ax . set_xlabel ( xlabel ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color ) # noinspection PyArgumentList , DuplicatedCode def _render_to_rgb_array ( self , gym_env : gym . Env , mode : str ) -> np . ndarray : \"\"\" calls gym_env.render(mode) and validates the return value to be a numpy rgb array throws an exception if not an rgb array Returns : numpy rgb array \"\"\" result = gym_env . render ( mode = mode ) assert result is not None , f ' gym_env.render(mode={mode}) yielded None ' assert isinstance ( result , np . ndarray ) , f ' gym_env.render(mode={mode}) did not yield a numpy.ndarray. ' assert result . min () >= 0 , f ' gym_env.render(mode={mode}) contains negative values => not an rgb array ' assert result . max () <= 255 , f ' gym_env.render(mode={mode}) contains values > 255 => not an rgb array ' assert len ( result . shape ) == 3 , f ' gym_env.render(mode={mode}) shape is not of the form (x,y,n) ' assert result . shape [ 2 ] == 3 or result . shape [ 2 ] == 4 , \\ f ' gym_env.render(mode={mode}) shape is not of the form (x,y,3|4) ' return result def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f ' gym.Env.render(mode=\"{self._render_mode}\") failed: \\n ' )","title":"State"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_4","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_4","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_3","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_4","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_4","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_4","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_4","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_4","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_4","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_4","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_4","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_4","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_4","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_4","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_4","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_4","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_4","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_4","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_4","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_4","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_4","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_3","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\"{self._render_mode}\") failed:\\n' )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_3","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_3","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_3","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_3","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#steprewards","text":"class StepRewards ( num_steps_between_plot = 100 ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class StepRewards ( _PlotCallback ) : def __init__ ( self , num_steps_between_plot = 100 ) : \"\"\" Plots the sum of rewards up to the current step during play or at the end of an evaluation period . Args : num_steps_between_plot : num of steps to play before plot is updated . \"\"\" super () . __init__ ( core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _xy_values : Dict [ int , Tuple [ List [ int ], List [ float ]]] = dict () self . _xmax : int = 0 self . _num_steps_between_plot = num_steps_between_plot def _replot ( self , agent_context : core . AgentContext ) : if self . _xmax >= 1 : self . clear_plot ( agent_context ) xlabel = ' steps played ' if agent_context . is_eval : xlabel = ' steps taken during last evaluation period ' self . plot_axes ( xlim = ( 1 , self . _xmax ) , ylabel = ' sum of rewards ' , xlabel = xlabel ) xy_values = list ( self . _xy_values . values ()) xlast , _ = xy_values [ - 1 ] for xvalues , yvalues in xy_values : pause = ( xvalues == xlast ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = None , marker = '' , pause = pause ) def _reset ( self ) : self . _xy_values = dict () self . _xmax = 0 def on_play_begin ( self , agent_context : core . AgentContext ) : super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ) : super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ( [], [] ) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ] ) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ( [ len ( step_rewards ) for step_rewards in pc . rewards . values () ] ) for episode in pc . rewards . keys () : step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ - 1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context )","title":"StepRewards"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_5","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_5","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_4","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_5","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_5","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_5","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_5","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_5","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_5","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_5","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_5","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_5","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): super (). on_play_begin ( agent_context ) self . _reset ()","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_5","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): super (). on_play_end ( agent_context ) self . _reset ()","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_5","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_5","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_5","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_5","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_5","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_5","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_5","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_5","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_4","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ( [], [] ) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ] ) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ( [ len ( step_rewards ) for step_rewards in pc . rewards . values () ] ) for episode in pc . rewards . keys () : step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ - 1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_4","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_4","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_4","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_4","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#steps","text":"class Steps ( yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Steps ( _PlotCallback ) : def __init__ ( self , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Plots the step counts observed during policy evaluation. Args : yscale : scale of the y - axes ( ' linear ' , ' log ' ) ylim : ( min , max ) for the y - axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = ' steps ' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = ' \u00d8 steps ' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len ( pc . actions [ episode ] ) for episode in pc . actions . keys () ] self . plot_subplot ( agent_context , color = ' blue ' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel )","title":"Steps"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_6","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_6","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_5","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ) : \"\"\" Clears the axes for this plot. Should be called by self.plot before replotting an axes. \"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_6","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_6","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_6","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_6","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_6","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_6","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_6","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_6","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_6","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_6","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_6","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_6","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_6","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_6","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_6","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ) : if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_6","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_6","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_6","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_5","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = ' steps ' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = ' \u00d8 steps ' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len ( pc . actions [ episode ] ) for episode in pc . actions . keys () ] self . plot_subplot ( agent_context , color = ' blue ' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_5","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None ) : \"\"\" Draws the x- and y-axes. Attributes : xlim : None or ( min , max ) for the x - axes xlabel : label of the x - axes ylim : None or ( min , max ) for the y - axes ( or None ) ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' , ... ) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ ' top ' ]. set_visible ( False ) self . axes . spines [ ' right ' ]. set_visible ( False ) self . axes . spines [ ' bottom ' ]. set_color ( axes_color ) self . axes . spines [ ' left ' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = ' - ' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_5","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = ' linear ' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = ' blue ' ) : \"\"\" Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) ylim : ( min , max ) for the x - axes xlabel : label of the x - axes ylabel : label of the y - axes yscale : scale of the y - axes ( ' linear ' , ' log ' ,... ) ylim : ( min , max ) for the y - axes ( or None ) color : the graphs color ( must be the name of a matplotlib color ) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = ' episodes ' if agent_context . is_play : xlabel = ' episodes played ' if agent_context . is_eval or agent_context . is_train : xlabel = ' episodes trained ' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_5","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0 . 5 , 0 . 5 , text , horizontalalignment = ' center ' , verticalalignment = ' center ' , color = ' blue ' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ( [] ) ax . get_yaxis () . set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ]. set_visible ( True ) ax . spines [ spin ]. set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_5","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = ' blue ' , marker : str = None , pause : bool = True ) : \"\"\" Draws the graph given by xvalues, yvalues. Attributes : agent_context : context containing the figure to plot to xvalues : the graphs x - values ( must have same length as y - values ) yvalues : the graphs y - values or ( min , y , max ) - tuples ( must have same length as x - values ) color : the graphs color ( must be the name of a matplotlib color ) or None marker : if None a marker is used if the graph has just a few points . pause : pause to redraw the plot . \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ) , \" xvalues do not match yvalues \" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ ( min , y , max ) ,... ) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ) : ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = ' o ' fill_alpha = 0 . 1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0 . 01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#tomovie","text":"class ToMovie ( fps : Union [ int , NoneType ] = None , filepath : str = None ) Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. View Source class ToMovie ( core . _ PostProcessCallback ) : \"\"\"Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. \"\"\" def __ init__ ( self , fps : Optional [ int ] = None , filepath : str = None ) : \"\"\"Writes the ploted graphs and images to the mp4 / gif file given by filepath. if filepath ends in '.gif' an animated gif is created. Args: fps: frames per second filepath: the filepath of the mp4 or gif file file. If None the file is written to a temp file. \"\"\" super (). __ init__ () self . fps = fps self . _ is_filepath_set = filepath is not None self . filepath = filepath if not self . _ is_filepath_set: self . filepath = self . _ get_temp_path () if ( not self . _ is_animated_gif ()) and ( not self . filepath . lower (). endswith ( '.mp4' )) : self . filepath = self . filepath + '.mp4' self . _ video = imageio . get_writer ( self . filepath , fps = fps ) if fps else imageio . get_writer ( self . filepath ) def _ close ( self , agent_context: core . AgentContext ) : \"\"\"closes the mp4 file and displays it in jupyter cell (if in a jupyter notebook)\"\"\" self . _ video . close () self . _ video = None if agent_context . pyplot . is_jupyter_active: with open ( self . filepath , 'rb' ) as f : video = f . read () b64 = base64 . b64encode ( video ) if not self . _ is_filepath_set: os . remove ( self . filepath ) width = 640 height = 480 if self . _ is_animated_gif () : result = ''' <img src=\"data:image/gif;base64,{2}\" alt=\"easyagents.plot\" width={0}/> ''' . format ( width , height , b64 . decode ()) else : result = ''' <video width=\"{0}\" height=\"{1}\" controls> <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\"> Your browser does not support the video tag. </video>''' . format ( width , height , b64 . decode ()) result = HTML ( result ) # noinspection PyTypeChecker clear_output ( wait = True ) # noinspection PyTypeChecker display ( result ) def _ get_rgb_array ( self , agent_context: core . AgentContext ) -> np . ndarray : \"\"\"Yields an rgb array representing the current content of the subplots.\"\"\" pyc = agent_context . pyplot pyc . figure . canvas . draw () result = np . frombuffer ( pyc . figure . canvas . tostring_rgb (), dtype='uint8' ) result = result . reshape ( pyc . figure . canvas . get_width_height ()[ ::- 1 ] + ( 3 ,)) return result def _ get_temp_path ( self ) : result = os . path . join ( tempfile . gettempdir (), tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f'-{n.year % 100:2}{n.month:02}{n.day:02}-{n.hour:02}{n.minute:02}{n.second:02}-' + \\ f'{n.microsecond:06}' return result def _ is_animated_gif ( self ) : return self . filepath . lower (). endswith ( '.gif' ) def _ write_figure_to_video ( self , agent_context: core . AgentContext ) : \"\"\"Appends the current pyplot figure to the video. if an exception occures no frame is added. \"\"\" try : rgb_array = self . _ get_rgb_array ( agent_context ) self . _ video . append_data ( rgb_array ) except : pass def on_play_episode_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) : self . _ write_figure_to_video ( agent_context ) def on_play_step_end ( self , agent_context: core . AgentContext , action , step_result: Tuple ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_STEP ) : self . _ write_figure_to_video ( agent_context ) def on_train_iteration_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . TRAIN_ITERATION ) : self . _ write_figure_to_video ( agent_context ) def on_play_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_EPISODE ) : self . _ close ( agent_context ) def on_train_end ( self , agent_context: core . AgentContext ) : self . _ close ( agent_context )","title":"ToMovie"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_7","text":"easyagents.core._PostProcessCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_7","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_7","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_7","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_7","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_7","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_7","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_7","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_7","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_7","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_7","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_7","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ) : if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ) : self . _close ( agent_context )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_7","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_7","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ) : if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) : self . _write_figure_to_video ( agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_7","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_7","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ) : if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ) : self . _write_figure_to_video ( agent_context )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_7","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_7","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : core . AgentContext ): self . _close ( agent_context )","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_7","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_7","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ) : if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ) : self . _write_figure_to_video ( agent_context )","title":"on_train_iteration_end"}]}